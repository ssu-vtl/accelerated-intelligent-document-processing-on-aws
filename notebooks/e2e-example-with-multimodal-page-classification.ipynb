{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Document Processing with IDP Common Package\n",
    "\n",
    "This notebook demonstrates how to process a document using the modular Document-based approach with:\n",
    "\n",
    "1. OCR Service - Convert a PDF document to text using AWS Textract\n",
    "2. Classification Service - Classify document pages into sections using Bedrock using the multi-model page based method.\n",
    "3. Extraction Service - Extract structured information from sections using Bedrock\n",
    "4. Evaluation Service - Evaluate accuracy of extracted information\n",
    "\n",
    "Each step uses the unified Document object model for data flow and consistency.\n",
    "\n",
    "> **Note**: This notebook uses AWS services including S3, Textract, and Bedrock. You need valid AWS credentials with appropriate permissions to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "The IDP common package supports granular installation through extras. You can install:\n",
    "- `[core]` - Just core functionality \n",
    "- `[ocr]` - OCR service with Textract dependencies\n",
    "- `[classification]` - Classification service dependencies\n",
    "- `[extraction]` - Extraction service dependencies\n",
    "- `[evaluation]` - Evaluation service dependencies\n",
    "- `[all]` - All of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure that modules are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# First uninstall existing package (to ensure we get the latest version)\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package with all components in development mode\n",
    "%pip install -q -e \"../lib/idp_common_pkg[dev, all]\"\n",
    "\n",
    "# Note: We can also install specific components like:\n",
    "# %pip install -q -e \"../lib/idp_common_pkg[ocr,classification,extraction,evaluation]\"\n",
    "\n",
    "# Check installed version\n",
    "%pip show idp_common | grep -E \"Version|Location\"\n",
    "\n",
    "# Optionally use a .env file for environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  \n",
    "except ImportError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Import base libraries\n",
    "from idp_common.models import Document, Status, Section, Page\n",
    "from idp_common import ocr, classification, extraction, evaluation\n",
    "\n",
    "# Configure logging \n",
    "logging.basicConfig(level=logging.WARNING)  # Set root logger to WARNING (less verbose)\n",
    "logging.getLogger('idp_common.ocr.service').setLevel(logging.INFO)  # Focus on service logs\n",
    "logging.getLogger('textractor').setLevel(logging.WARNING)  # Suppress textractor logs\n",
    "logging.getLogger('idp_common.evaluation.service').setLevel(logging.DEBUG)  # Enable evaluation logs\n",
    "logging.getLogger('idp_common.bedrock.client').setLevel(logging.DEBUG)  # show prompts\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Notebook-Example'\n",
    "os.environ['AWS_REGION'] = boto3.session.Session().region_name or 'us-east-1'\n",
    "\n",
    "# Get AWS account ID for unique bucket names\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = os.environ['AWS_REGION']\n",
    "\n",
    "# Define sample PDF path \n",
    "SAMPLE_PDF_PATH = \"../samples/rvl_cdip_package.pdf\"\n",
    "\n",
    "# Create unique bucket names based on account ID and region\n",
    "input_bucket_name =  os.getenv(\"IDP_INPUT_BUCKET_NAME\", f\"idp-notebook-input-{account_id}-{region}\")\n",
    "output_bucket_name = os.getenv(\"IDP_OUTPUT_BUCKET_NAME\", f\"idp-notebook-output-{account_id}-{region}\")\n",
    "\n",
    "# Helper function to parse S3 URIs\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "# Helper function to load JSON from S3\n",
    "def load_json_from_s3(uri):\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "print(\"Environment setup:\")\n",
    "print(f\"METRIC_NAMESPACE: {os.environ.get('METRIC_NAMESPACE')}\")\n",
    "print(f\"AWS_REGION: {os.environ.get('AWS_REGION')}\")\n",
    "print(f\"Input bucket: {input_bucket_name}\")\n",
    "print(f\"Output bucket: {output_bucket_name}\")\n",
    "print(f\"SAMPLE_PDF_PATH: {SAMPLE_PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up S3 Buckets and Upload Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Function to create a bucket if it doesn't exist\n",
    "def ensure_bucket_exists(bucket_name):\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            if region == 'us-east-1':\n",
    "                s3_client.create_bucket(Bucket=bucket_name)\n",
    "            else:\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=bucket_name,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                )\n",
    "            print(f\"Created bucket: {bucket_name}\")\n",
    "            \n",
    "            # Wait for bucket to be accessible\n",
    "            waiter = s3_client.get_waiter('bucket_exists')\n",
    "            waiter.wait(Bucket=bucket_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket {bucket_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Ensure both buckets exist\n",
    "ensure_bucket_exists(input_bucket_name)\n",
    "ensure_bucket_exists(output_bucket_name)\n",
    "\n",
    "# Upload the sample file to S3\n",
    "sample_file_key = \"sample-\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".pdf\"\n",
    "with open(SAMPLE_PDF_PATH, 'rb') as file_data:\n",
    "    s3_client.upload_fileobj(file_data, input_bucket_name, sample_file_key)\n",
    "\n",
    "print(f\"Uploaded sample file to: s3://{input_bucket_name}/{sample_file_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample configuration that mimics what would be in DynamoDB\n",
    "CONFIG = {\n",
    "    \"evaluation\": {\n",
    "        \"llm_method\": {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 5,\n",
    "            \"system_prompt\": \"\"\"You are an evaluator that helps determine if the predicted and expected values match for document attribute extraction. You will consider the context and meaning rather than just exact string matching.\"\"\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.  Here's the exact format:\n",
    "{\n",
    "  \"match\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"Your explanation here\"\n",
    "}\"\"\"\n",
    "        }\n",
    "    },\n",
    "    \"classes\": [\n",
    "        {\n",
    "        \"name\": \"letter\",\n",
    "        \"description\": \"A formal written message that is typically sent from one person to another\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"sender_name\",\n",
    "            \"description\": \"The name of the person or entity who wrote or sent the letter. Look for text following or near terms like 'from', 'sender', 'authored by', 'written by', or at the end of the letter before a signature.\",\n",
    "            \"evaluation_method\": \"LLM\" \n",
    "            },\n",
    "            {\n",
    "            \"name\": \"sender_address\",\n",
    "            \"description\": \"The physical address of the sender, typically appearing at the top of the letter. May be labeled as 'address', 'location', or 'from address'.\",\n",
    "            \"evaluation_method\": \"LLM\", \n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"form\",\n",
    "        \"description\": \"A document with blank spaces for filling in information\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"form_type\",\n",
    "            \"description\": \"The category or purpose of the form, such as 'application', 'registration', 'request', etc. May be identified by 'form name', 'document type', or 'form category'.\",\n",
    "            \"evaluation_method\": \"FUZZY\",\n",
    "            \"evaluation_threshold\": \"0.8\",\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"form_id\",\n",
    "            \"description\": \"The unique identifier for the form, typically a number or alphanumeric code. Often labeled as 'form number', 'id', or 'reference number'.\",\n",
    "            \"evaluation_method\": \"NUMERIC_EXACT\",\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"email\",\n",
    "        \"description\": \"An electronic message sent from one person to another over a computer network\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"from_address\",\n",
    "            \"description\": \"The email address of the sender. Look for text following 'from', 'sender', or 'sent by', typically at the beginning of the email header.\",\n",
    "            # Evaluation method not specified - will default to LLM\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"to_address\",\n",
    "            \"description\": \"The email address of the primary recipient. May be labeled as 'to', 'recipient', or 'sent to'.\",\n",
    "            # Evaluation method not specified - will default to LLM\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"specification\",\n",
    "        \"description\": \"A detailed description of technical requirements or characteristics\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"product_name\",\n",
    "            \"description\": \"The name of the item being specified. Look for text labeled as 'product', 'item', or 'model', typically appearing prominently at the beginning.\",\n",
    "            \"evaluation_method\": \"FUZZY\",\n",
    "            \"evaluation_threshold\": 0.7\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"version\",\n",
    "            \"description\": \"The iteration or release number. May be indicated by 'version', 'revision', or 'release', often followed by a number or code.\",\n",
    "            \"evaluation_method\": \"NUMERIC_EXACT\"\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"memo\",\n",
    "        \"description\": \"A brief written message used for internal communication within an organization\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"memo_date\",\n",
    "            \"description\": \"The date when the memo was written. Look for 'date' or 'memo date', typically near the top of the document.\",\n",
    "            \"evaluation_method\": \"EXACT\"\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"from\",\n",
    "            \"description\": \"The person or department that wrote the memo. May be labeled as 'from', 'sender', or 'author'.\",\n",
    "            \"evaluation_method\": \"LLM\", \n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"invoice\",\n",
    "        \"description\": \"A commercial document issued by a seller to a buyer relating to a sale\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"invoice_number\",\n",
    "            \"description\": \"The unique identifier for the invoice. Look for 'invoice no', 'invoice #', or 'bill number', typically near the top of the document.\",\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"invoice_date\",\n",
    "            \"description\": \"The date when the invoice was issued. May be labeled as 'date', 'invoice date', or 'billing date'.\",\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"resume\",\n",
    "        \"description\": \"A document summarizing a person's background, skills, and qualifications\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"full_name\",\n",
    "            \"description\": \"The complete name of the job applicant, typically appearing prominently at the top of the resume. May be simply labeled as 'name' or 'applicant name'.\",\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"contact_info\",\n",
    "            \"description\": \"The phone number, email, and address of the applicant. Look for a section with 'contact', 'phone', 'email', or 'address', usually near the top of the resume.\",\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"scientific_publication\",\n",
    "        \"description\": \"A formally published document presenting scientific research findings\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"title\",\n",
    "            \"description\": \"The name of the scientific paper, typically appearing prominently at the beginning. May be labeled as 'title', 'paper title', or 'article title'.\",\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"authors\",\n",
    "            \"description\": \"The researchers who conducted the study and wrote the paper. Look for names after 'authors', 'contributors', or 'researchers', usually following the title.\",\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"advertisement\",\n",
    "        \"description\": \"A public notice promoting a product, service, or event\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"product_name\",\n",
    "            \"description\": \"The name of the item or service being advertised. Look for prominently displayed text that could be a 'product', 'item', or 'service' name.\",\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"brand\",\n",
    "            \"description\": \"The company or manufacturer of the product. May be indicated by a logo or text labeled as 'brand', 'company', or 'manufacturer'.\",\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"questionnaire\",\n",
    "        \"description\": \"A set of written questions designed to collect information from respondents\",\n",
    "        \"attributes\": [\n",
    "            {\n",
    "            \"name\": \"form_title\",\n",
    "            \"description\": \"The name or title of the questionnaire. Look for prominently displayed text at the beginning that could be a 'title', 'survey name', or 'questionnaire name'.\",\n",
    "            },\n",
    "            {\n",
    "            \"name\": \"respondent_info\",\n",
    "            \"description\": \"Information about the person completing the questionnaire. May include fields labeled 'respondent', 'participant', or 'name'.\",\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "    ],\n",
    "  \"classification\": {\n",
    "    \"temperature\": \"0\",\n",
    "    \"model\": \"us.amazon.nova-pro-v1:0\",\n",
    "    \"classificationMethod\": \"multimodalPageLevelClassification\",  # Use multimodal page level classification\n",
    "    \"system_prompt\": \"You are a document classification system that analyzes business documents, forms, and publications. Your sole task is to classify documents based on their visual layout and textual content. You must:\\n\\n1. Output only a JSON object containing a single \\\"class\\\" field with the classification label\\n2. Use exactly one of the predefined categories, using the exact spelling and case provided\\n3. Never include explanations, reasoning, or additional text in your response\\n4. Respond with nothing but the JSON containing the classification\\n\\nExample correct response:\\n{\\\"class\\\": \\\"letter\\\"}\\n\",\n",
    "    \"top_k\": \"5\",\n",
    "    \"task_prompt\": \"Classify this document into exactly one of these categories:\\n\\n{CLASS_NAMES_AND_DESCRIPTIONS}\\n\\nRespond only with a JSON object containing the class label. For example: {{\\\"class\\\": \\\"letter\\\"}}\\n\\n<<CACHEPOINT>>\\n\\n<document_ocr_text>\\n{DOCUMENT_TEXT}\\n</document_ocr_text>\\n\\n<document_image>\\n{DOCUMENT_IMAGE}\\n</document_image>\"\n",
    "  },\n",
    "  \"extraction\": {\n",
    "    \"temperature\": \"0\",\n",
    "    \"model\": \"us.amazon.nova-pro-v1:0\",\n",
    "    \"system_prompt\": \"You are a document assistant. Respond only with JSON. Never make up data, only provide data found in the document being provided.\\n\",\n",
    "    \"top_k\": \"5\",\n",
    "    \"task_prompt\": \"<background>\\nYou are an expert in business document analysis and information extraction. \\nYou can understand and extract key information from business documents,\\n<task>\\nYour task is to take the unstructured text provided and convert it into a well-organized table format using JSON. Identify the main entities, attributes, or categories mentioned in the attributes list below and use them as keys in the JSON object. \\nThen, extract the relevant information from the text and populate the corresponding values in the JSON object. \\nGuidelines:\\nEnsure that the data is accurately represented and properly formatted within the JSON structure\\nInclude double quotes around all keys and values\\nDo not make up data - only extract information explicitly found in the document\\nDo not use /n for new lines, use a space instead\\nIf a field is not found or if unsure, return null\\nAll dates should be in MM/DD/YYYY format\\nDo not perform calculations or summations unless totals are explicitly given\\nIf an alias is not found in the document, return null\\nHere are the attributes you should extract:\\n<attributes>\\n{ATTRIBUTE_NAMES_AND_DESCRIPTIONS}\\n</attributes>\\n</task>\\n\\n<<CACHEPOINT>>\\n\\n</background>\\nThe document type is {DOCUMENT_CLASS}. Here is the document content:\\n<document_ocr_data>\\n\\n{DOCUMENT_TEXT}\\n\\n</document_ocr_data><document_image>\\n{DOCUMENT_IMAGE}\\n</document_image> <final-instructions>Extract key information from the document and return a JSON object </final-instructions>\\n\"\n",
    "  }\n",
    "}\n",
    "\n",
    "print(\"Test configuration created for IDP services with LLM evaluation method and enhanced logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Document with OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new Document\n",
    "document = Document(\n",
    "    id=\"rvl-cdip-package\",\n",
    "    input_bucket=input_bucket_name,\n",
    "    input_key=sample_file_key,\n",
    "    output_bucket=output_bucket_name,\n",
    "    status=Status.QUEUED\n",
    ")\n",
    "\n",
    "print(f\"Created document with ID: {document.id}\")\n",
    "print(f\"Status: {document.status.value}\")\n",
    "\n",
    "# Create OCR service with Textract\n",
    "# Valid features are 'LAYOUT', 'FORMS', 'SIGNATURES', 'TABLES' (uses analyze_document API)\n",
    "# or leave it empty (to use basic detect_document_text API)\n",
    "ocr_service = ocr.OcrService(\n",
    "    region=region,\n",
    "    enhanced_features=['LAYOUT']\n",
    ")\n",
    "\n",
    "# Process document with OCR\n",
    "print(\"\\nProcessing document with OCR...\")\n",
    "start_time = time.time()\n",
    "document = ocr_service.process_document(document)\n",
    "ocr_time = time.time() - start_time\n",
    "\n",
    "print(f\"OCR processing completed in {ocr_time:.2f} seconds\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of pages processed: {document.num_pages}\")\n",
    "\n",
    "# Show pages information\n",
    "print(\"\\nProcessed pages:\")\n",
    "for page_id, page in document.pages.items():\n",
    "    print(f\"Page {page_id}:\")\n",
    "    print(f\"  Image URI: {page.image_uri}\")\n",
    "    print(f\"  Raw Text URI: {page.raw_text_uri}\")\n",
    "    print(f\"  Parsed Text URI: {page.parsed_text_uri}\")\n",
    "print(\"\\nMetering:\")\n",
    "print(json.dumps(document.metering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classify the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that Config specifies => \"classificationMethod\": \"textbasedHolisticClassification\"\n",
    "print(\"*****************************************************************\")\n",
    "print(f'CONFIG classificationMethod: {CONFIG[\"classification\"].get(\"classificationMethod\")}')\n",
    "print(\"*****************************************************************\")\n",
    "\n",
    "# Create classification service with Bedrock backend\n",
    "# The classification method is set in the config\n",
    "classification_service = classification.ClassificationService(\n",
    "    config=CONFIG, \n",
    "    backend=\"bedrock\" \n",
    ")\n",
    "\n",
    "# Classify the document\n",
    "print(\"\\nClassifying document...\")\n",
    "start_time = time.time()\n",
    "document = classification_service.classify_document(document)\n",
    "classification_time = time.time() - start_time\n",
    "print(f\"Classification completed in {classification_time:.2f} seconds\")\n",
    "print(f\"Document status: {document.status.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification results\n",
    "if document.sections:\n",
    "    print(\"\\nDetected sections:\")\n",
    "    for section in document.sections:\n",
    "        print(f\"Section {section.section_id}: {section.classification}\")\n",
    "        print(f\"  Pages: {section.page_ids}\")\n",
    "else:\n",
    "    print(\"\\nNo sections detected\")\n",
    "\n",
    "# Show page classification\n",
    "print(\"\\nPage-level classifications:\")\n",
    "for page_id, page in sorted(document.pages.items()):\n",
    "    print(f\"Page {page_id}: {page.classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Information from Document Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extraction service with Bedrock\n",
    "extraction_service = extraction.ExtractionService(config=CONFIG)\n",
    "\n",
    "print(\"\\nExtracting information from document sections...\")\n",
    "extracted_results = {}\n",
    "\n",
    "n = 3 # Only process first 3 sections to save time\n",
    "# Process each section directly using the section_id\n",
    "for section in document.sections[:n]:  \n",
    "    print(f\"\\nProcessing section {section.section_id} (class: {section.classification})\")\n",
    "    \n",
    "    # Process section directly with the original document\n",
    "    start_time = time.time()\n",
    "    document = extraction_service.process_document_section(\n",
    "        document=document,\n",
    "        section_id=section.section_id\n",
    "    )\n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"Extraction for section {section.section_id} completed in {extraction_time:.2f} seconds\")\n",
    "    \n",
    "print(f\"\\nExtraction for first {n} sections complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nShow extraction results...\\n\")\n",
    "\n",
    "document_dict = document.to_dict()\n",
    "sections_json = json.dumps(document_dict[\"sections\"][:n], indent=2)\n",
    "print(f\"{sections_json}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Document Status Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update document status to COMPLETED\n",
    "document.status = Status.COMPLETED\n",
    "\n",
    "# Display final document state\n",
    "print(\"Final Document State:\")\n",
    "print(f\"Document ID: {document.id}\")\n",
    "print(f\"Status: {document.status.value}\")\n",
    "print(f\"Number of pages: {document.num_pages}\")\n",
    "print(f\"Number of sections: {len(document.sections)}\")\n",
    "\n",
    "# Show document serialization capabilities\n",
    "print(\"\\nDocument can be serialized to JSON:\")\n",
    "document_dict = document.to_dict()\n",
    "document_json = json.dumps(document_dict, indent=2)  \n",
    "print(f\"{document_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Results\n",
    "\n",
    "In this section, we'll demonstrate how to evaluate extraction results by comparing them with expected (ground truth) values. The evaluation process involves:\n",
    "\n",
    "1. Creating a ground truth document with expected values\n",
    "2. Comparing the actual extraction results against expected values\n",
    "3. Calculating metrics (precision, recall, F1 score)\n",
    "4. Generating an evaluation report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create a ground truth document from an existing document and expected results\n",
    "def create_ground_truth_document(source_document, expected_results_dict):\n",
    "    \"\"\"Creates a ground truth document for evaluation from an existing document and expected results.\n",
    "    \n",
    "    Args:\n",
    "        source_document: The original document to copy structure from\n",
    "        expected_results_dict: Dictionary mapping section IDs to expected attribute values\n",
    "        \n",
    "    Returns:\n",
    "        Document: A document with the same structure but with expected results\n",
    "    \"\"\"\n",
    "    # Create a new document with the same core attributes\n",
    "    ground_truth = Document(\n",
    "        id=source_document.id,\n",
    "        input_bucket=source_document.input_bucket,\n",
    "        input_key=source_document.input_key,\n",
    "        output_bucket=source_document.output_bucket,\n",
    "        status=Status.COMPLETED\n",
    "    )\n",
    "    \n",
    "    # Copy sections and add expected result URIs\n",
    "    for section in source_document.sections:\n",
    "        # Create section with same structure\n",
    "        expected_section = Section(\n",
    "            section_id=section.section_id,\n",
    "            classification=section.classification,\n",
    "            confidence=1.0,\n",
    "            page_ids=section.page_ids.copy(),\n",
    "            extraction_result_uri=section.extraction_result_uri  # Copy the URI from actual document\n",
    "        )\n",
    "        ground_truth.sections.append(expected_section)\n",
    "    \n",
    "    # Copy pages\n",
    "    for page_id, page in source_document.pages.items():\n",
    "        ground_truth.pages[page_id] = page\n",
    "    \n",
    "    # Store expected results to S3 for sections that have extraction results\n",
    "    for section_id, expected_data in expected_results_dict.items():\n",
    "        # Find the section in the document\n",
    "        for section in ground_truth.sections:\n",
    "            if section.section_id == section_id and section.extraction_result_uri:\n",
    "                # Load the original extraction result as template\n",
    "                uri = section.extraction_result_uri\n",
    "                bucket, key = parse_s3_uri(uri)\n",
    "                \n",
    "                try:\n",
    "                    # Get the original result structure\n",
    "                    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                    result_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                    \n",
    "                    # Replace the inference_result with our expected data\n",
    "                    if \"inference_result\" in result_data:\n",
    "                        result_data[\"inference_result\"] = expected_data\n",
    "                    else:\n",
    "                        # Or just replace the entire content if no inference_result key\n",
    "                        result_data = expected_data\n",
    "                    \n",
    "                    # Write back to S3 with a different key for expected values\n",
    "                    expected_key = key.replace(\"/result.json\", \"/expected.json\")\n",
    "                    s3_client.put_object(\n",
    "                        Bucket=bucket,\n",
    "                        Key=expected_key,\n",
    "                        Body=json.dumps(result_data).encode('utf-8')\n",
    "                    )\n",
    "                    \n",
    "                    # Update the section's extraction URI to point to our expected data\n",
    "                    section.extraction_result_uri = f\"s3://{bucket}/{expected_key}\"\n",
    "                    print(f\"Stored expected results for section {section_id} at {section.extraction_result_uri}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error storing expected results for section {section_id}: {e}\")\n",
    "    \n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected results for extraction (ground truth)\n",
    "# Customize values to showcase different evaluation methods from CONFIG\n",
    "expected_results = {\n",
    "    \"1\": {  # Section 1 (Letter)\n",
    "        # For sender_name with LLM matching - intentionally create a variant that should match semantically\n",
    "        \"sender_name\": \"William E. Clarke\",  \n",
    "        # For sender_address with LLM matching - formatting differences should still match\n",
    "        \"sender_address\": \"206 maple Street\\nP.O. Box 1056\\nMurray Kentucky 42071-1056\"  \n",
    "    },\n",
    "    \"2\": {  # Section 2 (Form)\n",
    "        # For form_type with FUZZY matching (threshold 0.7) - added qualifier but should still match\n",
    "        \"form_type\": \"LAB SERVICES CONSISTENCY REPORT - Annual Edition\",  \n",
    "        # For form_id with NUMERIC_EXACT - should match\n",
    "        \"form_id\": 2030053328  \n",
    "    },\n",
    "    \"3\": {  # Section 3 (Email)\n",
    "        # For from_address with default matching (LLM) - match\n",
    "        \"from_address\": \"Kelahan, Benjamin\",  \n",
    "        # For to_address field with LLM matching\n",
    "        \"to_address\": \"TI Minnesota, TI New York\"  \n",
    "    }\n",
    "}\n",
    "\n",
    "# Create ground truth document using the helper function\n",
    "expected_document = create_ground_truth_document(document, expected_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation service\n",
    "evaluation_service = evaluation.EvaluationService(config=CONFIG)\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running document evaluation...\")\n",
    "start_time = time.time()\n",
    "document = evaluation_service.evaluate_document(\n",
    "    actual_document=document,\n",
    "    expected_document=expected_document\n",
    ")\n",
    "evaluation_time = time.time() - start_time\n",
    "\n",
    "print(f\"Evaluation completed in {evaluation_time:.2f} seconds\")\n",
    "print(f\"Evaluation report URI: {document.evaluation_report_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show structured evaluation result\n",
    "print(\"Evaluation result object\")\n",
    "if document.evaluation_result:\n",
    "    print(f\"{document.evaluation_result}\")\n",
    "else:\n",
    "    print(\"ERROR.. No evaluation_result found\")\n",
    "\n",
    "# Read the evaluation report from S3\n",
    "print(\"Reading markdown report from S3...\")\n",
    "if document.evaluation_report_uri:\n",
    "    bucket, key = parse_s3_uri(document.evaluation_report_uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    s3_markdown = response['Body'].read().decode('utf-8')\n",
    "    print(f\"Successfully read report from {document.evaluation_report_uri}\")\n",
    "else:\n",
    "    print(\"No evaluation report URI found\")\n",
    "\n",
    "# Display the report in the notebook with proper formatting\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Display the markdown directly from S3 content\n",
    "display(Markdown(s3_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete objects in a bucket\n",
    "def delete_bucket_objects(bucket_name):\n",
    "    try:\n",
    "        # List all objects in the bucket\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            delete_keys = {'Objects': [{'Key': obj['Key']} for obj in response['Contents']]}\n",
    "            s3_client.delete_objects(Bucket=bucket_name, Delete=delete_keys)\n",
    "            print(f\"Deleted all objects in bucket {bucket_name}\")\n",
    "        else:\n",
    "            print(f\"Bucket {bucket_name} is already empty\")\n",
    "            \n",
    "        # Delete bucket\n",
    "        s3_client.delete_bucket(Bucket=bucket_name)\n",
    "        print(f\"Deleted bucket {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning up bucket {bucket_name}: {str(e)}\")\n",
    "\n",
    "# Uncomment the following lines to delete the buckets\n",
    "# print(\"Cleaning up resources...\")\n",
    "# delete_bucket_objects(input_bucket_name)\n",
    "# delete_bucket_objects(output_bucket_name)\n",
    "# print(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the end-to-end processing flow using AWS services and the unified Document model:\n",
    "\n",
    "1. **Document Creation** - Initialize a Document object with input/output locations\n",
    "2. **OCR Processing** - Convert PDF to text using AWS Textract via OcrService\n",
    "3. **Classification** - Identify document types and sections with Claude via ClassificationService\n",
    "4. **Extraction** - Extract structured information with Claude via ExtractionService\n",
    "5. **Evaluation** - Compare extraction results against expected values and generate metrics\n",
    "6. **Document Model** - Document object is consistently used between all services\n",
    "7. **Result Storage** - Extraction results are stored in S3 with URIs tracked in the Document\n",
    "\n",
    "Key benefits of this approach:\n",
    "\n",
    "1. **Modularity** - Each service has a clear responsibility\n",
    "2. **Consistency** - Same data model flows through the entire pipeline\n",
    "3. **Performance** - Focused document pattern reduces resource usage\n",
    "4. **Flexibility** - Support for multiple backends (Bedrock, SageMaker)\n",
    "5. **Maintainability** - Standardized patterns across services\n",
    "6. **Measurement** - Built-in evaluation capabilities to measure accuracy\n",
    "\n",
    "This example uses a  workflow with:\n",
    "1. S3 buckets (created specifically for this demo)\n",
    "2. AWS Textract OCR processing\n",
    "3. LLM inferencing via Bedrock\n",
    "4. A document sample (rvl_cdip_package.pdf)\n",
    "\n",
    "The Evaluation Service specifically provides:\n",
    "\n",
    "1. Multiple evaluation methods (EXACT, NUMERIC_EXACT, FUZZY)\n",
    "2. Per-attribute and document-level metrics\n",
    "3. Markdown and JSON format reporting\n",
    "4. Integration with the Document model\n",
    "5. Configuration-driven evaluation methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
