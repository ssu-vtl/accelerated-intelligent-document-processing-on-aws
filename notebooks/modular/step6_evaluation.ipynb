{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Evaluation\n",
    "\n",
    "This notebook performs evaluation of the entire IDP pipeline results, assessing accuracy and generating final reports.\n",
    "\n",
    "**Inputs:**\n",
    "- Document object with all processing results from Step 5\n",
    "- Evaluation configuration\n",
    "- Optional ground truth data for accuracy assessment\n",
    "\n",
    "**Outputs:**\n",
    "- Comprehensive evaluation report\n",
    "- Accuracy metrics for each processing step\n",
    "- Performance analysis and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Previous Step Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.models import Document, Status\n",
    "from idp_common import evaluation\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('idp_common.evaluation.service').setLevel(logging.INFO)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document from previous step\n",
    "summarization_data_dir = Path(\"data/step5_summarization\")\n",
    "\n",
    "# Load document object\n",
    "document_path = summarization_data_dir / \"document.pkl\"\n",
    "with open(document_path, 'rb') as f:\n",
    "    document = pickle.load(f)\n",
    "\n",
    "# Load configuration\n",
    "config_path = summarization_data_dir / \"config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Load environment info\n",
    "env_path = summarization_data_dir / \"environment.json\"\n",
    "with open(env_path, 'r') as f:\n",
    "    env_info = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['AWS_REGION'] = env_info['region']\n",
    "os.environ['METRIC_NAMESPACE'] = 'IDP-Modular-Pipeline'\n",
    "\n",
    "print(f\"Loaded document: {document.id}\")\n",
    "print(f\"Document status: {document.status.value}\")\n",
    "print(f\"Number of sections: {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"Final processing status: Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Processing Results from All Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from each step for comprehensive evaluation\n",
    "step_results = {}\n",
    "\n",
    "# Step 0: Setup\n",
    "setup_dir = Path(\"data/step0_setup\")\n",
    "if setup_dir.exists():\n",
    "    with open(setup_dir / \"environment.json\", 'r') as f:\n",
    "        step_results['setup'] = json.load(f)\n",
    "\n",
    "# Step 1: OCR\n",
    "ocr_dir = Path(\"data/step1_ocr\")\n",
    "if (ocr_dir / \"ocr_results.json\").exists():\n",
    "    with open(ocr_dir / \"ocr_results.json\", 'r') as f:\n",
    "        step_results['ocr'] = json.load(f)\n",
    "\n",
    "# Step 2: Classification\n",
    "classification_dir = Path(\"data/step2_classification\")\n",
    "if (classification_dir / \"classification_results.json\").exists():\n",
    "    with open(classification_dir / \"classification_results.json\", 'r') as f:\n",
    "        step_results['classification'] = json.load(f)\n",
    "\n",
    "# Step 3: Extraction\n",
    "extraction_dir = Path(\"data/step3_extraction\")\n",
    "if (extraction_dir / \"extraction_summary.json\").exists():\n",
    "    with open(extraction_dir / \"extraction_summary.json\", 'r') as f:\n",
    "        step_results['extraction'] = json.load(f)\n",
    "\n",
    "# Step 4: Assessment\n",
    "assessment_dir = Path(\"data/step4_assessment\")\n",
    "if (assessment_dir / \"assessment_summary.json\").exists():\n",
    "    with open(assessment_dir / \"assessment_summary.json\", 'r') as f:\n",
    "        step_results['assessment'] = json.load(f)\n",
    "\n",
    "# Step 5: Summarization\n",
    "summarization_dir = Path(\"data/step5_summarization\")\n",
    "if (summarization_dir / \"summarization_summary.json\").exists():\n",
    "    with open(summarization_dir / \"summarization_summary.json\", 'r') as f:\n",
    "        step_results['summarization'] = json.load(f)\n",
    "\n",
    "print(f\"Loaded results from {len(step_results)} processing steps: {list(step_results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Evaluation Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract evaluation configuration\n",
    "evaluation_config = CONFIG.get('evaluation', {})\n",
    "print(\"Evaluation Configuration:\")\n",
    "print(f\"Confidence Threshold: {evaluation_config.get('confidence_threshold')}\")\n",
    "print(f\"Accuracy Metrics: {evaluation_config.get('accuracy_metrics', [])}\")\n",
    "print(f\"Generate Reports: {evaluation_config.get('generate_reports')}\")\n",
    "print(f\"Detailed Analysis: {evaluation_config.get('detailed_analysis')}\")\n",
    "\n",
    "# Initialize evaluation service if available\n",
    "try:\n",
    "    evaluation_service = evaluation.EvaluationService(config=CONFIG)\n",
    "    print(\"\\nEvaluation service initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nNote: Evaluation service not available: {e}\")\n",
    "    evaluation_service = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Pipeline Performance Analysis ===\")\n",
    "\n",
    "# Calculate processing times for each step\n",
    "processing_times = {}\n",
    "if 'ocr' in step_results:\n",
    "    processing_times['OCR'] = step_results['ocr'].get('processing_time_seconds', 0)\n",
    "if 'classification' in step_results:\n",
    "    processing_times['Classification'] = step_results['classification'].get('processing_time_seconds', 0)\n",
    "if 'extraction' in step_results:\n",
    "    # Sum up extraction times for all sections\n",
    "    extraction_times = [result.get('processing_time', 0) for result in step_results['extraction'].get('section_results', [])]\n",
    "    processing_times['Extraction'] = sum(extraction_times)\n",
    "if 'assessment' in step_results:\n",
    "    # Sum up assessment times for all sections\n",
    "    assessment_times = [result.get('processing_time', 0) for result in step_results['assessment'].get('assessment_results', [])]\n",
    "    processing_times['Assessment'] = sum(assessment_times)\n",
    "if 'summarization' in step_results:\n",
    "    # Sum up summarization times\n",
    "    section_times = [result.get('processing_time', 0) for result in step_results['summarization'].get('section_results', [])]\n",
    "    doc_time = step_results['summarization'].get('document_processing_time', 0) or 0\n",
    "    processing_times['Summarization'] = sum(section_times) + doc_time\n",
    "\n",
    "# Display processing times\n",
    "print(\"\\nProcessing Times by Step:\")\n",
    "total_time = 0\n",
    "for step, time_seconds in processing_times.items():\n",
    "    print(f\"  {step}: {time_seconds:.2f} seconds\")\n",
    "    total_time += time_seconds\n",
    "\n",
    "print(f\"\\nTotal Processing Time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Calculate throughput metrics\n",
    "num_pages = document.num_pages if hasattr(document, 'num_pages') else 0\n",
    "if total_time > 0 and num_pages > 0:\n",
    "    pages_per_second = num_pages / total_time\n",
    "    seconds_per_page = total_time / num_pages\n",
    "    print(f\"\\nThroughput Metrics:\")\n",
    "    print(f\"  Pages processed: {num_pages}\")\n",
    "    print(f\"  Pages per second: {pages_per_second:.3f}\")\n",
    "    print(f\"  Seconds per page: {seconds_per_page:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Assessment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Quality Assessment Analysis ===\")\n",
    "\n",
    "# Helper function to parse S3 URIs and load JSON\n",
    "def parse_s3_uri(uri):\n",
    "    parts = uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = parts[0]\n",
    "    key = \"/\".join(parts[1:])\n",
    "    return bucket, key\n",
    "\n",
    "def load_json_from_s3(uri):\n",
    "    s3_client = boto3.client('s3')\n",
    "    bucket, key = parse_s3_uri(uri)\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(content)\n",
    "\n",
    "# Analyze confidence scores from assessment results\n",
    "confidence_scores = []\n",
    "confidence_threshold = float(evaluation_config.get('confidence_threshold', 0.8))\n",
    "\n",
    "if document.sections:\n",
    "    print(f\"\\nAnalyzing confidence scores (threshold: {confidence_threshold}):\")\n",
    "    \n",
    "    for section in document.sections:\n",
    "        if hasattr(section, 'extraction_result_uri') and section.extraction_result_uri:\n",
    "            try:\n",
    "                # Load extraction results with assessment data\n",
    "                extraction_data = load_json_from_s3(section.extraction_result_uri)\n",
    "                explainability_info = extraction_data.get('explainability_info', [])\n",
    "                \n",
    "                if explainability_info:\n",
    "                    assessments = explainability_info[0] if isinstance(explainability_info, list) else explainability_info\n",
    "                    section_scores = []\n",
    "                    \n",
    "                    for attr_name, assessment_data in assessments.items():\n",
    "                        score = assessment_data.get('confidence_score')\n",
    "                        if isinstance(score, (int, float)):\n",
    "                            confidence_scores.append(score)\n",
    "                            section_scores.append(score)\n",
    "                    \n",
    "                    if section_scores:\n",
    "                        avg_score = sum(section_scores) / len(section_scores)\n",
    "                        above_threshold = sum(1 for s in section_scores if s >= confidence_threshold)\n",
    "                        print(f\"  Section {section.section_id} ({section.classification}):\")\n",
    "                        print(f\"    Average confidence: {avg_score:.3f}\")\n",
    "                        print(f\"    Attributes above threshold: {above_threshold}/{len(section_scores)}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  Error analyzing section {section.section_id}: {e}\")\n",
    "\n",
    "# Overall confidence statistics\n",
    "if confidence_scores:\n",
    "    avg_confidence = sum(confidence_scores) / len(confidence_scores)\n",
    "    min_confidence = min(confidence_scores)\n",
    "    max_confidence = max(confidence_scores)\n",
    "    above_threshold_count = sum(1 for score in confidence_scores if score >= confidence_threshold)\n",
    "    \n",
    "    print(f\"\\nOverall Confidence Statistics:\")\n",
    "    print(f\"  Total attributes assessed: {len(confidence_scores)}\")\n",
    "    print(f\"  Average confidence: {avg_confidence:.3f}\")\n",
    "    print(f\"  Min confidence: {min_confidence:.3f}\")\n",
    "    print(f\"  Max confidence: {max_confidence:.3f}\")\n",
    "    print(f\"  Above threshold ({confidence_threshold}): {above_threshold_count}/{len(confidence_scores)} ({100*above_threshold_count/len(confidence_scores):.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo confidence scores available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary by Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Results Summary by Processing Step ===\")\n",
    "\n",
    "# OCR Results\n",
    "if 'ocr' in step_results:\n",
    "    ocr_data = step_results['ocr']\n",
    "    print(f\"\\nüìÑ OCR Processing:\")\n",
    "    print(f\"  Pages processed: {ocr_data.get('num_pages', 0)}\")\n",
    "    print(f\"  Processing time: {ocr_data.get('processing_time_seconds', 0):.2f}s\")\n",
    "    print(f\"  Features used: {', '.join(ocr_data.get('features_used', []))}\")\n",
    "    if 'metering' in ocr_data:\n",
    "        print(f\"  Pages metered: {ocr_data['metering'].get('pages_metered', 0)}\")\n",
    "\n",
    "# Classification Results\n",
    "if 'classification' in step_results:\n",
    "    class_data = step_results['classification']\n",
    "    print(f\"\\nüè∑Ô∏è Classification:\")\n",
    "    print(f\"  Sections identified: {class_data.get('num_sections', 0)}\")\n",
    "    print(f\"  Processing time: {class_data.get('processing_time_seconds', 0):.2f}s\")\n",
    "    print(f\"  Method: {class_data.get('classification_method')}\")\n",
    "    print(f\"  Model: {class_data.get('model_used')}\")\n",
    "    \n",
    "    # Show section types\n",
    "    sections = class_data.get('sections', [])\n",
    "    if sections:\n",
    "        section_types = {}\n",
    "        for section in sections:\n",
    "            section_type = section.get('classification', 'unknown')\n",
    "            section_types[section_type] = section_types.get(section_type, 0) + 1\n",
    "        print(f\"  Document types found: {dict(section_types)}\")\n",
    "\n",
    "# Extraction Results\n",
    "if 'extraction' in step_results:\n",
    "    extract_data = step_results['extraction']\n",
    "    print(f\"\\nüìä Extraction:\")\n",
    "    print(f\"  Sections processed: {extract_data.get('sections_processed', 0)}/{extract_data.get('total_sections', 0)}\")\n",
    "    print(f\"  Sections with results: {sum(1 for s in extract_data.get('sections_with_extractions', []) if s.get('has_results'))}\")\n",
    "    print(f\"  Model: {extract_data.get('model_used')}\")\n",
    "\n",
    "# Assessment Results\n",
    "if 'assessment' in step_results:\n",
    "    assess_data = step_results['assessment']\n",
    "    print(f\"\\nüîç Assessment:\")\n",
    "    print(f\"  Sections assessed: {assess_data.get('sections_assessed', 0)}/{assess_data.get('total_sections_with_extractions', 0)}\")\n",
    "    print(f\"  Model: {assess_data.get('model_used')}\")\n",
    "    print(f\"  Default threshold: {assess_data.get('default_confidence_threshold')}\")\n",
    "\n",
    "# Summarization Results\n",
    "if 'summarization' in step_results:\n",
    "    summ_data = step_results['summarization']\n",
    "    print(f\"\\nüìù Summarization:\")\n",
    "    print(f\"  Sections processed: {summ_data.get('sections_processed', 0)}/{summ_data.get('total_sections', 0)}\")\n",
    "    sections_with_summaries = sum(1 for s in summ_data.get('sections_with_summaries', []) if s.get('has_summary'))\n",
    "    print(f\"  Sections with summaries: {sections_with_summaries}\")\n",
    "    print(f\"  Document summary: {'Yes' if summ_data.get('document_summary_uri') else 'No'}\")\n",
    "    print(f\"  Output formats: {', '.join(summ_data.get('output_formats', []))}\")\n",
    "    print(f\"  Model: {summ_data.get('model_used')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive evaluation report\n",
    "evaluation_report = {\n",
    "    'document_id': document.id,\n",
    "    'evaluation_timestamp': datetime.now().isoformat(),\n",
    "    'pipeline_version': 'modular-v1.0',\n",
    "    \n",
    "    # Document overview\n",
    "    'document_overview': {\n",
    "        'total_pages': getattr(document, 'num_pages', 0),\n",
    "        'total_sections': len(document.sections) if document.sections else 0,\n",
    "        'input_file': f\"s3://{document.input_bucket}/{document.input_key}\",\n",
    "        'final_status': document.status.value\n",
    "    },\n",
    "    \n",
    "    # Performance metrics\n",
    "    'performance_metrics': {\n",
    "        'total_processing_time_seconds': total_time,\n",
    "        'processing_times_by_step': processing_times,\n",
    "        'throughput': {\n",
    "            'pages_per_second': pages_per_second if 'pages_per_second' in locals() else None,\n",
    "            'seconds_per_page': seconds_per_page if 'seconds_per_page' in locals() else None\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Quality metrics\n",
    "    'quality_metrics': {\n",
    "        'confidence_analysis': {\n",
    "            'total_attributes_assessed': len(confidence_scores),\n",
    "            'average_confidence': sum(confidence_scores) / len(confidence_scores) if confidence_scores else None,\n",
    "            'min_confidence': min(confidence_scores) if confidence_scores else None,\n",
    "            'max_confidence': max(confidence_scores) if confidence_scores else None,\n",
    "            'above_threshold_percentage': (100 * sum(1 for score in confidence_scores if score >= confidence_threshold) / len(confidence_scores)) if confidence_scores else None,\n",
    "            'confidence_threshold': confidence_threshold\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Step-by-step results\n",
    "    'step_results': step_results,\n",
    "    \n",
    "    # Configuration used\n",
    "    'configuration_summary': {\n",
    "        'ocr_features': CONFIG.get('ocr', {}).get('features', []),\n",
    "        'classification_method': CONFIG.get('classification', {}).get('classificationMethod'),\n",
    "        'models_used': {\n",
    "            'classification': CONFIG.get('classification', {}).get('model'),\n",
    "            'extraction': CONFIG.get('extraction', {}).get('model'),\n",
    "            'assessment': CONFIG.get('assessment', {}).get('model'),\n",
    "            'summarization': CONFIG.get('summarization', {}).get('model')\n",
    "        },\n",
    "        'document_classes': [cls['name'] for cls in CONFIG.get('classes', [])]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Generated comprehensive evaluation report\")\n",
    "print(f\"Report contains {len(evaluation_report)} main sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory for this step\n",
    "data_dir = Path(\"data/step6_evaluation\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Update document status to completed\n",
    "document.status = Status.COMPLETED\n",
    "\n",
    "# Save final document object\n",
    "document_path = data_dir / \"document.pkl\"\n",
    "with open(document_path, 'wb') as f:\n",
    "    pickle.dump(document, f)\n",
    "\n",
    "# Save configuration (pass through)\n",
    "config_path = data_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# Save environment info (pass through)\n",
    "env_path = data_dir / \"environment.json\"\n",
    "with open(env_path, 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "# Save comprehensive evaluation report\n",
    "evaluation_report_path = data_dir / \"evaluation_report.json\"\n",
    "with open(evaluation_report_path, 'w') as f:\n",
    "    json.dump(evaluation_report, f, indent=2)\n",
    "\n",
    "# Save processing times summary\n",
    "processing_summary = {\n",
    "    'total_time_seconds': total_time,\n",
    "    'step_times': processing_times,\n",
    "    'throughput_metrics': {\n",
    "        'pages_processed': getattr(document, 'num_pages', 0),\n",
    "        'pages_per_second': pages_per_second if 'pages_per_second' in locals() else None,\n",
    "        'seconds_per_page': seconds_per_page if 'seconds_per_page' in locals() else None\n",
    "    }\n",
    "}\n",
    "\n",
    "processing_summary_path = data_dir / \"processing_summary.json\"\n",
    "with open(processing_summary_path, 'w') as f:\n",
    "    json.dump(processing_summary, f, indent=2)\n",
    "\n",
    "print(f\"Saved final document to: {document_path}\")\n",
    "print(f\"Saved configuration to: {config_path}\")\n",
    "print(f\"Saved environment info to: {env_path}\")\n",
    "print(f\"Saved evaluation report to: {evaluation_report_path}\")\n",
    "print(f\"Saved processing summary to: {processing_summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Step 6: Evaluation Complete ===\")\n",
    "print(f\"‚úÖ Document processed: {document.id}\")\n",
    "print(f\"‚úÖ Final status: {document.status.value}\")\n",
    "print(f\"‚úÖ Total processing time: {total_time:.2f} seconds\")\n",
    "print(f\"‚úÖ Pages processed: {getattr(document, 'num_pages', 0)}\")\n",
    "print(f\"‚úÖ Sections identified: {len(document.sections) if document.sections else 0}\")\n",
    "print(f\"‚úÖ Average confidence: {sum(confidence_scores)/len(confidence_scores):.3f}\" if confidence_scores else \"‚úÖ No confidence scores available\")\n",
    "print(f\"‚úÖ Evaluation report saved to: data/step6_evaluation/\")\n",
    "\n",
    "print(\"\\n=== üéâ MODULAR IDP PIPELINE COMPLETE! üéâ ===\")\n",
    "print(\"\\nAll steps have been successfully executed:\")\n",
    "print(\"  0Ô∏è‚É£ Setup - Environment and document initialization\")\n",
    "print(\"  1Ô∏è‚É£ OCR - Text and image extraction from PDF\")\n",
    "print(\"  2Ô∏è‚É£ Classification - Document type identification\")\n",
    "print(\"  3Ô∏è‚É£ Extraction - Structured data extraction\")\n",
    "print(\"  4Ô∏è‚É£ Assessment - Confidence evaluation\")\n",
    "print(\"  5Ô∏è‚É£ Summarization - Content summarization\")\n",
    "print(\"  6Ô∏è‚É£ Evaluation - Final analysis and reporting\")\n",
    "\n",
    "print(\"\\nüìä Key Benefits of Modular Approach:\")\n",
    "print(\"  ‚Ä¢ Independent step execution and testing\")\n",
    "print(\"  ‚Ä¢ Modular configuration management\")\n",
    "print(\"  ‚Ä¢ Step-by-step result persistence\")\n",
    "print(\"  ‚Ä¢ Easy experimentation with different configurations\")\n",
    "print(\"  ‚Ä¢ Comprehensive evaluation and reporting\")\n",
    "\n",
    "print(\"\\nüîß Next Steps for Experimentation:\")\n",
    "print(\"  ‚Ä¢ Modify config files to try different models or parameters\")\n",
    "print(\"  ‚Ä¢ Add new document classes in classes.yaml\")\n",
    "print(\"  ‚Ä¢ Run individual steps with different
