{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova Lite Fine-tuning Dataset Preparation\n",
    "\n",
    "This notebook prepares the RVL-CDIP dataset for fine-tuning a Nova Lite model. We will:\n",
    "1. Load the dataset\n",
    "2. Sample 500 data samples per label\n",
    "3. Save the images in PNG format\n",
    "4. Create a train.jsonl file in the required format\n",
    "5. Upload both the images and the train.jsonl file to the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import uuid\n",
    "from tqdm import tqdm  # Use standard tqdm instead of tqdm.notebook\n",
    "import io\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "import time\n",
    "from functools import partial\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 bucket and directory\n",
    "bucket_name = \"test-idp-finetuning-data-us-east-1\"\n",
    "directory = \"rvl-cdip-sampled\"\n",
    "SAMPLES_PER_LABEL = 10\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mapping according to the requirements\n",
    "label_mapping = {\n",
    "    0: \"advertissement\",\n",
    "    1: \"budget\",\n",
    "    2: \"email\",\n",
    "    3: \"file_folder\",\n",
    "    4: \"form\",\n",
    "    5: \"handwritten\",\n",
    "    6: \"invoice\",\n",
    "    7: \"letter\",\n",
    "    8: \"memo\",\n",
    "    9: \"news_article\",\n",
    "    10: \"presentation\",\n",
    "    11: \"questionnaire\",\n",
    "    12: \"resume\",\n",
    "    13: \"scientific_publication\",\n",
    "    14: \"scientific_report\",\n",
    "    15: \"specification\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"chainyo/rvl-cdip\")\n",
    "print(f\"Dataset loaded: {ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the types of the image and label fields\n",
    "print(f\"Image type: {type(ds['train'][0]['image'])}, Label type: {type(ds['train'][0]['label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique labels\n",
    "unique_labels = np.unique(ds[\"train\"][\"label\"])\n",
    "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
    "print(f\"Labels: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label names if available\n",
    "if hasattr(ds['train'].features['label'], 'names'):\n",
    "    label_names = ds['train'].features['label'].names\n",
    "    print(f\"Label names: {label_names}\")\n",
    "else:\n",
    "    print(\"Label names not available\")\n",
    "    label_names = [f\"class_{i}\" for i in unique_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint file path\n",
    "checkpoint_file = \"sampled_data_checkpoint.pkl\"\n",
    "\n",
    "# Function to save sampled data to checkpoint file\n",
    "def save_checkpoint(data):\n",
    "    print(f\"Saving checkpoint to {checkpoint_file}...\")\n",
    "    # Create a list of dictionaries that can be serialized\n",
    "    serializable_data = []\n",
    "    for sample in data:\n",
    "        # Convert PIL image to bytes for serialization\n",
    "        sample_dict = dict(sample)\n",
    "        if \"image\" in sample_dict:\n",
    "            img_bytes = io.BytesIO()\n",
    "            sample[\"image\"].save(img_bytes, format=\"PNG\")\n",
    "            sample_dict[\"image_bytes\"] = img_bytes.getvalue()\n",
    "            del sample_dict[\"image\"]\n",
    "        serializable_data.append(sample_dict)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(checkpoint_file, \"wb\") as f:\n",
    "        import pickle\n",
    "        pickle.dump(serializable_data, f)\n",
    "    print(f\"Checkpoint saved with {len(data)} samples\")\n",
    "\n",
    "# Function to load sampled data from checkpoint file\n",
    "def load_checkpoint():\n",
    "    print(f\"Loading data from checkpoint file {checkpoint_file}...\")\n",
    "    try:\n",
    "        with open(checkpoint_file, \"rb\") as f:\n",
    "            import pickle\n",
    "            serializable_data = pickle.load(f)\n",
    "        \n",
    "        # Convert back to samples with PIL images\n",
    "        loaded_data = []\n",
    "        for sample_dict in serializable_data:\n",
    "            if \"image_bytes\" in sample_dict:\n",
    "                # Convert bytes back to PIL image\n",
    "                img_bytes = sample_dict[\"image_bytes\"]\n",
    "                sample_dict[\"image\"] = Image.open(io.BytesIO(img_bytes))\n",
    "                del sample_dict[\"image_bytes\"]\n",
    "            loaded_data.append(sample_dict)\n",
    "        \n",
    "        print(f\"Successfully loaded {len(loaded_data)} samples from checkpoint\")\n",
    "        return loaded_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process a single label\n",
    "def process_label(label, samples_per_label=500):\n",
    "    label_name = label_names[label]\n",
    "    result_samples = []\n",
    "    \n",
    "    # Get indices for this label\n",
    "    indices = [i for i, sample in enumerate(ds[\"train\"]) if sample[\"label\"] == label]\n",
    "    \n",
    "    # If there are fewer than samples_per_label samples, use all of them\n",
    "    if len(indices) <= samples_per_label:\n",
    "        sampled_indices = indices\n",
    "        message = f\"Label {label} ({label_name}): Using all {len(indices)} samples\"\n",
    "    else:\n",
    "        # Randomly sample samples_per_label indices\n",
    "        sampled_indices = np.random.choice(indices, samples_per_label, replace=False)\n",
    "        # Convert numpy.int64 to regular Python int to avoid indexing issues\n",
    "        sampled_indices = [int(idx) for idx in sampled_indices]\n",
    "        message = f\"Label {label} ({label_name}): Sampled {samples_per_label} out of {len(indices)} samples\"\n",
    "    \n",
    "    # Get the actual samples\n",
    "    for idx in sampled_indices:\n",
    "        result_samples.append(ds[\"train\"][idx])\n",
    "    \n",
    "    return result_samples, message\n",
    "\n",
    "# Check if checkpoint file exists\n",
    "if os.path.exists(checkpoint_file):\n",
    "    # Try to load from checkpoint\n",
    "    sampled_data = load_checkpoint()\n",
    "    if sampled_data is not None:\n",
    "        print(f\"Successfully loaded {len(sampled_data)} samples from checkpoint\")\n",
    "    else:\n",
    "        # If loading failed, run the sampling process\n",
    "        print(\"Failed to load from checkpoint. Running sampling process...\")\n",
    "        sampled_data = None\n",
    "else:\n",
    "    print(\"No checkpoint file found. Running sampling process...\")\n",
    "    sampled_data = None\n",
    "\n",
    "# If we couldn't load from checkpoint, run the sampling process\n",
    "if sampled_data is None:\n",
    "    # Parallel sampling approach\n",
    "    samples_per_label = SAMPLES_PER_LABEL\n",
    "    sampled_data = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Starting parallel sampling of data...\")\n",
    "\n",
    "    # Set the number of workers based on CPU cores\n",
    "    max_workers = min(16, os.cpu_count())  # Use up to 1 worker per CPU core, but no more than 16\n",
    "    print(f\"Using {max_workers} workers for parallel sampling\")\n",
    "\n",
    "    # Process labels in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit sampling tasks for each label\n",
    "        future_to_label = {executor.submit(process_label, label, samples_per_label): label \n",
    "                          for label in unique_labels}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_label), total=len(future_to_label), desc=\"Sampling labels\"):\n",
    "            label = future_to_label[future]\n",
    "            try:\n",
    "                samples, message = future.result()\n",
    "                sampled_data.extend(samples)\n",
    "                print(message)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing label {label}: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Parallel sampling completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Save the sampled data to checkpoint file\n",
    "    save_checkpoint(sampled_data)\n",
    "\n",
    "print(f\"Total sampled data: {len(sampled_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local directory to temporarily store the images\n",
    "os.makedirs(\"temp_images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save image and upload to S3\n",
    "def save_and_upload_image(image, label, index):\n",
    "    # Generate a unique filename\n",
    "    filename = f\"{label_names[label]}_{index}_{uuid.uuid4()}.png\"\n",
    "    # Replace spaces with underscores in the filename\n",
    "    filename = filename.replace(' ', '_')\n",
    "    local_path = os.path.join(\"temp_images\", filename)\n",
    "    s3_path = f\"{directory}/images/{filename}\"\n",
    "    \n",
    "    # Save the image locally\n",
    "    image.save(local_path, format=\"PNG\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3.upload_file(local_path, bucket_name, s3_path)\n",
    "    \n",
    "    # Remove the local file\n",
    "    os.remove(local_path)\n",
    "    \n",
    "    return f\"s3://{bucket_name}/{s3_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt and task prompt for document classification\n",
    "system_prompt = \"You are a document classification expert who can analyze and identify document types from images. Your task is to determine the document type based on its visual appearance, layout, and content, using the provided document type definitions. Your output must be valid JSON according to the requested format.\"\n",
    "\n",
    "task_prompt_template = \"\"\"The <document-types> XML tags contain a markdown table of known document types for detection.\n",
    "<document-types>\n",
    "| Document Type | Description |\n",
    "|---------------|-------------|\n",
    "| advertissement | Marketing or promotional material with graphics, product information, and calls to action |\n",
    "| budget | Financial document with numerical data, calculations, and monetary figures organized in tables or lists |\n",
    "| email | Electronic correspondence with header information, sender/recipient details, and message body |\n",
    "| file_folder | Document with tabs, labels, or folder-like structure used for organizing other documents |\n",
    "| form | Structured document with fields to be filled in, checkboxes, or data collection sections |\n",
    "| handwritten | Document containing primarily handwritten text rather than typed or printed content |\n",
    "| invoice | Billing document with itemized list of goods/services, costs, payment terms, and company information |\n",
    "| letter | Formal correspondence with letterhead, date, recipient address, salutation, and signature |\n",
    "| memo | Internal business communication with brief, direct message and minimal formatting |\n",
    "| news_article | Journalistic content with headlines, columns, and reporting on events or topics |\n",
    "| presentation | Slides or visual aids with bullet points, graphics, and concise information for display |\n",
    "| questionnaire | Document with series of questions designed to collect information from respondents |\n",
    "| resume | Professional summary of a person's work experience, skills, and qualifications |\n",
    "| scientific_publication | Academic paper with abstract, methodology, results, and references in formal structure |\n",
    "| scientific_report | Technical document presenting research findings, data, and analysis in structured format |\n",
    "| specification | Detailed technical document outlining requirements, standards, or procedures |\n",
    "</document-types>\n",
    "\n",
    "CRITICAL: You must ONLY use document types explicitly listed in the <document-types> section. Do not create, invent, or use any document type not found in this list. If a document doesn't clearly match any listed type, assign it to the most similar listed type.\n",
    "\n",
    "Follow these steps when classifying the document image:\n",
    "1. Examine the document image carefully, noting its layout, content, and visual characteristics.\n",
    "2. Identify visual cues that indicate the document type (e.g., tables for budgets, letterhead for letters).\n",
    "3. Match the document with one of the document types from the provided list ONLY.\n",
    "4. Before finalizing, verify that your selected document type exactly matches one from the <document-types> list.\n",
    "\n",
    "Return your response as valid JSON according to this format:\n",
    "```json\n",
    "{\"type\": \"document_type_name\"}\n",
    "```\n",
    "where document_type_name is one of the document types listed in the <document-types> section.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single sample\n",
    "def process_sample(sample, index, account_id=None):\n",
    "    image = sample[\"image\"]\n",
    "    label = sample[\"label\"]\n",
    "    \n",
    "    # Save and upload the image\n",
    "    s3_uri = save_and_upload_image(image, label, index)\n",
    "    \n",
    "    # Store the original sample and S3 URI\n",
    "    updated_sample = dict(sample)\n",
    "    updated_sample[\"s3_uri\"] = s3_uri\n",
    "    \n",
    "    # Get the mapped label\n",
    "    mapped_label = label_mapping[label]\n",
    "    \n",
    "    # Create the JSONL record\n",
    "    record = {\n",
    "        \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "        \"system\": [{\n",
    "            \"text\": system_prompt\n",
    "        }],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": task_prompt_template\n",
    "                    },\n",
    "                    {\n",
    "                        \"image\": {\n",
    "                            \"format\": \"png\",\n",
    "                            \"source\": {\n",
    "                                \"s3Location\": {\n",
    "                                    \"uri\": s3_uri,\n",
    "                                    \"bucketOwner\": account_id if account_id else boto3.client('sts').get_caller_identity().get('Account')\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\n",
    "                    \"text\": f\"\"\"```json\n",
    "                    {{\"type\": {mapped_label}}}\n",
    "                    ```\n",
    "                    \"\"\"\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return updated_sample, record\n",
    "\n",
    "# Process the sampled data in parallel\n",
    "updated_samples = []\n",
    "jsonl_records = []\n",
    "\n",
    "# Get AWS account ID once to avoid repeated calls\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Set the number of workers based on CPU cores\n",
    "max_workers = min(32, os.cpu_count() * 2)  # Use up to 2 workers per CPU core, but no more than 32\n",
    "print(f\"Using {max_workers} workers for parallel processing\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a partial function with the account_id\n",
    "process_sample_with_account = partial(process_sample, account_id=account_id)\n",
    "\n",
    "# Process samples in parallel with standard tqdm\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit all tasks and create a dictionary to track them\n",
    "    future_to_idx = {executor.submit(process_sample_with_account, sample, i): i \n",
    "                    for i, sample in enumerate(sampled_data)}\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(future_to_idx), total=len(future_to_idx)):\n",
    "        try:\n",
    "            updated_sample, record = future.result()\n",
    "            updated_samples.append(updated_sample)\n",
    "            jsonl_records.append(record)\n",
    "        except Exception as e:\n",
    "            idx = future_to_idx[future]\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Parallel processing completed in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Hugging Face dataset with the updated samples\n",
    "updated_ds = Dataset.from_list(updated_samples)\n",
    "print(f\"Updated dataset created with {len(updated_ds)} samples\")\n",
    "print(f\"Dataset features: {updated_ds.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset locally\n",
    "updated_ds_path = \"rvl_cdip_with_s3_uris\"\n",
    "if len(updated_ds) > 0:  # Only save if we have samples\n",
    "    updated_ds.save_to_disk(updated_ds_path)\n",
    "    print(f\"Updated dataset saved locally to {updated_ds_path}\")\n",
    "else:\n",
    "    print(\"No samples to save locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the updated dataset to S3\n",
    "# First, create a JSON file with the dataset\n",
    "updated_ds_json_path = \"updated_dataset.json\"\n",
    "with open(updated_ds_json_path, \"w\") as f:\n",
    "    json_data = []\n",
    "    for sample in updated_ds:\n",
    "        # Convert PIL image to base64 for JSON serialization\n",
    "        sample_dict = dict(sample)\n",
    "        if \"image\" in sample_dict:\n",
    "            # Remove the image to avoid serialization issues\n",
    "            del sample_dict[\"image\"]\n",
    "        json_data.append(sample_dict)\n",
    "    json.dump(json_data, f)\n",
    "\n",
    "# Upload the JSON file to S3\n",
    "s3.upload_file(updated_ds_json_path, bucket_name, f\"{directory}/updated_dataset.json\")\n",
    "print(f\"Updated dataset uploaded to S3: s3://{bucket_name}/{directory}/updated_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets (90% train, 10% validation)\n",
    "np.random.shuffle(jsonl_records)\n",
    "split_idx = int(len(jsonl_records) * 0.9)\n",
    "train_records = jsonl_records[:split_idx]\n",
    "validation_records = jsonl_records[split_idx:]\n",
    "\n",
    "print(f\"Training records: {len(train_records)}\")\n",
    "print(f\"Validation records: {len(validation_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the JSONL files locally\n",
    "train_jsonl_path = \"train.jsonl\"\n",
    "validation_jsonl_path = \"validation.jsonl\"\n",
    "\n",
    "with open(train_jsonl_path, \"w\") as f:\n",
    "    for record in train_records:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "with open(validation_jsonl_path, \"w\") as f:\n",
    "    for record in validation_records:\n",
    "        f.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the JSONL files to S3\n",
    "s3.upload_file(train_jsonl_path, bucket_name, f\"{directory}/train.jsonl\")\n",
    "s3.upload_file(validation_jsonl_path, bucket_name, f\"{directory}/validation.jsonl\")\n",
    "\n",
    "print(f\"Train JSONL uploaded to s3://{bucket_name}/{directory}/train.jsonl\")\n",
    "print(f\"Validation JSONL uploaded to s3://{bucket_name}/{directory}/validation.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "os.remove(train_jsonl_path)\n",
    "os.remove(validation_jsonl_path)\n",
    "os.remove(updated_ds_json_path)\n",
    "shutil.rmtree(\"temp_images\")\n",
    "\n",
    "print(\"Local files cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset preparation complete!\")\n",
    "print(f\"Total samples: {len(jsonl_records)}\")\n",
    "print(f\"Training samples: {len(train_records)}\")\n",
    "print(f\"Validation samples: {len(validation_records)}\")\n",
    "print(f\"Data uploaded to s3://{bucket_name}/{directory}/\")\n",
    "print(f\"Updated dataset with S3 URIs saved locally to {updated_ds_path}\")\n",
    "print(f\"Updated dataset with S3 URIs uploaded to s3://{bucket_name}/{directory}/updated_dataset.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiic-idp-accelerator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
