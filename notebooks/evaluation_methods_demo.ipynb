{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Methods Demonstration\n",
    "\n",
    "This notebook demonstrates the various evaluation methods available in the IDP library for comparing expected values with actual extraction results. It covers:\n",
    "\n",
    "1. All evaluation methods with both match and no-match scenarios\n",
    "2. Threshold testing for applicable methods\n",
    "3. Edge cases:\n",
    "   - Attribute not found in actual results\n",
    "   - Attribute not found in expected results\n",
    "   - Attribute not found in either actual or expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: idp_common 0.2.19\n",
      "Uninstalling idp_common-0.2.19:\n",
      "  Successfully uninstalled idp_common-0.2.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First uninstall existing package (to ensure we get the latest version)\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package with all components in development mode\n",
    "%pip install -q -e \"../lib/idp_common_pkg[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Add parent directory to path to import the library\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.evaluation.models import EvaluationMethod\n",
    "from idp_common.evaluation.comparator import compare_values\n",
    "from idp_common.evaluation.service import EvaluationService\n",
    "from idp_common.models import Document, Section, Status\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Comparing Individual Values with Different Methods\n",
    "\n",
    "We'll test each evaluation method with matching and non-matching examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_comparison(method: EvaluationMethod, expected: Any, actual: Any, \n",
    "                    threshold: float = 0.8, document_class: str = \"TestDoc\",\n",
    "                    attr_name: str = \"test_attr\", attr_description: str = \"Test attribute\"):\n",
    "    \"\"\"Test a comparison method and print results.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Method: {method.name}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Actual: {actual}\")\n",
    "    \n",
    "    if method in [EvaluationMethod.FUZZY, EvaluationMethod.BERT]:\n",
    "        print(f\"Threshold: {threshold}\")\n",
    "    \n",
    "    # Set up LLM config for the LLM method\n",
    "    llm_config = None\n",
    "    if method == EvaluationMethod.LLM:\n",
    "        llm_config = {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator that helps determine if the predicted and expected values match for document attribute extraction.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.  Here's the exact format:\n",
    "{\n",
    "  \"match\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"Your explanation here\"\n",
    "}\n",
    "\"\"\"\n",
    "        }\n",
    "    \n",
    "    # Perform the comparison\n",
    "    matched, score, reason = compare_values(\n",
    "        expected=expected,\n",
    "        actual=actual,\n",
    "        method=method,\n",
    "        threshold=threshold,\n",
    "        document_class=document_class,\n",
    "        attr_name=attr_name,\n",
    "        attr_description=attr_description,\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "    \n",
    "    print(f\"Matched: {matched}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    if reason:\n",
    "        print(f\"Reason: {reason}\")\n",
    "        \n",
    "    return matched, score, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: EXACT Method\n",
    "Testing exact string matching with both match and non-match cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: Account #12345\n",
      "Actual: Account #12345\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: Account #12345\n",
      "Actual: Account #12346\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: Account Number: 12345\n",
      "Actual: account number 12345\n",
      "Matched: True\n",
      "Score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 1.0, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXACT method - Match\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account #12345\", \"Account #12345\")\n",
    "\n",
    "# EXACT method - No match\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account #12345\", \"Account #12346\")\n",
    "\n",
    "# EXACT method - Match with different casing and punctuation\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account Number: 12345\", \"account number 12345\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: NUMERIC_EXACT Method\n",
    "Testing numeric comparison with different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: $1,250.00\n",
      "Actual: 1250\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: $1,250.00\n",
      "Actual: 1251\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: (1,250.00)\n",
      "Actual: -1250\n",
      "Matched: False\n",
      "Score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0.0, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NUMERIC_EXACT method - Match\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"$1,250.00\", 1250)\n",
    "\n",
    "# NUMERIC_EXACT method - No match\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"$1,250.00\", 1251)\n",
    "\n",
    "# NUMERIC_EXACT method - Match with different formats\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"(1,250.00)\", \"-1250\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: FUZZY Method\n",
    "Testing fuzzy comparison with different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: John A. Smith\n",
      "Actual: John Smith\n",
      "Threshold: 0.8\n",
      "Matched: True\n",
      "Score: 0.8333333333333334\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: John A. Smith\n",
      "Actual: John Simpson\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.41666666666666663\n",
      "With threshold=0.6: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: John Alexander Smith\n",
      "Actual: Jane Marie Johnson\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.15000000000000002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0.15000000000000002, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUZZY method - High match\n",
    "test_comparison(EvaluationMethod.FUZZY, \"John A. Smith\", \"John Smith\", threshold=0.8)\n",
    "\n",
    "# FUZZY method - Medium match \n",
    "matched, score, _ = test_comparison(EvaluationMethod.FUZZY, \"John A. Smith\", \"John Simpson\", threshold=0.8)\n",
    "print(f\"With threshold=0.6: {score >= 0.6}\")\n",
    "\n",
    "# FUZZY method - Low match\n",
    "test_comparison(EvaluationMethod.FUZZY, \"John Alexander Smith\", \"Jane Marie Johnson\", threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: HUNGARIAN Method\n",
    "Testing list comparison using the Hungarian algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $200']\n",
      "Actual: ['Deposit: $500', 'Transfer: $200', 'Withdrawal: $150']\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $200']\n",
      "Actual: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $210']\n",
      "Matched: False\n",
      "Score: 0.6666666666666666\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $200']\n",
      "Actual: ['Deposit: $500', 'Withdrawal: $150']\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: Single item\n",
      "Actual: Single item\n",
      "Matched: True\n",
      "Score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 1.0, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HUNGARIAN method - Full match\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Transfer: $200\", \"Withdrawal: $150\"]\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, expected_list, actual_list)\n",
    "\n",
    "# HUNGARIAN method - Partial match\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $210\"]\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, expected_list, actual_list)\n",
    "\n",
    "# HUNGARIAN method - Different number of items\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Withdrawal: $150\"]\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, expected_list, actual_list)\n",
    "\n",
    "# HUNGARIAN method - Non-list values (should convert to list)\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, \"Single item\", \"Single item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: LLM Method\n",
    "Testing semantic comparison using a Large Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\n",
      "Actual: Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'dfd81eee-70e6-4b70-896e-0a7b7c62c24f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 19:51:50 GMT', 'content-type': 'application/json', 'content-length': '522', 'connection': 'keep-alive', 'x-amzn-requestid': 'dfd81eee-70e6-4b70-896e-0a7b7c62c24f'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"The actual value conveys the same information as the expected value, despite slight differences in wording and structure. Both mention deposits of $1,250, withdrawals of $850, and an ending balance of $2,400, which are the key elements of the statement summary.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 245, 'outputTokens': 87, 'totalTokens': 332}, 'metrics': {'latencyMs': 533}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for statement_summary (from code block): match=True, score=0.95, reason=The actual value conveys the same information as the expected value, despite slight differences in wording and structure. Both mention deposits of $1,250, withdrawals of $850, and an ending balance of $2,400, which are the key elements of the statement summary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.95\n",
      "Reason: The actual value conveys the same information as the expected value, despite slight differences in wording and structure. Both mention deposits of $1,250, withdrawals of $850, and an ending balance of $2,400, which are the key elements of the statement summary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 0.95,\n",
       " 'The actual value conveys the same information as the expected value, despite slight differences in wording and structure. Both mention deposits of $1,250, withdrawals of $850, and an ending balance of $2,400, which are the key elements of the statement summary.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM method - High semantic match (different wording, same meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",\n",
    "    document_class=\"BankStatement\",\n",
    "    attr_name=\"statement_summary\",\n",
    "    attr_description=\"Summary of the bank statement\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\n",
      "Actual: Statement with deposits of $2,500 and withdrawals of $1,200, leaving a balance of $3,800.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'bb4198ce-24c9-418a-9de6-e03e799e7f14', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 19:51:51 GMT', 'content-type': 'application/json', 'content-length': '411', 'connection': 'keep-alive', 'x-amzn-requestid': 'bb4198ce-24c9-418a-9de6-e03e799e7f14'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": false,\\n  \"score\": 0.4,\\n  \"reason\": \"The expected and actual values describe different financial transactions and balances, despite some similarities in the type of information presented.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 247, 'outputTokens': 48, 'totalTokens': 295}, 'metrics': {'latencyMs': 643}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for statement_summary (from code block): match=False, score=0.4, reason=The expected and actual values describe different financial transactions and balances, despite some similarities in the type of information presented.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.4\n",
      "Reason: The expected and actual values describe different financial transactions and balances, despite some similarities in the type of information presented.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " 0.4,\n",
       " 'The expected and actual values describe different financial transactions and balances, despite some similarities in the type of information presented.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM method - No semantic match (different meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits of $2,500 and withdrawals of $1,200, leaving a balance of $3,800.\",\n",
    "    document_class=\"BankStatement\",\n",
    "    attr_name=\"statement_summary\",\n",
    "    attr_description=\"Summary of the bank statement\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Policy effective date: January 15, 2023 to January 14, 2024\n",
      "Actual: Policy period begins on Jan 15, 2023 and expires on Jan 15, 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '1f75e1fd-b17f-4946-a5b5-029e34ac30f4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 19:51:53 GMT', 'content-type': 'application/json', 'content-length': '607', 'connection': 'keep-alive', 'x-amzn-requestid': '1f75e1fd-b17f-4946-a5b5-029e34ac30f4'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values convey the same information about the policy period, with slight differences in wording and formatting. The expected value specifies the period as \\'January 15, 2023 to January 14, 2024\\', while the actual value states \\'begins on Jan 15, 2023 and expires on Jan 15, 2024\\'. Despite these differences, the semantic meaning is equivalent.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 245, 'outputTokens': 121, 'totalTokens': 366}, 'metrics': {'latencyMs': 737}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for policy_period (from code block): match=True, score=0.95, reason=Both values convey the same information about the policy period, with slight differences in wording and formatting. The expected value specifies the period as 'January 15, 2023 to January 14, 2024', while the actual value states 'begins on Jan 15, 2023 and expires on Jan 15, 2024'. Despite these differences, the semantic meaning is equivalent.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.95\n",
      "Reason: Both values convey the same information about the policy period, with slight differences in wording and formatting. The expected value specifies the period as 'January 15, 2023 to January 14, 2024', while the actual value states 'begins on Jan 15, 2023 and expires on Jan 15, 2024'. Despite these differences, the semantic meaning is equivalent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 0.95,\n",
       " \"Both values convey the same information about the policy period, with slight differences in wording and formatting. The expected value specifies the period as 'January 15, 2023 to January 14, 2024', while the actual value states 'begins on Jan 15, 2023 and expires on Jan 15, 2024'. Despite these differences, the semantic meaning is equivalent.\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM method - Partial semantic match (some differences)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Policy effective date: January 15, 2023 to January 14, 2024\",\n",
    "    \"Policy period begins on Jan 15, 2023 and expires on Jan 15, 2024\",\n",
    "    document_class=\"InsurancePolicy\",\n",
    "    attr_name=\"policy_period\",\n",
    "    attr_description=\"The dates during which the insurance policy is effective\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Edge Cases - Missing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: Attributes Not Found\n",
    "Testing scenarios where attributes are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Case 1: Attribute not found in actual results\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: This value exists\n",
      "Actual: None\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: EXACT - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: This value exists\n",
      "Actual: None\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: NUMERIC_EXACT - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: This value exists\n",
      "Actual: None\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: FUZZY - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: This value exists\n",
      "Actual: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'b10a255d-33b3-46d1-955a-411ac3829ad9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 19:51:54 GMT', 'content-type': 'application/json', 'content-length': '438', 'connection': 'keep-alive', 'x-amzn-requestid': 'b10a255d-33b3-46d1-955a-411ac3829ad9'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": false,\\n  \"score\": 0.0,\\n  \"reason\": \"The expected value indicates that the attribute \\'test_attr\\' exists, while the actual value is \\'None\\', meaning the attribute does not exist. These values do not match in meaning.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 190, 'outputTokens': 64, 'totalTokens': 254}, 'metrics': {'latencyMs': 650}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for test_attr (from code block): match=False, score=0.0, reason=The expected value indicates that the attribute 'test_attr' exists, while the actual value is 'None', meaning the attribute does not exist. These values do not match in meaning.\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.0\n",
      "Reason: The expected value indicates that the attribute 'test_attr' exists, while the actual value is 'None', meaning the attribute does not exist. These values do not match in meaning.\n",
      "Method: LLM - Score: 0.0 - Matched: False\n",
      "\n",
      "Case 2: Attribute not found in expected results\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: None\n",
      "Actual: This value exists\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: EXACT - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: None\n",
      "Actual: This value exists\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: NUMERIC_EXACT - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: None\n",
      "Actual: This value exists\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "Method: FUZZY - Score: 0.0 - Matched: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: None\n",
      "Actual: This value exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '6890e77f-8041-4d34-8f59-28f45cccfcb8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 19:51:55 GMT', 'content-type': 'application/json', 'content-length': '436', 'connection': 'keep-alive', 'x-amzn-requestid': '6890e77f-8041-4d34-8f59-28f45cccfcb8'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": false,\\n  \"score\": 0.0,\\n  \"reason\": \"The expected value is \\'None\\', indicating the absence of a value, while the actual value \\'This value exists\\' indicates the presence of a value. These are semantically opposite.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 190, 'outputTokens': 62, 'totalTokens': 252}, 'metrics': {'latencyMs': 608}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for test_attr (from code block): match=False, score=0.0, reason=The expected value is 'None', indicating the absence of a value, while the actual value 'This value exists' indicates the presence of a value. These are semantically opposite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.0\n",
      "Reason: The expected value is 'None', indicating the absence of a value, while the actual value 'This value exists' indicates the presence of a value. These are semantically opposite.\n",
      "Method: LLM - Score: 0.0 - Matched: False\n",
      "\n",
      "Case 3: Attribute not found in either expected or actual results\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: None\n",
      "Actual: None\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Reason: Both actual and expected values are missing, so they are matched.\n",
      "Method: EXACT - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: None\n",
      "Actual: None\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Reason: Both actual and expected values are missing, so they are matched.\n",
      "Method: NUMERIC_EXACT - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: None\n",
      "Actual: None\n",
      "Threshold: 0.8\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Reason: Both actual and expected values are missing, so they are matched.\n",
      "Method: FUZZY - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: None\n",
      "Actual: None\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Reason: Both actual and expected values are missing, so they are matched.\n",
      "Method: LLM - Score: 1.0 - Matched: True\n",
      "\n",
      "Case 4: Empty string values\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: \n",
      "Actual: \n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Reason: Both actual and expected values are missing, so they are matched.\n",
      "Method: EXACT - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: \n",
      "Actual: \n",
      "Threshold: 0.8\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Reason: Both actual and expected values are missing, so they are matched.\n",
      "Method: FUZZY - Score: 1.0 - Matched: True\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: \n",
      "Actual: \n",
      "Matched: True\n",
      "Score: 1.0\n",
      "Reason: Both actual and expected values are missing, so they are matched.\n",
      "Method: LLM - Score: 1.0 - Matched: True\n"
     ]
    }
   ],
   "source": [
    "# Case 1: Attribute not found in actual (expected exists, actual is None)\n",
    "print(\"\\nCase 1: Attribute not found in actual results\")\n",
    "for method in [EvaluationMethod.EXACT, EvaluationMethod.NUMERIC_EXACT, \n",
    "               EvaluationMethod.FUZZY, EvaluationMethod.LLM]:\n",
    "    matched, score, reason = test_comparison(method, \"This value exists\", None)\n",
    "    print(f\"Method: {method.name} - Score: {score} - Matched: {matched}\")\n",
    "\n",
    "# Case 2: Attribute not found in expected (expected is None, actual exists)\n",
    "print(\"\\nCase 2: Attribute not found in expected results\")\n",
    "for method in [EvaluationMethod.EXACT, EvaluationMethod.NUMERIC_EXACT, \n",
    "               EvaluationMethod.FUZZY, EvaluationMethod.LLM]:\n",
    "    matched, score, reason = test_comparison(method, None, \"This value exists\")\n",
    "    print(f\"Method: {method.name} - Score: {score} - Matched: {matched}\")\n",
    "\n",
    "# Case 3: Attribute not found in either (both are None)\n",
    "print(\"\\nCase 3: Attribute not found in either expected or actual results\")\n",
    "for method in [EvaluationMethod.EXACT, EvaluationMethod.NUMERIC_EXACT, \n",
    "               EvaluationMethod.FUZZY, EvaluationMethod.LLM]:\n",
    "    matched, score, reason = test_comparison(method, None, None)\n",
    "    print(f\"Method: {method.name} - Score: {score} - Matched: {matched}\")\n",
    "\n",
    "# Case 4: Empty string values (\"\")\n",
    "print(\"\\nCase 4: Empty string values\")\n",
    "for method in [EvaluationMethod.EXACT, EvaluationMethod.FUZZY, EvaluationMethod.LLM]:\n",
    "    matched, score, reason = test_comparison(method, \"\", \"\")\n",
    "    print(f\"Method: {method.name} - Score: {score} - Matched: {matched}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Full Document Evaluation\n",
    "\n",
    "Now we'll test the full document evaluation service with different evaluation methods. We'll use the real AWS Bedrock service for LLM-based evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup test config\n",
    "test_config = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"name\": \"TestDocument\",\n",
    "            \"attributes\": [\n",
    "                {\n",
    "                    \"name\": \"exact_match_attr\",\n",
    "                    \"description\": \"Attribute for exact matching\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"numeric_attr\",\n",
    "                    \"description\": \"Attribute for numeric matching\",\n",
    "                    \"evaluation_method\": \"NUMERIC_EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"fuzzy_attr\",\n",
    "                    \"description\": \"Attribute for fuzzy matching\",\n",
    "                    \"evaluation_method\": \"FUZZY\",\n",
    "                    \"evaluation_threshold\": 0.8\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"list_attr\",\n",
    "                    \"description\": \"Attribute for list comparison\",\n",
    "                    \"evaluation_method\": \"HUNGARIAN\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"llm_attr\",\n",
    "                    \"description\": \"Attribute for semantic comparison\",\n",
    "                    \"evaluation_method\": \"LLM\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_in_actual\",\n",
    "                    \"description\": \"Attribute missing in actual results\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_in_expected\",\n",
    "                    \"description\": \"Attribute missing in expected results\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_everywhere\",\n",
    "                    \"description\": \"Attribute missing in both expected and actual\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"evaluation\": {\n",
    "        \"llm_method\": {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator for document extraction attributes.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.  Here's the exact format:\n",
    "{\n",
    "  \"match\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"Your explanation here\"\n",
    "}\n",
    "\"\"\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock S3 retrieval function\n",
    "def mock_s3_get_json(uri: str) -> Dict[str, Any]:\n",
    "    \"\"\"Mock S3 file retrieval.\"\"\"\n",
    "    if \"expected\" in uri:\n",
    "        return {\n",
    "            \"exact_match_attr\": \"Exact Match Value\",\n",
    "            \"numeric_attr\": \"$1,250.00\",\n",
    "            \"fuzzy_attr\": \"John Alexander Smith\",\n",
    "            \"list_attr\": [\"Item 1\", \"Item 2\", \"Item 3\"],\n",
    "            \"llm_attr\": \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "            \"missing_in_actual\": \"This value exists in expected only\",\n",
    "            # missing_in_expected is intentionally omitted\n",
    "            # missing_everywhere is intentionally omitted\n",
    "        }\n",
    "    else:  # actual results\n",
    "        return {\n",
    "            \"exact_match_attr\": \"Exact Match Value\",  # Exact match\n",
    "            \"numeric_attr\": 1250,  # Numeric match\n",
    "            \"fuzzy_attr\": \"John A Smith\",  # Fuzzy match\n",
    "            \"list_attr\": [\"Item 1\", \"Item 3\", \"Item 2\"],  # List with different order\n",
    "            \"llm_attr\": \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",  # Semantic match\n",
    "            # missing_in_actual is intentionally omitted\n",
    "            \"missing_in_expected\": \"This value exists in actual only\",\n",
    "            # missing_everywhere is intentionally omitted\n",
    "        }\n",
    "\n",
    "# Set up mock storage - we'll still use this for S3\n",
    "class MockS3:\n",
    "    # Store report content for later display\n",
    "    report_content = \"\"\n",
    "    results_content = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_json_content(uri: str) -> Dict[str, Any]:\n",
    "        return mock_s3_get_json(uri)\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_content(content: Any, bucket: str, key: str, content_type: str = None):\n",
    "        print(f\"Writing content to s3://{bucket}/{key}\")\n",
    "        if key.endswith(\"results.json\"):\n",
    "            # Store the results for later access\n",
    "            MockS3.results_content = content\n",
    "            print(f\"Evaluation results summary: {json.dumps(content.get('overall_metrics', {}), indent=2)}\")\n",
    "        elif key.endswith(\"report.md\"):\n",
    "            # Store the markdown report for later display\n",
    "            MockS3.report_content = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock documents for evaluation\n",
    "def create_test_document(doc_id: str, is_expected: bool = False) -> Document:\n",
    "    \"\"\"Create a test document with a section.\"\"\"\n",
    "    section = Section(\n",
    "        section_id=\"sec-001\",\n",
    "        classification=\"TestDocument\",\n",
    "        extraction_result_uri=f\"s3://test-bucket/{doc_id}/{'expected' if is_expected else 'actual'}/extraction.json\"\n",
    "    )\n",
    "    \n",
    "    doc = Document(\n",
    "        id=doc_id,\n",
    "        sections=[section],\n",
    "        input_key=doc_id,\n",
    "        input_bucket=\"test-bucket\",\n",
    "        output_bucket=\"test-bucket\",\n",
    "        status=Status.EXTRACTED\n",
    "    )\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Create test documents\n",
    "actual_doc = create_test_document(\"test-doc-001\")\n",
    "expected_doc = create_test_document(\"test-doc-001-baseline\", is_expected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.evaluation.service:Initialized evaluation service with LLM configuration\n",
      "INFO:idp_common.evaluation.service:Comparing: exact_match_attr using EvaluationMethod.EXACT - from class TestDocument, section sec-001\n",
      "INFO:idp_common.evaluation.service:Comparing: numeric_attr using EvaluationMethod.NUMERIC_EXACT - from class TestDocument, section sec-001\n",
      "INFO:idp_common.evaluation.service:Comparing: fuzzy_attr using EvaluationMethod.FUZZY - from class TestDocument, section sec-001\n",
      "INFO:idp_common.evaluation.service:Comparing: list_attr using EvaluationMethod.HUNGARIAN - from class TestDocument, section sec-001\n",
      "INFO:idp_common.evaluation.service:Comparing: llm_attr using EvaluationMethod.LLM - from class TestDocument, section sec-001\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'f749a7b8-197f-4cb9-9fe7-a548f90c6117', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 18 Apr 2025 19:51:57 GMT', 'content-type': 'application/json', 'content-length': '424', 'connection': 'keep-alive', 'x-amzn-requestid': 'f749a7b8-197f-4cb9-9fe7-a548f90c6117'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values describe the same financial transactions and resulting balance, despite slight differences in wording and structure. The semantic meaning is preserved.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 235, 'outputTokens': 52, 'totalTokens': 287}, 'metrics': {'latencyMs': 614}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for llm_attr (from code block): match=True, score=0.95, reason=Both values describe the same financial transactions and resulting balance, despite slight differences in wording and structure. The semantic meaning is preserved.\n",
      "INFO:idp_common.evaluation.service:Comparing: missing_in_actual using EvaluationMethod.EXACT - from class TestDocument, section sec-001\n",
      "INFO:idp_common.evaluation.service:Comparing: missing_in_expected using EvaluationMethod.EXACT - from class TestDocument, section sec-001\n",
      "INFO:idp_common.evaluation.service:Comparing: missing_everywhere using EvaluationMethod.EXACT - from class TestDocument, section sec-001\n",
      "INFO:idp_common.evaluation.service:Evaluation complete for document test-doc-001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing content to s3://test-bucket/test-doc-001/evaluation/results.json\n",
      "Evaluation results summary: {\n",
      "  \"precision\": 0.6666666666666666,\n",
      "  \"recall\": 0.8,\n",
      "  \"f1_score\": 0.7272727272727272,\n",
      "  \"accuracy\": 0.625,\n",
      "  \"false_alarm_rate\": 0.5,\n",
      "  \"false_discovery_rate\": 0.2\n",
      "}\n",
      "Writing content to s3://test-bucket/test-doc-001/evaluation/report.md\n",
      "\n",
      "Overall metrics: {'precision': 0.6666666666666666, 'recall': 0.8, 'f1_score': 0.7272727272727272, 'accuracy': 0.625, 'false_alarm_rate': 0.5, 'false_discovery_rate': 0.2}\n",
      "\n",
      "Section sec-001 - Class: TestDocument\n",
      "Metrics: {'precision': 0.6666666666666666, 'recall': 0.8, 'f1_score': 0.7272727272727272, 'accuracy': 0.625, 'false_alarm_rate': 0.5, 'false_discovery_rate': 0.2}\n",
      "\n",
      "Attribute Details:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Name                 Method          Expected                  Actual                    Matched    Score      Reason\n",
      "----------------------------------------------------------------------------------------------------\n",
      "exact_match_attr     EXACT           Exact Match Value         Exact Match Value         True       1.00       \n",
      "numeric_attr         NUMERIC_EXACT   $1,250.00                 1250                      True       1.00       \n",
      "fuzzy_attr           FUZZY           John Alexander Smith      John A Smith              False      0.60       \n",
      "list_attr            HUNGARIAN       ['Item 1', 'Item 2', 'Ite ['Item 1', 'Item 3', 'Ite True       1.00       \n",
      "llm_attr             LLM             Monthly statement showing Statement with deposits t True       0.95       Both values describe the same financial transactio...\n",
      "missing_in_actual    EXACT           This value exists in expe None                      False      0.00       \n",
      "missing_in_expected  EXACT           None                      This value exists in actu False      0.00       \n",
      "missing_everywhere   EXACT           None                      None                      True       1.00       Both actual and expected values are missing, so th...\n"
     ]
    }
   ],
   "source": [
    "# Evaluate document\n",
    "# Only patch S3 module - use real Bedrock\n",
    "import idp_common.evaluation.service\n",
    "idp_common.evaluation.service.s3 = MockS3\n",
    "\n",
    "# Create evaluation service\n",
    "evaluation_service = EvaluationService(region=\"us-east-1\", config=test_config)\n",
    "\n",
    "# Evaluate document\n",
    "result_doc = evaluation_service.evaluate_document(actual_doc, expected_doc, store_results=True)\n",
    "\n",
    "# Print results\n",
    "if hasattr(result_doc, 'evaluation_result'):\n",
    "    eval_result = result_doc.evaluation_result\n",
    "    print(f\"\\nOverall metrics: {eval_result.overall_metrics}\")\n",
    "    \n",
    "    # Check section results\n",
    "    for section_result in eval_result.section_results:\n",
    "        print(f\"\\nSection {section_result.section_id} - Class: {section_result.document_class}\")\n",
    "        print(f\"Metrics: {section_result.metrics}\")\n",
    "        \n",
    "        # Print attribute details\n",
    "        print(\"\\nAttribute Details:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Name':<20} {'Method':<15} {'Expected':<25} {'Actual':<25} {'Matched':<10} {'Score':<10} {'Reason'}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for attr in section_result.attributes:\n",
    "            expected_val = str(attr.expected)[:25]\n",
    "            actual_val = str(attr.actual)[:25]\n",
    "            method = attr.evaluation_method\n",
    "            reason = attr.reason[:50] + \"...\" if attr.reason and len(attr.reason) > 50 else (attr.reason or \"\")\n",
    "            print(f\"{attr.name:<20} {method:<15} {expected_val:<25} {actual_val:<25} {attr.matched!s:<10} {attr.score:<10.2f} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Evaluation Report\n",
    "\n",
    "Let's display the markdown evaluation report that was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Document Evaluation: test-doc-001\n",
       "\n",
       "## Summary\n",
       "- **Match Rate**:  5/8 attributes matched [] 62%\n",
       "- **Precision**: 0.67 | **Recall**: 0.80 | **F1 Score**:  0.73\n",
       "\n",
       "## Overall Metrics\n",
       "| Metric | Value | Rating |\n",
       "| ------ | :----: | :----: |\n",
       "| precision | 0.6667 |  Fair |\n",
       "| recall | 0.8000 |  Good |\n",
       "| f1_score | 0.7273 |  Good |\n",
       "| accuracy | 0.6250 |  Fair |\n",
       "| false_alarm_rate | 0.5000 |  Fair |\n",
       "| false_discovery_rate | 0.2000 |  Good |\n",
       "\n",
       "\n",
       "## Section: sec-001 (TestDocument)\n",
       "### Metrics\n",
       "| Metric | Value | Rating |\n",
       "| ------ | :----: | :----: |\n",
       "| precision | 0.6667 |  Fair |\n",
       "| recall | 0.8000 |  Good |\n",
       "| f1_score | 0.7273 |  Good |\n",
       "| accuracy | 0.6250 |  Fair |\n",
       "| false_alarm_rate | 0.5000 |  Fair |\n",
       "| false_discovery_rate | 0.2000 |  Good |\n",
       "\n",
       "\n",
       "### Attributes\n",
       "| Status | Attribute | Expected | Actual | Score | Method | Reason |\n",
       "| :----: | --------- | -------- | ------ | ----- | ------ | ------ |\n",
       "|  | exact_match_attr | Exact Match Value | Exact Match Value | 1.00 | EXACT |  |\n",
       "|  | numeric_attr | $1,250.00 | 1250 | 1.00 | NUMERIC_EXACT |  |\n",
       "|  | fuzzy_attr | John Alexander Smith | John A Smith | 0.60 | FUZZY (evaluation_threshold: 0.8) |  |\n",
       "|  | list_attr | ['Item 1', 'Item 2', 'Item 3'] | ['Item 1', 'Item 3', 'Item 2'] | 1.00 | HUNGARIAN |  |\n",
       "|  | llm_attr | Monthly statement showing deposits of $1,250, with | Statement with deposits totaling $1,250 and withdr | 0.95 | LLM | Both values describe the same financial transactions and resulting balance, despite slight differences in wording and structure. The semantic meaning is preserved. |\n",
       "|  | missing_in_actual | This value exists in expected only | None | 0.00 | EXACT |  |\n",
       "|  | missing_in_expected | None | This value exists in actual only | 0.00 | EXACT |  |\n",
       "|  | missing_everywhere | None | None | 1.00 | EXACT | Both actual and expected values are missing, so they are matched. |\n",
       "\n",
       "\n",
       "Execution time: 1.37 seconds\n",
       "\n",
       "## Evaluation Methods Used\n",
       "\n",
       "This evaluation used the following methods to compare expected and actual values:\n",
       "\n",
       "1. **EXACT** - Exact string match after stripping punctuation and whitespace\n",
       "2. **NUMERIC_EXACT** - Exact numeric match after normalizing\n",
       "3. **FUZZY** - Fuzzy string matching using string similarity metrics (with optional evaluation_threshold)\n",
       "4. **BERT** - Semantic similarity comparison using BERT embeddings (with evaluation_threshold)\n",
       "5. **HUNGARIAN** - Bipartite matching algorithm for lists of values\n",
       "6. **LLM** - Advanced semantic evaluation using Bedrock large language models\n",
       "\n",
       "Each attribute is configured with a specific evaluation method based on the data type and comparison needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Display the markdown report\n",
    "if MockS3.report_content:\n",
    "    display(Markdown(MockS3.report_content))\n",
    "else:\n",
    "    print(\"No evaluation report was generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. All evaluation methods available in the IDP library:\n",
    "   - EXACT - Exact string matching\n",
    "   - NUMERIC_EXACT - Numeric value matching\n",
    "   - FUZZY - Fuzzy string matching with adjustable thresholds\n",
    "   - HUNGARIAN - List comparison using the Hungarian algorithm\n",
    "   - LLM - Semantic comparison using Large Language Models\n",
    "\n",
    "2. Handling of edge cases:\n",
    "   - Attributes missing in actual results\n",
    "   - Attributes missing in expected results\n",
    "   - Attributes missing in both actual and expected results\n",
    "   - Empty string values\n",
    "\n",
    "3. Full document evaluation with mixed evaluation methods\n",
    "   - Comprehensive metrics calculation\n",
    "   - Detailed attribute-level results\n",
    "\n",
    "4. Threshold sensitivity analysis for fuzzy matching\n",
    "   - How different threshold values affect match results\n",
    "   - Trade-offs between precision and recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
