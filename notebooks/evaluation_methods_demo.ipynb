{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Methods Demonstration\n",
    "\n",
    "This notebook demonstrates the various evaluation methods available in the IDP library for comparing expected values with actual extraction results. It covers:\n",
    "\n",
    "1. All evaluation methods with both match and no-match scenarios\n",
    "2. Threshold testing for applicable methods\n",
    "3. Edge cases:\n",
    "   - Attribute not found in actual results\n",
    "   - Attribute not found in expected results\n",
    "   - Attribute not found in either actual or expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: idp_common 0.2.19\n",
      "Uninstalling idp_common-0.2.19:\n",
      "  Successfully uninstalled idp_common-0.2.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First uninstall existing package (to ensure we get the latest version)\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package with all components in development mode\n",
    "%pip install -q -e \"../lib/idp_common_pkg[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Add parent directory to path to import the library\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.evaluation.models import EvaluationMethod\n",
    "from idp_common.evaluation.comparator import compare_values\n",
    "from idp_common.evaluation.service import EvaluationService\n",
    "from idp_common.models import Document, Section, Status\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Comparing Individual Values with Different Methods\n",
    "\n",
    "We'll test each evaluation method with matching and non-matching examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_comparison(method: EvaluationMethod, expected: Any, actual: Any, \n",
    "                    threshold: float = 0.8, document_class: str = \"TestDoc\",\n",
    "                    attr_name: str = \"test_attr\", attr_description: str = \"Test attribute\"):\n",
    "    \"\"\"Test a comparison method and print results.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Method: {method.name}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Actual: {actual}\")\n",
    "    \n",
    "    if method in [EvaluationMethod.FUZZY, EvaluationMethod.SEMANTIC]:\n",
    "        print(f\"Threshold: {threshold}\")\n",
    "    \n",
    "    # Set up LLM config for the LLM method\n",
    "    llm_config = None\n",
    "    if method == EvaluationMethod.LLM:\n",
    "        llm_config = {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator that helps determine if the predicted and expected values match for document attribute extraction.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.  Here's the exact format:\n",
    "{\n",
    "  \"match\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"Your explanation here\"\n",
    "}\n",
    "\"\"\"\n",
    "        }\n",
    "    \n",
    "    # Perform the comparison\n",
    "    matched, score, reason = compare_values(\n",
    "        expected=expected,\n",
    "        actual=actual,\n",
    "        method=method,\n",
    "        threshold=threshold,\n",
    "        document_class=document_class,\n",
    "        attr_name=attr_name,\n",
    "        attr_description=attr_description,\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "    \n",
    "    print(f\"Matched: {matched}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    if reason:\n",
    "        print(f\"Reason: {reason}\")\n",
    "        \n",
    "    return matched, score, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: EXACT Method\n",
    "Testing exact string matching with both match and non-match cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: Account #12345\n",
      "Actual: Account #12345\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: Account #12345\n",
      "Actual: Account #12346\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: EXACT\n",
      "Expected: Account Number: 12345\n",
      "Actual: account number 12345\n",
      "Matched: True\n",
      "Score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 1.0, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXACT method - Match\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account #12345\", \"Account #12345\")\n",
    "\n",
    "# EXACT method - No match\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account #12345\", \"Account #12346\")\n",
    "\n",
    "# EXACT method - Match with different casing and punctuation\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account Number: 12345\", \"account number 12345\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: NUMERIC_EXACT Method\n",
    "Testing numeric comparison with different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: $1,250.00\n",
      "Actual: 1250\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: $1,250.00\n",
      "Actual: 1251\n",
      "Matched: False\n",
      "Score: 0.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: NUMERIC_EXACT\n",
      "Expected: (1,250.00)\n",
      "Actual: -1250\n",
      "Matched: False\n",
      "Score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0.0, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NUMERIC_EXACT method - Match\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"$1,250.00\", 1250)\n",
    "\n",
    "# NUMERIC_EXACT method - No match\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"$1,250.00\", 1251)\n",
    "\n",
    "# NUMERIC_EXACT method - Match with different formats\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"(1,250.00)\", \"-1250\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: FUZZY Method\n",
    "Testing fuzzy comparison with different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: John A. Smith\n",
      "Actual: John Smith\n",
      "Threshold: 0.8\n",
      "Matched: True\n",
      "Score: 0.8333333333333334\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: John A. Smith\n",
      "Actual: John Simpson\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.41666666666666663\n",
      "With threshold=0.6: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: FUZZY\n",
      "Expected: John Alexander Smith\n",
      "Actual: Jane Marie Johnson\n",
      "Threshold: 0.8\n",
      "Matched: False\n",
      "Score: 0.15000000000000002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, 0.15000000000000002, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUZZY method - High match\n",
    "test_comparison(EvaluationMethod.FUZZY, \"John A. Smith\", \"John Smith\", threshold=0.8)\n",
    "\n",
    "# FUZZY method - Medium match \n",
    "matched, score, _ = test_comparison(EvaluationMethod.FUZZY, \"John A. Smith\", \"John Simpson\", threshold=0.8)\n",
    "print(f\"With threshold=0.6: {score >= 0.6}\")\n",
    "\n",
    "# FUZZY method - Low match\n",
    "test_comparison(EvaluationMethod.FUZZY, \"John Alexander Smith\", \"Jane Marie Johnson\", threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>### Test 4: HUNGARIAN Method\n",
    "Testing list comparison using the Hungarian algorithm.\n",
    "\n",
    "The Hungarian method is used for optimal matching between two lists when order doesn't matter. It automatically pairs items from each list for the maximum possible match score.\n",
    "\n",
    "**New Feature:** The Hungarian method now supports a `comparator_type` configuration which can be set to:\n",
    "- `EXACT`: Exact string matching (default)\n",
    "- `FUZZY`: Fuzzy string matching with similarity scoring\n",
    "- `NUMERIC`: Numeric comparison for handling different number formats\n",
    "\n",
    "This allows for greater flexibility when comparing lists of different types of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $200']\n",
      "Actual: ['Deposit: $500', 'Transfer: $200', 'Withdrawal: $150']\n",
      "Matched: True\n",
      "Score: 1.0\n",
      "\n",
      "Demonstrating the new comparator_type functionality:\n",
      "------------------------------------------------------------\n",
      "HUNGARIAN with EXACT comparator:\n",
      "  Expected: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $200']\n",
      "  Actual: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $210']\n",
      "  Result: Matched=False, Score=0.67\n",
      "\n",
      "HUNGARIAN with FUZZY comparator (threshold 0.7):\n",
      "  Expected: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $200']\n",
      "  Actual: ['Deposit: $500', 'Withdrawal: $150', 'Transfer: $210']\n",
      "  Result: Matched=True, Score=0.97\n",
      "\n",
      "HUNGARIAN with NUMERIC comparator (non-matching case):\n",
      "  Expected: ['$500', '$150.00', '$200']\n",
      "  Actual: ['500', '150', '210']\n",
      "  Result: Matched=False, Score=0.67\n",
      "  Note: The values '$200' and '210' don't match numerically\n",
      "\n",
      "HUNGARIAN with NUMERIC comparator (matching case):\n",
      "  Expected: ['$500', '$150.00', '$200']\n",
      "  Actual: ['500', '150', '200']\n",
      "  Result: Matched=True, Score=1.00\n",
      "  Note: All numeric values match after normalization\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: HUNGARIAN\n",
      "Expected: Single item\n",
      "Actual: Single item\n",
      "Matched: True\n",
      "Score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 1.0, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HUNGARIAN method - Full match with EXACT comparator (default)\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Transfer: $200\", \"Withdrawal: $150\"]\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, expected_list, actual_list)\n",
    "\n",
    "# HUNGARIAN method - Test with explicit comparator_type\n",
    "print(\"\\nDemonstrating the new comparator_type functionality:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Test with EXACT comparator\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $210\"]  # Small difference in last value\n",
    "matched, score, reason = compare_values(\n",
    "    expected=expected_list,\n",
    "    actual=actual_list,\n",
    "    method=EvaluationMethod.HUNGARIAN,\n",
    "    comparator_type=\"EXACT\"  # Using the new comparator_type parameter\n",
    ")\n",
    "print(f\"HUNGARIAN with EXACT comparator:\")\n",
    "print(f\"  Expected: {expected_list}\")\n",
    "print(f\"  Actual: {actual_list}\")\n",
    "print(f\"  Result: Matched={matched}, Score={score:.2f}\")\n",
    "\n",
    "# Test with FUZZY comparator\n",
    "matched, score, reason = compare_values(\n",
    "    expected=expected_list,\n",
    "    actual=actual_list,\n",
    "    method=EvaluationMethod.HUNGARIAN,\n",
    "    comparator_type=\"FUZZY\",  # Using fuzzy comparator\n",
    "    threshold=0.7  # Explicit threshold for fuzzy matching\n",
    ")\n",
    "print(f\"\\nHUNGARIAN with FUZZY comparator (threshold 0.7):\")\n",
    "print(f\"  Expected: {expected_list}\")\n",
    "print(f\"  Actual: {actual_list}\")\n",
    "print(f\"  Result: Matched={matched}, Score={score:.2f}\")\n",
    "\n",
    "# Test with NUMERIC comparator (non-matching case)\n",
    "expected_num_list = [\"$500\", \"$150.00\", \"$200\"]\n",
    "actual_num_list = [\"500\", \"150\", \"210\"]  # Different number representation for the last value\n",
    "matched, score, reason = compare_values(\n",
    "    expected=expected_num_list,\n",
    "    actual=actual_num_list,\n",
    "    method=EvaluationMethod.HUNGARIAN,\n",
    "    comparator_type=\"NUMERIC\"  # Using numeric comparator\n",
    ")\n",
    "print(f\"\\nHUNGARIAN with NUMERIC comparator (non-matching case):\")\n",
    "print(f\"  Expected: {expected_num_list}\")\n",
    "print(f\"  Actual: {actual_num_list}\")\n",
    "print(f\"  Result: Matched={matched}, Score={score:.2f}\")\n",
    "print(f\"  Note: The values '$200' and '210' don't match numerically\")\n",
    "\n",
    "# Test with NUMERIC comparator (matching case)\n",
    "expected_num_list = [\"$500\", \"$150.00\", \"$200\"]\n",
    "actual_num_list = [\"500\", \"150\", \"200\"]  # Exact numeric matches after normalization\n",
    "matched, score, reason = compare_values(\n",
    "    expected=expected_num_list,\n",
    "    actual=actual_num_list,\n",
    "    method=EvaluationMethod.HUNGARIAN,\n",
    "    comparator_type=\"NUMERIC\"  # Using numeric comparator\n",
    ")\n",
    "print(f\"\\nHUNGARIAN with NUMERIC comparator (matching case):\")\n",
    "print(f\"  Expected: {expected_num_list}\")\n",
    "print(f\"  Actual: {actual_num_list}\")\n",
    "print(f\"  Result: Matched={matched}, Score={score:.2f}\")\n",
    "print(f\"  Note: All numeric values match after normalization\")\n",
    "\n",
    "# HUNGARIAN method - Non-list values (should convert to list)\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, \"Single item\", \"Single item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: LLM Method\n",
    "Testing semantic comparison using a Large Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\n",
      "Actual: Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '1a261044-c072-4dcc-b7f9-f159a4ba60a0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:33 GMT', 'content-type': 'application/json', 'content-length': '511', 'connection': 'keep-alive', 'x-amzn-requestid': '1a261044-c072-4dcc-b7f9-f159a4ba60a0'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"The actual value conveys the same information as the expected value, despite slight differences in wording and structure. Both mention deposits of $1,250, withdrawals of $850, and a resulting balance of $2,400, which aligns with the expected summary.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 245, 'outputTokens': 84, 'totalTokens': 329}, 'metrics': {'latencyMs': 581}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for statement_summary (from code block): match=True, score=0.95, reason=The actual value conveys the same information as the expected value, despite slight differences in wording and structure. Both mention deposits of $1,250, withdrawals of $850, and a resulting balance of $2,400, which aligns with the expected summary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.95\n",
      "Reason: The actual value conveys the same information as the expected value, despite slight differences in wording and structure. Both mention deposits of $1,250, withdrawals of $850, and a resulting balance of $2,400, which aligns with the expected summary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 0.95,\n",
       " 'The actual value conveys the same information as the expected value, despite slight differences in wording and structure. Both mention deposits of $1,250, withdrawals of $850, and a resulting balance of $2,400, which aligns with the expected summary.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM method - High semantic match (different wording, same meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",\n",
    "    document_class=\"BankStatement\",\n",
    "    attr_name=\"statement_summary\",\n",
    "    attr_description=\"Summary of the bank statement\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\n",
      "Actual: Statement with deposits of $2,500 and withdrawals of $1,200, leaving a balance of $3,800.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '94d553c0-5815-47cc-a1d3-de495f2dcfe5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:34 GMT', 'content-type': 'application/json', 'content-length': '411', 'connection': 'keep-alive', 'x-amzn-requestid': '94d553c0-5815-47cc-a1d3-de495f2dcfe5'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": false,\\n  \"score\": 0.4,\\n  \"reason\": \"The expected and actual values describe different financial transactions and balances, despite some similarities in the type of information presented.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 247, 'outputTokens': 48, 'totalTokens': 295}, 'metrics': {'latencyMs': 480}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for statement_summary (from code block): match=False, score=0.4, reason=The expected and actual values describe different financial transactions and balances, despite some similarities in the type of information presented.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.4\n",
      "Reason: The expected and actual values describe different financial transactions and balances, despite some similarities in the type of information presented.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " 0.4,\n",
       " 'The expected and actual values describe different financial transactions and balances, despite some similarities in the type of information presented.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM method - No semantic match (different meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits of $2,500 and withdrawals of $1,200, leaving a balance of $3,800.\",\n",
    "    document_class=\"BankStatement\",\n",
    "    attr_name=\"statement_summary\",\n",
    "    attr_description=\"Summary of the bank statement\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Policy effective date: January 15, 2023 to January 14, 2024\n",
      "Actual: Policy period begins on Jan 15, 2023 and expires on Jan 15, 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'b8d0c1ee-69c5-48ac-bacf-cfa069fbf2c6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:36 GMT', 'content-type': 'application/json', 'content-length': '607', 'connection': 'keep-alive', 'x-amzn-requestid': 'b8d0c1ee-69c5-48ac-bacf-cfa069fbf2c6'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values convey the same information about the policy period, with slight differences in wording and formatting. The expected value specifies the period as \\'January 15, 2023 to January 14, 2024\\', while the actual value states \\'begins on Jan 15, 2023 and expires on Jan 15, 2024\\'. Despite these differences, the semantic meaning is equivalent.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 245, 'outputTokens': 121, 'totalTokens': 366}, 'metrics': {'latencyMs': 835}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for policy_period (from code block): match=True, score=0.95, reason=Both values convey the same information about the policy period, with slight differences in wording and formatting. The expected value specifies the period as 'January 15, 2023 to January 14, 2024', while the actual value states 'begins on Jan 15, 2023 and expires on Jan 15, 2024'. Despite these differences, the semantic meaning is equivalent.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.95\n",
      "Reason: Both values convey the same information about the policy period, with slight differences in wording and formatting. The expected value specifies the period as 'January 15, 2023 to January 14, 2024', while the actual value states 'begins on Jan 15, 2023 and expires on Jan 15, 2024'. Despite these differences, the semantic meaning is equivalent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 0.95,\n",
       " \"Both values convey the same information about the policy period, with slight differences in wording and formatting. The expected value specifies the period as 'January 15, 2023 to January 14, 2024', while the actual value states 'begins on Jan 15, 2023 and expires on Jan 15, 2024'. Despite these differences, the semantic meaning is equivalent.\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM method - Partial semantic match (some differences)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Policy effective date: January 15, 2023 to January 14, 2024\",\n",
    "    \"Policy period begins on Jan 15, 2023 and expires on Jan 15, 2024\",\n",
    "    document_class=\"InsurancePolicy\",\n",
    "    attr_name=\"policy_period\",\n",
    "    attr_description=\"The dates during which the insurance policy is effective\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: SEMANTIC Method\n",
    "Testing semantic comparison using embeddings with different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\n",
      "Actual: Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\n",
      "Threshold: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.9229\n",
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.9229397546670253\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: Policy effective date: January 15, 2023 to January 14, 2024\n",
      "Actual: Coverage period: From Jan 15, 2023 through January 15, 2024\n",
      "Threshold: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.4553\n",
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.45525245943571513\n",
      "With threshold=0.7: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\n",
      "Actual: Insurance policy with a premium of $850 per year and a deductible of $500.\n",
      "Threshold: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.4246\n",
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.424557628167674\n",
      "\n",
      "Testing different thresholds with medium similarity content:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: The patient was diagnosed with hypertension and prescribed lisinopril 10mg daily.\n",
      "Actual: Patient has high blood pressure and was given medication to take once per day.\n",
      "Threshold: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.8106\n",
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.8105557996454206\n",
      "Threshold 0.5: Match=True, Score=0.8106\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: The patient was diagnosed with hypertension and prescribed lisinopril 10mg daily.\n",
      "Actual: Patient has high blood pressure and was given medication to take once per day.\n",
      "Threshold: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.8106\n",
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.8105557996454206\n",
      "Threshold 0.7: Match=True, Score=0.8106\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: The patient was diagnosed with hypertension and prescribed lisinopril 10mg daily.\n",
      "Actual: Patient has high blood pressure and was given medication to take once per day.\n",
      "Threshold: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.8106\n",
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.8105557996454206\n",
      "Threshold 0.8: Match=True, Score=0.8106\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: The patient was diagnosed with hypertension and prescribed lisinopril 10mg daily.\n",
      "Actual: Patient has high blood pressure and was given medication to take once per day.\n",
      "Threshold: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.8106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.8105557996454206\n",
      "Threshold 0.9: Match=False, Score=0.8106\n"
     ]
    }
   ],
   "source": [
    "# SEMANTIC method - High similarity (different wording but same meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.SEMANTIC,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",\n",
    "    threshold=0.8\n",
    ")\n",
    "\n",
    "# SEMANTIC method - Medium similarity (related content)\n",
    "matched, score, _ = test_comparison(\n",
    "    EvaluationMethod.SEMANTIC,\n",
    "    \"Policy effective date: January 15, 2023 to January 14, 2024\",\n",
    "    \"Coverage period: From Jan 15, 2023 through January 15, 2024\",\n",
    "    threshold=0.8\n",
    ")\n",
    "print(f\"With threshold=0.7: {score >= 0.7}\")\n",
    "\n",
    "# SEMANTIC method - Low similarity (different content)\n",
    "test_comparison(\n",
    "    EvaluationMethod.SEMANTIC,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Insurance policy with a premium of $850 per year and a deductible of $500.\",\n",
    "    threshold=0.8\n",
    ")\n",
    "\n",
    "# SEMANTIC method - Different threshold test\n",
    "print(\"\\nTesting different thresholds with medium similarity content:\")\n",
    "for threshold in [0.5, 0.7, 0.8, 0.9]:\n",
    "    matched, score, _ = test_comparison(\n",
    "        EvaluationMethod.SEMANTIC,\n",
    "        \"The patient was diagnosed with hypertension and prescribed lisinopril 10mg daily.\",\n",
    "        \"Patient has high blood pressure and was given medication to take once per day.\",\n",
    "        threshold=threshold\n",
    "    )\n",
    "    print(f\"Threshold {threshold}: Match={matched}, Score={score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing SEMANTIC vs LLM Methods\n",
    "\n",
    "Let's compare results from the SEMANTIC method (using embeddings) with the LLM method on various examples to understand their respective strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comparison: Similar medical information with different wording\n",
      "Expected: Patient diagnosed with pneumonia and prescribed antibiotics for 10 days.\n",
      "Actual: The patient has pneumonia and was given a 10-day course of antibiotics.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: Patient diagnosed with pneumonia and prescribed antibiotics for 10 days.\n",
      "Actual: The patient has pneumonia and was given a 10-day course of antibiotics.\n",
      "Threshold: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.8057\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.8056780651446329\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Patient diagnosed with pneumonia and prescribed antibiotics for 10 days.\n",
      "Actual: The patient has pneumonia and was given a 10-day course of antibiotics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'e80df115-688b-43d9-baa3-7f4d614cf08d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:44 GMT', 'content-type': 'application/json', 'content-length': '455', 'connection': 'keep-alive', 'x-amzn-requestid': 'e80df115-688b-43d9-baa3-7f4d614cf08d'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values convey the same information: a patient diagnosed with pneumonia who was prescribed a 10-day course of antibiotics. The differences in wording and structure do not change the meaning.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 214, 'outputTokens': 64, 'totalTokens': 278}, 'metrics': {'latencyMs': 654}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for test_attr (from code block): match=True, score=0.95, reason=Both values convey the same information: a patient diagnosed with pneumonia who was prescribed a 10-day course of antibiotics. The differences in wording and structure do not change the meaning.\n",
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.95\n",
      "Reason: Both values convey the same information: a patient diagnosed with pneumonia who was prescribed a 10-day course of antibiotics. The differences in wording and structure do not change the meaning.\n",
      "\n",
      "Comparison of results:\n",
      "Method     Matched    Score      Reason\n",
      "--------------------------------------------------------------------------------\n",
      "SEMANTIC   True       0.8057     \n",
      "LLM        True       0.9500     Both values convey the same information: a patient diagnosed with pneumonia who was prescribed a 10-day course of antibiotics. The differences in wording and structure do not change the meaning.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comparison: Financial information with different formats but exact amounts\n",
      "Expected: Total payment due: $1,543.27\n",
      "Actual: Amount to be paid: $1,543.27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: Total payment due: $1,543.27\n",
      "Actual: Amount to be paid: $1,543.27\n",
      "Threshold: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.7884\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: False\n",
      "Score: 0.7884301092781865\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Total payment due: $1,543.27\n",
      "Actual: Amount to be paid: $1,543.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': 'dbbd9a9c-e4b7-4faa-a3c5-0328f8bc1d91', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:46 GMT', 'content-type': 'application/json', 'content-length': '477', 'connection': 'keep-alive', 'x-amzn-requestid': 'dbbd9a9c-e4b7-4faa-a3c5-0328f8bc1d91'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values convey the same financial information, with only minor differences in wording and formatting. \\'Total payment due\\' and \\'Amount to be paid\\' are semantically equivalent, and the dollar amounts match exactly.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 213, 'outputTokens': 67, 'totalTokens': 280}, 'metrics': {'latencyMs': 710}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for test_attr (from code block): match=True, score=0.95, reason=Both values convey the same financial information, with only minor differences in wording and formatting. 'Total payment due' and 'Amount to be paid' are semantically equivalent, and the dollar amounts match exactly.\n",
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.95\n",
      "Reason: Both values convey the same financial information, with only minor differences in wording and formatting. 'Total payment due' and 'Amount to be paid' are semantically equivalent, and the dollar amounts match exactly.\n",
      "\n",
      "Comparison of results:\n",
      "Method     Matched    Score      Reason\n",
      "--------------------------------------------------------------------------------\n",
      "SEMANTIC   False      0.7884     \n",
      "LLM        True       0.9500     Both values convey the same financial information, with only minor differences in wording and formatting. 'Total payment due' and 'Amount to be paid' are semantically equivalent, and the dollar amounts match exactly.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comparison: Implicit vs explicit information\n",
      "Expected: Room temperature maintained at 72°F during the experiment.\n",
      "Actual: The experiment was conducted in standard laboratory conditions at room temperature.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: SEMANTIC\n",
      "Expected: Room temperature maintained at 72°F during the experiment.\n",
      "Actual: The experiment was conducted in standard laboratory conditions at room temperature.\n",
      "Threshold: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.8604\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.8603925642525886\n",
      "\n",
      "------------------------------------------------------------\n",
      "Method: LLM\n",
      "Expected: Room temperature maintained at 72°F during the experiment.\n",
      "Actual: The experiment was conducted in standard laboratory conditions at room temperature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '62124888-9690-4f9c-b887-00cc129744ff', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:48 GMT', 'content-type': 'application/json', 'content-length': '447', 'connection': 'keep-alive', 'x-amzn-requestid': '62124888-9690-4f9c-b887-00cc129744ff'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both the expected and actual values convey that the experiment was conducted at room temperature, which is semantically equivalent to \\'72°F\\' being a specific detail of room temperature.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 209, 'outputTokens': 62, 'totalTokens': 271}, 'metrics': {'latencyMs': 648}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for test_attr (from code block): match=True, score=0.95, reason=Both the expected and actual values convey that the experiment was conducted at room temperature, which is semantically equivalent to '72°F' being a specific detail of room temperature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n",
      "Score: 0.95\n",
      "Reason: Both the expected and actual values convey that the experiment was conducted at room temperature, which is semantically equivalent to '72°F' being a specific detail of room temperature.\n",
      "\n",
      "Comparison of results:\n",
      "Method     Matched    Score      Reason\n",
      "--------------------------------------------------------------------------------\n",
      "SEMANTIC   True       0.8604     \n",
      "LLM        True       0.9500     Both the expected and actual values convey that the experiment was conducted at room temperature, which is semantically equivalent to '72°F' being a specific detail of room temperature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'semantic': (True, 0.8603925642525886, None),\n",
       " 'llm': (True,\n",
       "  0.95,\n",
       "  \"Both the expected and actual values convey that the experiment was conducted at room temperature, which is semantically equivalent to '72°F' being a specific detail of room temperature.\")}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to compare both methods\n",
    "def compare_semantic_vs_llm(expected: str, actual: str, description: str = \"Test comparison\"):\n",
    "    \"\"\"Compare SEMANTIC and LLM methods on the same input.\"\"\"\n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(f\"Comparison: {description}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Actual: {actual}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    # Run semantic comparison\n",
    "    semantic_matched, semantic_score, semantic_reason = test_comparison(\n",
    "        EvaluationMethod.SEMANTIC,\n",
    "        expected,\n",
    "        actual,\n",
    "        threshold=0.8\n",
    "    )\n",
    "    \n",
    "    # Run LLM comparison\n",
    "    llm_matched, llm_score, llm_reason = test_comparison(\n",
    "        EvaluationMethod.LLM,\n",
    "        expected,\n",
    "        actual,\n",
    "        document_class=\"TestDoc\",\n",
    "        attr_name=\"test_attr\",\n",
    "        attr_description=\"Test attribute\"\n",
    "    )\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\nComparison of results:\")\n",
    "    print(f\"{'Method':<10} {'Matched':<10} {'Score':<10} {'Reason'}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"{'SEMANTIC':<10} {semantic_matched!s:<10} {semantic_score:<10.4f} {semantic_reason or ''}\")\n",
    "    print(f\"{'LLM':<10} {llm_matched!s:<10} {llm_score:<10.4f} {llm_reason or ''}\")\n",
    "    \n",
    "    return {\n",
    "        \"semantic\": (semantic_matched, semantic_score, semantic_reason),\n",
    "        \"llm\": (llm_matched, llm_score, llm_reason)\n",
    "    }\n",
    "\n",
    "# Test cases where both methods should give similar results\n",
    "compare_semantic_vs_llm(\n",
    "    \"Patient diagnosed with pneumonia and prescribed antibiotics for 10 days.\",\n",
    "    \"The patient has pneumonia and was given a 10-day course of antibiotics.\",\n",
    "    \"Similar medical information with different wording\"\n",
    ")\n",
    "\n",
    "# Test cases where SEMANTIC might be better\n",
    "compare_semantic_vs_llm(\n",
    "    \"Total payment due: $1,543.27\",\n",
    "    \"Amount to be paid: $1,543.27\",\n",
    "    \"Financial information with different formats but exact amounts\"\n",
    ")\n",
    "\n",
    "# Test cases where LLM might be better\n",
    "compare_semantic_vs_llm(\n",
    "    \"Room temperature maintained at 72°F during the experiment.\",\n",
    "    \"The experiment was conducted in standard laboratory conditions at room temperature.\",\n",
    "    \"Implicit vs explicit information\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup test config\n",
    "test_config = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"name\": \"TestDocument\",\n",
    "            \"attributes\": [\n",
    "                {\n",
    "                    \"name\": \"exact_match_attr\",\n",
    "                    \"description\": \"Attribute for exact matching\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"numeric_attr\",\n",
    "                    \"description\": \"Attribute for numeric matching\",\n",
    "                    \"evaluation_method\": \"NUMERIC_EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"fuzzy_attr\",\n",
    "                    \"description\": \"Attribute for fuzzy matching\",\n",
    "                    \"evaluation_method\": \"FUZZY\",\n",
    "                    \"evaluation_threshold\": 0.8\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"list_attr\",\n",
    "                    \"description\": \"Attribute for list comparison\",\n",
    "                    \"evaluation_method\": \"HUNGARIAN\",\n",
    "                    \"hungarian_comparator\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"list_attr_fuzzy\",\n",
    "                    \"description\": \"Attribute for list comparison with fuzzy matching\",\n",
    "                    \"evaluation_method\": \"HUNGARIAN\",\n",
    "                    \"hungarian_comparator\": \"FUZZY\",\n",
    "                    \"evaluation_threshold\": 0.7  # Threshold for fuzzy matching within Hungarian\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"list_attr_numeric\",\n",
    "                    \"description\": \"Attribute for list comparison with numeric matching\",\n",
    "                    \"evaluation_method\": \"HUNGARIAN\",\n",
    "                    \"hungarian_comparator\": \"NUMERIC\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"llm_attr\",\n",
    "                    \"description\": \"Attribute for semantic comparison\",\n",
    "                    \"evaluation_method\": \"LLM\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_in_actual\",\n",
    "                    \"description\": \"Attribute missing in actual results\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_in_expected\",\n",
    "                    \"description\": \"Attribute missing in expected results\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_everywhere\",\n",
    "                    \"description\": \"Attribute missing in both expected and actual\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"semantic_attr\",\n",
    "                    \"description\": \"Attribute for semantic embedding comparison\",\n",
    "                    \"evaluation_method\": \"SEMANTIC\",\n",
    "                    \"evaluation_threshold\": 0.8\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"evaluation\": {\n",
    "        \"llm_method\": {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator for document extraction attributes.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.  Here's the exact format:\n",
    "{\n",
    "  \"match\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"Your explanation here\"\n",
    "}\n",
    "\"\"\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update mock S3 retrieval function to include semantic_attr\n",
    "def mock_s3_get_json(uri: str) -> Dict[str, Any]:\n",
    "    \"\"\"Mock S3 file retrieval.\"\"\"\n",
    "    if \"expected\" in uri:\n",
    "        return {\n",
    "            \"exact_match_attr\": \"Exact Match Value\",\n",
    "            \"numeric_attr\": \"$1,250.00\",\n",
    "            \"fuzzy_attr\": \"John Alexander Smith\",\n",
    "            \"list_attr\": [\"Item 1\", \"Item 2\", \"Item 3\"],\n",
    "            \"list_attr_fuzzy\": [\"Payment method: Credit Card\", \"Due date: Jan 15, 2023\", \"Reference: ABC-123\"],\n",
    "            \"list_attr_numeric\": [\"$500.00\", \"$150.00\", \"$200.00\"],\n",
    "            \"llm_attr\": \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "            \"semantic_attr\": \"Patient was diagnosed with hypertension and prescribed lisinopril 10mg daily.\",\n",
    "            \"missing_in_actual\": \"This value exists in expected only\",\n",
    "            # missing_in_expected is intentionally omitted\n",
    "            # missing_everywhere is intentionally omitted\n",
    "        }\n",
    "    else:  # actual results\n",
    "        return {\n",
    "            \"exact_match_attr\": \"Exact Match Value\",  # Exact match\n",
    "            \"numeric_attr\": 1250,  # Numeric match\n",
    "            \"fuzzy_attr\": \"John A Smith\",  # Fuzzy match\n",
    "            \"list_attr\": [\"Item 1\", \"Item 3\", \"Item 2\"],  # List with different order\n",
    "            \"list_attr_fuzzy\": [\"Reference: ABC123\", \"Payment: CC\", \"Due: January 15, 2023\"],  # Fuzzy matches\n",
    "            \"list_attr_numeric\": [500, 150, 200],  # Numeric comparison (now matches)\n",
    "            \"llm_attr\": \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",  # Semantic match\n",
    "            \"semantic_attr\": \"Patient has high blood pressure and was given medication to take once per day.\",  # Semantic embedding match\n",
    "            # missing_in_actual is intentionally omitted\n",
    "            \"missing_in_expected\": \"This value exists in actual only\",\n",
    "            # missing_everywhere is intentionally omitted\n",
    "        }\n",
    "\n",
    "# Set up mock storage - we'll still use this for S3\n",
    "class MockS3:\n",
    "    # Store report content for later display\n",
    "    report_content = \"\"\n",
    "    results_content = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_json_content(uri: str) -> Dict[str, Any]:\n",
    "        return mock_s3_get_json(uri)\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_content(content: Any, bucket: str, key: str, content_type: str = None):\n",
    "        print(f\"Writing content to s3://{bucket}/{key}\")\n",
    "        if key.endswith(\"results.json\"):\n",
    "            # Store the results for later access\n",
    "            MockS3.results_content = content\n",
    "            print(f\"Evaluation results summary: {json.dumps(content.get('overall_metrics', {}), indent=2)}\")\n",
    "        elif key.endswith(\"report.md\"):\n",
    "            # Store the markdown report for later display\n",
    "            MockS3.report_content = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock documents for evaluation\n",
    "def create_test_document(doc_id: str, is_expected: bool = False) -> Document:\n",
    "    \"\"\"Create a test document with a section.\"\"\"\n",
    "    section = Section(\n",
    "        section_id=\"sec-001\",\n",
    "        classification=\"TestDocument\",\n",
    "        extraction_result_uri=f\"s3://test-bucket/{doc_id}/{'expected' if is_expected else 'actual'}/extraction.json\"\n",
    "    )\n",
    "    \n",
    "    doc = Document(\n",
    "        id=doc_id,\n",
    "        sections=[section],\n",
    "        input_key=doc_id,\n",
    "        input_bucket=\"test-bucket\",\n",
    "        output_bucket=\"test-bucket\",\n",
    "        status=Status.EXTRACTED\n",
    "    )\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Create test documents\n",
    "actual_doc = create_test_document(\"test-doc-001\")\n",
    "expected_doc = create_test_document(\"test-doc-001-baseline\", is_expected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.evaluation.service:Initialized evaluation service with LLM configuration and max_workers=10\n",
      "INFO:idp_common.evaluation.service:Comparing: exact_match_attr using EvaluationMethod.EXACT - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: numeric_attr using EvaluationMethod.NUMERIC_EXACT - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: fuzzy_attr using EvaluationMethod.FUZZY - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: list_attr using EvaluationMethod.HUNGARIAN - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: list_attr_fuzzy using EvaluationMethod.HUNGARIAN - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: list_attr_numeric using EvaluationMethod.HUNGARIAN - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: missing_in_actual using EvaluationMethod.EXACT - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: missing_in_expected using EvaluationMethod.EXACT - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: missing_everywhere using EvaluationMethod.EXACT - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: llm_attr using EvaluationMethod.LLM - from class TestDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: semantic_attr using EvaluationMethod.SEMANTIC - from class TestDocument\n",
      "INFO:idp_common.evaluation.comparator:Generating embeddings for semantic comparison using model: amazon.titan-embed-text-v1\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '97eea3b7-ed4f-4ac4-89a4-ec49bf3841b6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:50 GMT', 'content-type': 'application/json', 'content-length': '424', 'connection': 'keep-alive', 'x-amzn-requestid': '97eea3b7-ed4f-4ac4-89a4-ec49bf3841b6'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values describe the same financial transactions and resulting balance, despite slight differences in wording and structure. The semantic meaning is preserved.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 235, 'outputTokens': 52, 'totalTokens': 287}, 'metrics': {'latencyMs': 501}}\n",
      "INFO:idp_common.bedrock:Bedrock embedding request attempt 1/8:\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for llm_attr (from code block): match=True, score=0.95, reason=Both values describe the same financial transactions and resulting balance, despite slight differences in wording and structure. The semantic meaning is preserved.\n",
      "INFO:idp_common.evaluation.comparator:Semantic similarity score: 0.7978\n",
      "INFO:idp_common.evaluation.service:Evaluation complete for document test-doc-001 in 1.48 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing content to s3://test-bucket/test-doc-001/evaluation/results.json\n",
      "Evaluation results summary: {\n",
      "  \"precision\": 0.5555555555555556,\n",
      "  \"recall\": 0.8333333333333334,\n",
      "  \"f1_score\": 0.6666666666666667,\n",
      "  \"accuracy\": 0.5454545454545454,\n",
      "  \"false_alarm_rate\": 0.5,\n",
      "  \"false_discovery_rate\": 0.375\n",
      "}\n",
      "Writing content to s3://test-bucket/test-doc-001/evaluation/report.md\n",
      "\n",
      "Overall metrics: {'precision': 0.5555555555555556, 'recall': 0.8333333333333334, 'f1_score': 0.6666666666666667, 'accuracy': 0.5454545454545454, 'false_alarm_rate': 0.5, 'false_discovery_rate': 0.375}\n",
      "\n",
      "Section sec-001 - Class: TestDocument\n",
      "Metrics: {'precision': 0.5555555555555556, 'recall': 0.8333333333333334, 'f1_score': 0.6666666666666667, 'accuracy': 0.5454545454545454, 'false_alarm_rate': 0.5, 'false_discovery_rate': 0.375}\n",
      "\n",
      "Attribute Details:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Name                 Method          Expected                  Actual                    Matched    Score      Reason\n",
      "----------------------------------------------------------------------------------------------------\n",
      "exact_match_attr     EXACT           Exact Match Value         Exact Match Value         True       1.00       \n",
      "fuzzy_attr           FUZZY           John Alexander Smith      John A Smith              False      0.60       \n",
      "list_attr            HUNGARIAN       ['Item 1', 'Item 2', 'Ite ['Item 1', 'Item 3', 'Ite True       1.00       \n",
      "list_attr_fuzzy      HUNGARIAN       ['Payment method: Credit  ['Reference: ABC123', 'Pa False      0.68       \n",
      "list_attr_numeric    HUNGARIAN       ['$500.00', '$150.00', '$ [500, 150, 200]           True       1.00       \n",
      "llm_attr             LLM             Monthly statement showing Statement with deposits t True       0.95       Both values describe the same financial transactio...\n",
      "missing_everywhere   EXACT           None                      None                      True       1.00       Both actual and expected values are missing, so th...\n",
      "missing_in_actual    EXACT           This value exists in expe None                      False      0.00       \n",
      "missing_in_expected  EXACT           None                      This value exists in actu False      0.00       \n",
      "numeric_attr         NUMERIC_EXACT   $1,250.00                 1250                      True       1.00       \n",
      "semantic_attr        SEMANTIC        Patient was diagnosed wit Patient has high blood pr False      0.80       \n"
     ]
    }
   ],
   "source": [
    "# Evaluate document\n",
    "# Only patch S3 module - use real Bedrock\n",
    "import idp_common.evaluation.service\n",
    "idp_common.evaluation.service.s3 = MockS3\n",
    "\n",
    "# Create evaluation service\n",
    "evaluation_service = EvaluationService(region=\"us-east-1\", config=test_config)\n",
    "\n",
    "# Evaluate document\n",
    "result_doc = evaluation_service.evaluate_document(actual_doc, expected_doc, store_results=True)\n",
    "\n",
    "# Print results\n",
    "if hasattr(result_doc, 'evaluation_result'):\n",
    "    eval_result = result_doc.evaluation_result\n",
    "    print(f\"\\nOverall metrics: {eval_result.overall_metrics}\")\n",
    "    \n",
    "    # Check section results\n",
    "    for section_result in eval_result.section_results:\n",
    "        print(f\"\\nSection {section_result.section_id} - Class: {section_result.document_class}\")\n",
    "        print(f\"Metrics: {section_result.metrics}\")\n",
    "        \n",
    "        # Print attribute details\n",
    "        print(\"\\nAttribute Details:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Name':<20} {'Method':<15} {'Expected':<25} {'Actual':<25} {'Matched':<10} {'Score':<10} {'Reason'}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for attr in section_result.attributes:\n",
    "            expected_val = str(attr.expected)[:25]\n",
    "            actual_val = str(attr.actual)[:25]\n",
    "            method = attr.evaluation_method\n",
    "            reason = attr.reason[:50] + \"...\" if attr.reason and len(attr.reason) > 50 else (attr.reason or \"\")\n",
    "            print(f\"{attr.name:<20} {method:<15} {expected_val:<25} {actual_val:<25} {attr.matched!s:<10} {attr.score:<10.2f} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Evaluation Report\n",
    "\n",
    "Let's display the markdown evaluation report that was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Document Evaluation: test-doc-001\n",
       "\n",
       "## Summary\n",
       "- **Match Rate**: 🟠 6/11 attributes matched [██████████░░░░░░░░░░] 54%\n",
       "- **Precision**: 0.56 | **Recall**: 0.83 | **F1 Score**: 🟠 0.67\n",
       "\n",
       "## Overall Metrics\n",
       "| Metric | Value | Rating |\n",
       "| ------ | :----: | :----: |\n",
       "| precision | 0.5556 | 🟠 Fair |\n",
       "| recall | 0.8333 | 🟡 Good |\n",
       "| f1_score | 0.6667 | 🟠 Fair |\n",
       "| accuracy | 0.5455 | 🟠 Fair |\n",
       "| false_alarm_rate | 0.5000 | 🟠 Fair |\n",
       "| false_discovery_rate | 0.3750 | 🟠 Fair |\n",
       "\n",
       "\n",
       "## Section: sec-001 (TestDocument)\n",
       "### Metrics\n",
       "| Metric | Value | Rating |\n",
       "| ------ | :----: | :----: |\n",
       "| precision | 0.5556 | 🟠 Fair |\n",
       "| recall | 0.8333 | 🟡 Good |\n",
       "| f1_score | 0.6667 | 🟠 Fair |\n",
       "| accuracy | 0.5455 | 🟠 Fair |\n",
       "| false_alarm_rate | 0.5000 | 🟠 Fair |\n",
       "| false_discovery_rate | 0.3750 | 🟠 Fair |\n",
       "\n",
       "\n",
       "### Attributes\n",
       "| Status | Attribute | Expected | Actual | Score | Method | Reason |\n",
       "| :----: | --------- | -------- | ------ | ----- | ------ | ------ |\n",
       "| ✅ | exact_match_attr | Exact Match Value | Exact Match Value | 1.00 | EXACT |  |\n",
       "| ❌ | fuzzy_attr | John Alexander Smith | John A Smith | 0.60 | FUZZY (threshold: 0.8) |  |\n",
       "| ✅ | list_attr | ['Item 1', 'Item 2', 'Item 3'] | ['Item 1', 'Item 3', 'Item 2'] | 1.00 | HUNGARIAN (comparator: EXACT) |  |\n",
       "| ❌ | list_attr_fuzzy | ['Payment method: Credit Card', 'Due date: Jan 15, 2023', 'Reference: ABC-123'] | ['Reference: ABC123', 'Payment: CC', 'Due: January 15, 2023'] | 0.68 | HUNGARIAN (comparator: FUZZY, threshold: 0.7) |  |\n",
       "| ✅ | list_attr_numeric | ['$500.00', '$150.00', '$200.00'] | [500, 150, 200] | 1.00 | HUNGARIAN (comparator: NUMERIC) |  |\n",
       "| ✅ | llm_attr | Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400. | Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400. | 0.95 | LLM | Both values describe the same financial transactions and resulting balance, despite slight differences in wording and structure. The semantic meaning is preserved. |\n",
       "| ✅ | missing_everywhere | None | None | 1.00 | EXACT | Both actual and expected values are missing, so they are matched. |\n",
       "| ❌ | missing_in_actual | This value exists in expected only | None | 0.00 | EXACT |  |\n",
       "| ❌ | missing_in_expected | None | This value exists in actual only | 0.00 | EXACT |  |\n",
       "| ✅ | numeric_attr | $1,250.00 | 1250 | 1.00 | NUMERIC_EXACT |  |\n",
       "| ❌ | semantic_attr | Patient was diagnosed with hypertension and prescribed lisinopril 10mg daily. | Patient has high blood pressure and was given medication to take once per day. | 0.80 | SEMANTIC (threshold: 0.8) |  |\n",
       "\n",
       "\n",
       "Execution time: 1.48 seconds\n",
       "\n",
       "## Evaluation Methods Used\n",
       "\n",
       "This evaluation used the following methods to compare expected and actual values:\n",
       "\n",
       "1. **EXACT** - Exact string match after stripping punctuation and whitespace\n",
       "2. **NUMERIC_EXACT** - Exact numeric match after normalizing\n",
       "3. **FUZZY** - Fuzzy string matching using string similarity metrics (with evaluation_threshold)\n",
       "4. **SEMANTIC** - Semantic similarity comparison using Bedrock Titan embeddings (with evaluation_threshold)\n",
       "5. **HUNGARIAN** - Bipartite matching algorithm for lists of values\n",
       "   - **EXACT** - Hungarian matching with exact string comparison\n",
       "   - **FUZZY** - Hungarian matching with fuzzy string comparison (with evaluation_threshold)\n",
       "   - **NUMERIC** - Hungarian matching with numeric comparison\n",
       "6. **LLM** - Advanced semantic evaluation using Bedrock large language models\n",
       "\n",
       "Each attribute is configured with a specific evaluation method based on the data type and comparison needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Display the markdown report\n",
    "if MockS3.report_content:\n",
    "    display(Markdown(MockS3.report_content))\n",
    "else:\n",
    "    print(\"No evaluation report was generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Smart Attribute Discovery and Evaluation\n",
    "\n",
    "This section demonstrates the \"Smart attribute discovery and evaluation\" feature of the EvaluationService, which:\n",
    "1. Automatically discovers attributes in the data not defined in configuration\n",
    "2. Applies default evaluation methods to unconfigured attributes\n",
    "3. Properly handles attributes found only in expected data, only in actual data, or in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a minimal configuration with only some attributes defined\n",
    "minimal_config = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"name\": \"InvoiceDocument\",\n",
    "            \"attributes\": [\n",
    "                {\n",
    "                    \"name\": \"invoice_number\",\n",
    "                    \"description\": \"The unique identifier for the invoice\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"amount_due\",\n",
    "                    \"description\": \"The total amount to be paid\",\n",
    "                    \"evaluation_method\": \"NUMERIC_EXACT\"\n",
    "                }\n",
    "                # Note: Other attributes are intentionally omitted from configuration\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"evaluation\": {\n",
    "        \"llm_method\": {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator for document extraction attributes.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.  Here's the exact format:\n",
    "{\n",
    "  \"match\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"Your explanation here\"\n",
    "}\n",
    "\"\"\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override S3 mock data function to simulate invoice data with unconfigured attributes\n",
    "def mock_invoice_s3_get_json(uri: str) -> Dict[str, Any]:\n",
    "    \"\"\"Mock S3 file retrieval with invoice data including unconfigured attributes.\"\"\"\n",
    "    if \"expected\" in uri:\n",
    "        return {\n",
    "            # Configured attributes\n",
    "            \"invoice_number\": \"INV-12345\",  # Configured with EXACT method\n",
    "            \"amount_due\": \"$1,250.00\",     # Configured with NUMERIC_EXACT method\n",
    "            \n",
    "            # Unconfigured attributes - only in expected\n",
    "            \"reference_number\": \"REF-98765\",\n",
    "            \n",
    "            # Unconfigured attributes - in both expected and actual\n",
    "            \"issue_date\": \"January 15, 2023\",\n",
    "            \"due_date\": \"February 15, 2023\",\n",
    "            \"vendor_name\": \"Acme Corporation Inc.\",\n",
    "            \"payment_terms\": \"Net 30\"\n",
    "        }\n",
    "    else:  # actual results\n",
    "        return {\n",
    "            # Configured attributes\n",
    "            \"invoice_number\": \"INV-12345\",  # Exact match with expected\n",
    "            \"amount_due\": 1250,            # Numeric match with expected\n",
    "            \n",
    "            # Unconfigured attributes - only in actual\n",
    "            \"purchase_order\": \"PO-54321\",\n",
    "            \n",
    "            # Unconfigured attributes - in both expected and actual with various match qualities\n",
    "            \"issue_date\": \"01/15/2023\",                     # Different format but same date\n",
    "            \"due_date\": \"02/15/2023\",                       # Different format but same date\n",
    "            \"vendor_name\": \"ACME Corp.\",                    # Abbreviated but similar\n",
    "            \"payment_terms\": \"Payment due within 30 days\"    # Different wording but same meaning\n",
    "        }\n",
    "\n",
    "# Create test invoice documents\n",
    "def create_invoice_document(doc_id: str, is_expected: bool = False) -> Document:\n",
    "    \"\"\"Create a test invoice document with a section.\"\"\"\n",
    "    section = Section(\n",
    "        section_id=\"inv-001\",\n",
    "        classification=\"InvoiceDocument\",\n",
    "        extraction_result_uri=f\"s3://test-bucket/{doc_id}/{'expected' if is_expected else 'actual'}/extraction.json\"\n",
    "    )\n",
    "    \n",
    "    doc = Document(\n",
    "        id=doc_id,\n",
    "        sections=[section],\n",
    "        input_key=doc_id,\n",
    "        input_bucket=\"test-bucket\",\n",
    "        output_bucket=\"test-bucket\",\n",
    "        status=Status.EXTRACTED\n",
    "    )\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Set up a new mock for this example\n",
    "class MockInvoiceS3:\n",
    "    # Store report content for later display\n",
    "    report_content = \"\"\n",
    "    results_content = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_json_content(uri: str) -> Dict[str, Any]:\n",
    "        return mock_invoice_s3_get_json(uri)\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_content(content: Any, bucket: str, key: str, content_type: str = None):\n",
    "        print(f\"Writing content to s3://{bucket}/{key}\")\n",
    "        if key.endswith(\"results.json\"):\n",
    "            # Store the results for later access\n",
    "            MockInvoiceS3.results_content = content\n",
    "        elif key.endswith(\"report.md\"):\n",
    "            # Store the markdown report for later display\n",
    "            MockInvoiceS3.report_content = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.evaluation.service:Initialized evaluation service with LLM configuration and max_workers=10\n",
      "INFO:idp_common.evaluation.service:Comparing: invoice_number using EvaluationMethod.EXACT - from class InvoiceDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: amount_due using EvaluationMethod.NUMERIC_EXACT - from class InvoiceDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: reference_number using EvaluationMethod.LLM - from class InvoiceDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: purchase_order using EvaluationMethod.LLM - from class InvoiceDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: vendor_name using EvaluationMethod.LLM - from class InvoiceDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: payment_terms using EvaluationMethod.LLM - from class InvoiceDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: due_date using EvaluationMethod.LLM - from class InvoiceDocument\n",
      "INFO:idp_common.evaluation.service:Comparing: issue_date using EvaluationMethod.LLM - from class InvoiceDocument\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating invoice document with smart attribute discovery...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Bedrock request attempt 1/8:\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '80859633-7665-4d07-8b2a-a6d91a395bd1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:51 GMT', 'content-type': 'application/json', 'content-length': '518', 'connection': 'keep-alive', 'x-amzn-requestid': '80859633-7665-4d07-8b2a-a6d91a395bd1'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"The expected value \\'Acme Corporation Inc.\\' and the actual value \\'ACME Corp.\\' refer to the same entity despite differences in formatting and abbreviations. \\'Acme\\' and \\'ACME\\' are case variations, \\'Corporation\\' is abbreviated to \\'Corp.\\', and \\'Inc.\\' is omitted.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 188, 'outputTokens': 90, 'totalTokens': 278}, 'metrics': {'latencyMs': 700}}\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '501344fa-e215-4a83-9b2e-a9ed055f44b0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:51 GMT', 'content-type': 'application/json', 'content-length': '469', 'connection': 'keep-alive', 'x-amzn-requestid': '501344fa-e215-4a83-9b2e-a9ed055f44b0'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values convey the same payment terms, with \\'Net 30\\' being a common abbreviation for \\'Payment due within 30 days\\'. Despite slight differences in wording and formatting, the semantic meaning is equivalent.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 193, 'outputTokens': 71, 'totalTokens': 264}, 'metrics': {'latencyMs': 664}}\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '09889121-c6c0-4a89-8cf0-e9c7728e49e6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:51 GMT', 'content-type': 'application/json', 'content-length': '442', 'connection': 'keep-alive', 'x-amzn-requestid': '09889121-c6c0-4a89-8cf0-e9c7728e49e6'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"The expected value \\'February 15, 2023\\' and the actual value \\'02/15/2023\\' represent the same date despite differences in formatting. Both values convey the same semantic information.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 203, 'outputTokens': 75, 'totalTokens': 278}, 'metrics': {'latencyMs': 500}}\n",
      "INFO:idp_common.bedrock:Response: {'ResponseMetadata': {'RequestId': '02587148-0b16-4546-a06e-3414dd01ede3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 24 Apr 2025 15:24:52 GMT', 'content-type': 'application/json', 'content-length': '475', 'connection': 'keep-alive', 'x-amzn-requestid': '02587148-0b16-4546-a06e-3414dd01ede3'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '```json\\n{\\n  \"match\": true,\\n  \"score\": 0.95,\\n  \"reason\": \"Both values represent the same date, January 15, 2023, despite differences in formatting. The expected value is in a word format while the actual value is in a numeric format, but both clearly convey the same date.\"\\n}\\n```'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 203, 'outputTokens': 76, 'totalTokens': 279}, 'metrics': {'latencyMs': 556}}\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for vendor_name (from code block): match=True, score=0.95, reason=The expected value 'Acme Corporation Inc.' and the actual value 'ACME Corp.' refer to the same entity despite differences in formatting and abbreviations. 'Acme' and 'ACME' are case variations, 'Corporation' is abbreviated to 'Corp.', and 'Inc.' is omitted.\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for due_date (from code block): match=True, score=0.95, reason=The expected value 'February 15, 2023' and the actual value '02/15/2023' represent the same date despite differences in formatting. Both values convey the same semantic information.\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for payment_terms (from code block): match=True, score=0.95, reason=Both values convey the same payment terms, with 'Net 30' being a common abbreviation for 'Payment due within 30 days'. Despite slight differences in wording and formatting, the semantic meaning is equivalent.\n",
      "INFO:idp_common.evaluation.comparator:LLM evaluation for issue_date (from code block): match=True, score=0.95, reason=Both values represent the same date, January 15, 2023, despite differences in formatting. The expected value is in a word format while the actual value is in a numeric format, but both clearly convey the same date.\n",
      "INFO:idp_common.evaluation.service:Evaluation complete for document invoice-001 in 3.10 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing content to s3://test-bucket/invoice-001/evaluation/results.json\n",
      "Writing content to s3://test-bucket/invoice-001/evaluation/report.md\n",
      "\n",
      "Overall metrics: {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1_score': 0.8571428571428571, 'accuracy': 0.75, 'false_alarm_rate': 1.0, 'false_discovery_rate': 0.0}\n",
      "\n",
      "Section inv-001 - Class: InvoiceDocument\n",
      "Total attributes evaluated: 8\n",
      "  - Configured attributes: 8\n",
      "  - Auto-discovered attributes: 0\n",
      "\n",
      "CONFIGURED Attribute Details:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Name                 Method          Expected                  Actual                    Matched    Score     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "amount_due           NUMERIC_EXACT   $1,250.00                 1250                      True       1.00      \n",
      "due_date             LLM             February 15, 2023         02/15/2023                True       0.95      \n",
      "invoice_number       EXACT           INV-12345                 INV-12345                 True       1.00      \n",
      "issue_date           LLM             January 15, 2023          01/15/2023                True       0.95      \n",
      "payment_terms        LLM             Net 30                    Payment due within 30 day True       0.95      \n",
      "purchase_order       LLM             None                      PO-54321                  False      0.00      \n",
      "reference_number     LLM             REF-98765                 None                      False      0.00      \n",
      "vendor_name          LLM             Acme Corporation Inc.     ACME Corp.                True       0.95      \n"
     ]
    }
   ],
   "source": [
    "# Evaluate document with smart attribute discovery\n",
    "# Patch S3 module with our invoice mock\n",
    "idp_common.evaluation.service.s3 = MockInvoiceS3\n",
    "\n",
    "# Create test documents\n",
    "actual_invoice = create_invoice_document(\"invoice-001\")\n",
    "expected_invoice = create_invoice_document(\"invoice-001-baseline\", is_expected=True)\n",
    "\n",
    "# Create evaluation service with minimal config\n",
    "evaluation_service = EvaluationService(region=\"us-east-1\", config=minimal_config)\n",
    "\n",
    "# Evaluate document\n",
    "print(\"Evaluating invoice document with smart attribute discovery...\")\n",
    "result_doc = evaluation_service.evaluate_document(actual_invoice, expected_invoice, store_results=True)\n",
    "\n",
    "# Print results with focus on discovered attributes\n",
    "if hasattr(result_doc, 'evaluation_result'):\n",
    "    eval_result = result_doc.evaluation_result\n",
    "    print(f\"\\nOverall metrics: {eval_result.overall_metrics}\")\n",
    "    \n",
    "    # Check section results\n",
    "    for section_result in eval_result.section_results:\n",
    "        print(f\"\\nSection {section_result.section_id} - Class: {section_result.document_class}\")\n",
    "        \n",
    "        # Find configured vs unconfigured attributes\n",
    "        configured_attrs = []\n",
    "        unconfigured_attrs = []\n",
    "        \n",
    "        for attr in section_result.attributes:\n",
    "            # Check if the attribute has a message about being unconfigured\n",
    "            if attr.reason and \"attribute not in the configuration\" in attr.reason:\n",
    "                unconfigured_attrs.append(attr)\n",
    "            else:\n",
    "                configured_attrs.append(attr)\n",
    "        \n",
    "        # Print summary of attribute counts\n",
    "        print(f\"Total attributes evaluated: {len(section_result.attributes)}\")\n",
    "        print(f\"  - Configured attributes: {len(configured_attrs)}\")\n",
    "        print(f\"  - Auto-discovered attributes: {len(unconfigured_attrs)}\")\n",
    "        \n",
    "        # Print attribute details - CONFIGURED\n",
    "        if configured_attrs:\n",
    "            print(\"\\nCONFIGURED Attribute Details:\")\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"{'Name':<20} {'Method':<15} {'Expected':<25} {'Actual':<25} {'Matched':<10} {'Score':<10}\")\n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            for attr in configured_attrs:\n",
    "                expected_val = str(attr.expected)[:25]\n",
    "                actual_val = str(attr.actual)[:25]\n",
    "                method = attr.evaluation_method\n",
    "                print(f\"{attr.name:<20} {method:<15} {expected_val:<25} {actual_val:<25} {attr.matched!s:<10} {attr.score:<10.2f}\")\n",
    "        \n",
    "        # Print attribute details - UNCONFIGURED\n",
    "        if unconfigured_attrs:\n",
    "            print(\"\\nAUTO-DISCOVERED Attribute Details:\")\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"{'Name':<20} {'Method':<15} {'Expected':<25} {'Actual':<25} {'Matched':<10} {'Score':<10} {'Reason'}\")\n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            for attr in unconfigured_attrs:\n",
    "                expected_val = str(attr.expected)[:25]\n",
    "                actual_val = str(attr.actual)[:25]\n",
    "                method = attr.evaluation_method\n",
    "                reason = attr.reason[:50] + \"...\" if attr.reason and len(attr.reason) > 50 else (attr.reason or \"\")\n",
    "                print(f\"{attr.name:<20} {method:<15} {expected_val:<25} {actual_val:<25} {attr.matched!s:<10} {attr.score:<10.2f} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Smart Attribute Discovery Evaluation Report\n",
    "\n",
    "Let's display the markdown evaluation report that was generated for the smart attribute discovery scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Document Evaluation: invoice-001\n",
       "\n",
       "## Summary\n",
       "- **Match Rate**: 🟡 6/8 attributes matched [███████████████░░░░░] 75%\n",
       "- **Precision**: 0.86 | **Recall**: 0.86 | **F1 Score**: 🟡 0.86\n",
       "\n",
       "## Overall Metrics\n",
       "| Metric | Value | Rating |\n",
       "| ------ | :----: | :----: |\n",
       "| precision | 0.8571 | 🟡 Good |\n",
       "| recall | 0.8571 | 🟡 Good |\n",
       "| f1_score | 0.8571 | 🟡 Good |\n",
       "| accuracy | 0.7500 | 🟡 Good |\n",
       "| false_alarm_rate | 1.0000 | 🔴 Poor |\n",
       "| false_discovery_rate | 0.0000 | 🟢 Excellent |\n",
       "\n",
       "\n",
       "## Section: inv-001 (InvoiceDocument)\n",
       "### Metrics\n",
       "| Metric | Value | Rating |\n",
       "| ------ | :----: | :----: |\n",
       "| precision | 0.8571 | 🟡 Good |\n",
       "| recall | 0.8571 | 🟡 Good |\n",
       "| f1_score | 0.8571 | 🟡 Good |\n",
       "| accuracy | 0.7500 | 🟡 Good |\n",
       "| false_alarm_rate | 1.0000 | 🔴 Poor |\n",
       "| false_discovery_rate | 0.0000 | 🟢 Excellent |\n",
       "\n",
       "\n",
       "### Attributes\n",
       "| Status | Attribute | Expected | Actual | Score | Method | Reason |\n",
       "| :----: | --------- | -------- | ------ | ----- | ------ | ------ |\n",
       "| ✅ | amount_due | $1,250.00 | 1250 | 1.00 | NUMERIC_EXACT |  |\n",
       "| ✅ | due_date | February 15, 2023 | 02/15/2023 | 0.95 | LLM | The expected value 'February 15, 2023' and the actual value '02/15/2023' represent the same date despite differences in formatting. Both values convey the same semantic information. [Default method - attribute not specified in the configuration] |\n",
       "| ✅ | invoice_number | INV-12345 | INV-12345 | 1.00 | EXACT |  |\n",
       "| ✅ | issue_date | January 15, 2023 | 01/15/2023 | 0.95 | LLM | Both values represent the same date, January 15, 2023, despite differences in formatting. The expected value is in a word format while the actual value is in a numeric format, but both clearly convey the same date. [Default method - attribute not specified in the configuration] |\n",
       "| ✅ | payment_terms | Net 30 | Payment due within 30 days | 0.95 | LLM | Both values convey the same payment terms, with 'Net 30' being a common abbreviation for 'Payment due within 30 days'. Despite slight differences in wording and formatting, the semantic meaning is equivalent. [Default method - attribute not specified in the configuration] |\n",
       "| ❌ | purchase_order | None | PO-54321 | 0.00 | LLM | [Default method - attribute not specified in the configuration] |\n",
       "| ❌ | reference_number | REF-98765 | None | 0.00 | LLM | [Default method - attribute not specified in the configuration] |\n",
       "| ✅ | vendor_name | Acme Corporation Inc. | ACME Corp. | 0.95 | LLM | The expected value 'Acme Corporation Inc.' and the actual value 'ACME Corp.' refer to the same entity despite differences in formatting and abbreviations. 'Acme' and 'ACME' are case variations, 'Corporation' is abbreviated to 'Corp.', and 'Inc.' is omitted. [Default method - attribute not specified in the configuration] |\n",
       "\n",
       "\n",
       "Execution time: 3.10 seconds\n",
       "\n",
       "## Evaluation Methods Used\n",
       "\n",
       "This evaluation used the following methods to compare expected and actual values:\n",
       "\n",
       "1. **EXACT** - Exact string match after stripping punctuation and whitespace\n",
       "2. **NUMERIC_EXACT** - Exact numeric match after normalizing\n",
       "3. **FUZZY** - Fuzzy string matching using string similarity metrics (with evaluation_threshold)\n",
       "4. **SEMANTIC** - Semantic similarity comparison using Bedrock Titan embeddings (with evaluation_threshold)\n",
       "5. **HUNGARIAN** - Bipartite matching algorithm for lists of values\n",
       "   - **EXACT** - Hungarian matching with exact string comparison\n",
       "   - **FUZZY** - Hungarian matching with fuzzy string comparison (with evaluation_threshold)\n",
       "   - **NUMERIC** - Hungarian matching with numeric comparison\n",
       "6. **LLM** - Advanced semantic evaluation using Bedrock large language models\n",
       "\n",
       "Each attribute is configured with a specific evaluation method based on the data type and comparison needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the markdown report\n",
    "if MockInvoiceS3.report_content:\n",
    "    display(Markdown(MockInvoiceS3.report_content))\n",
    "else:\n",
    "    print(\"No evaluation report was generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Attribute Discovery Scenario Summary\n",
    "\n",
    "The smart attribute discovery and evaluation feature provides the following benefits:\n",
    "\n",
    "1. **Auto-discovery of attributes**\n",
    "   - Finds attributes not explicitly defined in the configuration\n",
    "   - Compares all data fields across expected and actual results\n",
    "   - Works with minimal or even no attribute configuration\n",
    "\n",
    "2. **Default Evaluation Method**\n",
    "   - Applies LLM method to unconfigured attributes by default \n",
    "   - Provides semantic comparison for discovered attributes\n",
    "   - Attaches explanations that the attribute was not in configuration\n",
    "\n",
    "3. **Handles All Possible Cases**\n",
    "   - Attributes in both expected and actual results\n",
    "   - Attributes only in expected results (false negatives)\n",
    "   - Attributes only in actual results (false positives)\n",
    "   - Attributes that don't exist in either (true negatives)\n",
    "\n",
    "4. **Benefits**\n",
    "   - Exploratory evaluation without complete configuration\n",
    "   - Comprehensive metrics that include all found attributes\n",
    "   - Flexibility as extraction models evolve or change output formats\n",
    "   - Identification of potential new attributes to add to configuration\n",
    "\n",
    "This feature is particularly useful during the early stages of implementation when the complete attribute schema may not be fully defined, or when handling variations in extraction outputs that contain unexpected information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## Summary of All Demonstrated Features\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. All evaluation methods available in the IDP library:\n",
    "   - EXACT - Exact string matching\n",
    "   - NUMERIC_EXACT - Numeric value matching\n",
    "   - FUZZY - Fuzzy string matching with adjustable thresholds\n",
    "   - SEMANTIC - Semantic similarity comparison using Titan embeddings\n",
    "   - HUNGARIAN - List comparison using the Hungarian algorithm with configurable comparator types:\n",
    "     - EXACT comparator - Exact string matching for list items\n",
    "     - FUZZY comparator - Fuzzy string matching for list items\n",
    "     - NUMERIC comparator - Numeric comparison for list items\n",
    "   - LLM - Semantic comparison using Large Language Models\n",
    "\n",
    "2. Semantic Comparison Methods:\n",
    "   - SEMANTIC - Uses Bedrock Titan embeddings and cosine similarity for efficient matching\n",
    "   - LLM - Uses Bedrock Claude for more nuanced semantic understanding with reasoning\n",
    "\n",
    "3. Benefits of SEMANTIC vs LLM methods:\n",
    "   - SEMANTIC is faster and less expensive than LLM-based evaluation\n",
    "   - LLM provides explanations for matches/mismatches\n",
    "   - SEMANTIC works well for standard text comparisons\n",
    "   - LLM better understands implicit information and complex reasoning\n",
    "\n",
    "4. Handling of edge cases:\n",
    "   - Attributes missing in actual results\n",
    "   - Attributes missing in expected results\n",
    "   - Attributes missing in both actual and expected results\n",
    "   - Empty string values\n",
    "\n",
    "5. Full document evaluation with mixed evaluation methods\n",
    "   - Comprehensive metrics calculation\n",
    "   - Detailed attribute-level results\n",
    "\n",
    "6. Threshold sensitivity analysis for fuzzy and semantic matching\n",
    "   - How different threshold values affect match results\n",
    "   - Trade-offs between precision and recall\n",
    "\n",
    "7. Smart attribute discovery and evaluation:\n",
    "   - Auto-discovery of attributes not in configuration\n",
    "   - Default semantic evaluation with LLM method\n",
    "   - Comprehensive handling of all attribute cases\n",
    "   - Support for exploratory evaluation and evolving schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. All evaluation methods available in the IDP library:\n",
    "   - EXACT - Exact string matching\n",
    "   - NUMERIC_EXACT - Numeric value matching\n",
    "   - FUZZY - Fuzzy string matching with adjustable thresholds\n",
    "   - SEMANTIC - Semantic similarity comparison using Titan embeddings\n",
    "   - HUNGARIAN - List comparison using the Hungarian algorithm\n",
    "   - LLM - Semantic comparison using Large Language Models\n",
    "\n",
    "2. Semantic Comparison Methods:\n",
    "   - SEMANTIC - Uses Bedrock Titan embeddings and cosine similarity for efficient matching\n",
    "   - LLM - Uses Bedrock Claude for more nuanced semantic understanding with reasoning\n",
    "\n",
    "3. Benefits of SEMANTIC vs LLM methods:\n",
    "   - SEMANTIC is faster and less expensive than LLM-based evaluation\n",
    "   - LLM provides explanations for matches/mismatches\n",
    "   - SEMANTIC works well for standard text comparisons\n",
    "   - LLM better understands implicit information and complex reasoning\n",
    "\n",
    "4. Handling of edge cases:\n",
    "   - Attributes missing in actual results\n",
    "   - Attributes missing in expected results\n",
    "   - Attributes missing in both actual and expected results\n",
    "   - Empty string values\n",
    "\n",
    "5. Full document evaluation with mixed evaluation methods\n",
    "   - Comprehensive metrics calculation\n",
    "   - Detailed attribute-level results\n",
    "\n",
    "6. Threshold sensitivity analysis for fuzzy and semantic matching\n",
    "   - How different threshold values affect match results\n",
    "   - Trade-offs between precision and recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
