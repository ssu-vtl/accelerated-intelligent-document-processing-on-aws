{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Methods Demonstration\n",
    "\n",
    "This notebook demonstrates the various evaluation methods available in the IDP library for comparing expected values with actual extraction results. It covers:\n",
    "\n",
    "1. All evaluation methods with both match and no-match scenarios\n",
    "2. Threshold testing for applicable methods\n",
    "3. Edge cases:\n",
    "   - Attribute not found in actual results\n",
    "   - Attribute not found in expected results\n",
    "   - Attribute not found in either actual or expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure that modules are autoreloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# First uninstall existing package (to ensure we get the latest version)\n",
    "%pip uninstall -y idp_common\n",
    "\n",
    "# Install the IDP common package with all components in development mode\n",
    "%pip install -q -e \"../lib/idp_common_pkg[dev, all]\"\n",
    "\n",
    "# Optionally use a .env file for environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  \n",
    "except ImportError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Add parent directory to path to import the library\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Import IDP libraries\n",
    "from idp_common.evaluation.models import EvaluationMethod\n",
    "from idp_common.evaluation.comparator import compare_values\n",
    "from idp_common.evaluation.service import EvaluationService\n",
    "from idp_common.models import Document, Section, Status\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Comparing Individual Values with Different Methods\n",
    "\n",
    "We'll test each evaluation method with matching and non-matching examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_comparison(method: EvaluationMethod, expected: Any, actual: Any, \n",
    "                    threshold: float = 0.8, document_class: str = \"TestDoc\",\n",
    "                    attr_name: str = \"test_attr\", attr_description: str = \"Test attribute\"):\n",
    "    \"\"\"Test a comparison method and print results.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Method: {method.name}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Actual: {actual}\")\n",
    "    \n",
    "    if method in [EvaluationMethod.FUZZY, EvaluationMethod.SEMANTIC]:\n",
    "        print(f\"Threshold: {threshold}\")\n",
    "    \n",
    "    # Set up LLM config for the LLM method\n",
    "    llm_config = None\n",
    "    if method == EvaluationMethod.LLM:\n",
    "        llm_config = {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator that helps determine if the predicted and expected values match for document attribute extraction.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.  Here's the exact format:\n",
    "{\n",
    "  \"match\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"Your explanation here\"\n",
    "}\n",
    "\"\"\"\n",
    "        }\n",
    "    \n",
    "    # Perform the comparison\n",
    "    matched, score, reason = compare_values(\n",
    "        expected=expected,\n",
    "        actual=actual,\n",
    "        method=method,\n",
    "        threshold=threshold,\n",
    "        document_class=document_class,\n",
    "        attr_name=attr_name,\n",
    "        attr_description=attr_description,\n",
    "        llm_config=llm_config\n",
    "    )\n",
    "    \n",
    "    print(f\"Matched: {matched}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    if reason:\n",
    "        print(f\"Reason: {reason}\")\n",
    "        \n",
    "    return matched, score, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: EXACT Method\n",
    "Testing exact string matching with both match and non-match cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXACT method - Match\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account #12345\", \"Account #12345\")\n",
    "\n",
    "# EXACT method - No match\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account #12345\", \"Account #12346\")\n",
    "\n",
    "# EXACT method - Match with different casing and punctuation\n",
    "test_comparison(EvaluationMethod.EXACT, \"Account Number: 12345\", \"account number 12345\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: NUMERIC_EXACT Method\n",
    "Testing numeric comparison with different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERIC_EXACT method - Match\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"$1,250.00\", 1250)\n",
    "\n",
    "# NUMERIC_EXACT method - No match\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"$1,250.00\", 1251)\n",
    "\n",
    "# NUMERIC_EXACT method - Match with different formats\n",
    "test_comparison(EvaluationMethod.NUMERIC_EXACT, \"(1,250.00)\", \"-1250\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: FUZZY Method\n",
    "Testing fuzzy comparison with different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUZZY method - High match\n",
    "test_comparison(EvaluationMethod.FUZZY, \"John A. Smith\", \"John Smith\", threshold=0.8)\n",
    "\n",
    "# FUZZY method - Medium match \n",
    "matched, score, _ = test_comparison(EvaluationMethod.FUZZY, \"John A. Smith\", \"John Simpson\", threshold=0.8)\n",
    "print(f\"With threshold=0.6: {score >= 0.6}\")\n",
    "\n",
    "# FUZZY method - Low match\n",
    "test_comparison(EvaluationMethod.FUZZY, \"John Alexander Smith\", \"Jane Marie Johnson\", threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>### Test 4: HUNGARIAN Method\n",
    "Testing list comparison using the Hungarian algorithm.\n",
    "\n",
    "The Hungarian method is used for optimal matching between two lists when order doesn't matter. It automatically pairs items from each list for the maximum possible match score.\n",
    "\n",
    "**New Feature:** The Hungarian method now supports a `comparator_type` configuration which can be set to:\n",
    "- `EXACT`: Exact string matching (default)\n",
    "- `FUZZY`: Fuzzy string matching with similarity scoring\n",
    "- `NUMERIC`: Numeric comparison for handling different number formats\n",
    "\n",
    "This allows for greater flexibility when comparing lists of different types of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUNGARIAN method - Full match with EXACT comparator (default)\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Transfer: $200\", \"Withdrawal: $150\"]\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, expected_list, actual_list)\n",
    "\n",
    "# HUNGARIAN method - Test with explicit comparator_type\n",
    "print(\"\\nDemonstrating the new comparator_type functionality:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Test with EXACT comparator\n",
    "expected_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $200\"]\n",
    "actual_list = [\"Deposit: $500\", \"Withdrawal: $150\", \"Transfer: $210\"]  # Small difference in last value\n",
    "matched, score, reason = compare_values(\n",
    "    expected=expected_list,\n",
    "    actual=actual_list,\n",
    "    method=EvaluationMethod.HUNGARIAN,\n",
    "    comparator_type=\"EXACT\"  # Using the new comparator_type parameter\n",
    ")\n",
    "print(f\"HUNGARIAN with EXACT comparator:\")\n",
    "print(f\"  Expected: {expected_list}\")\n",
    "print(f\"  Actual: {actual_list}\")\n",
    "print(f\"  Result: Matched={matched}, Score={score:.2f}\")\n",
    "\n",
    "# Test with FUZZY comparator\n",
    "matched, score, reason = compare_values(\n",
    "    expected=expected_list,\n",
    "    actual=actual_list,\n",
    "    method=EvaluationMethod.HUNGARIAN,\n",
    "    comparator_type=\"FUZZY\",  # Using fuzzy comparator\n",
    "    threshold=0.7  # Explicit threshold for fuzzy matching\n",
    ")\n",
    "print(f\"\\nHUNGARIAN with FUZZY comparator (threshold 0.7):\")\n",
    "print(f\"  Expected: {expected_list}\")\n",
    "print(f\"  Actual: {actual_list}\")\n",
    "print(f\"  Result: Matched={matched}, Score={score:.2f}\")\n",
    "\n",
    "# Test with NUMERIC comparator (non-matching case)\n",
    "expected_num_list = [\"$500\", \"$150.00\", \"$200\"]\n",
    "actual_num_list = [\"500\", \"150\", \"210\"]  # Different number representation for the last value\n",
    "matched, score, reason = compare_values(\n",
    "    expected=expected_num_list,\n",
    "    actual=actual_num_list,\n",
    "    method=EvaluationMethod.HUNGARIAN,\n",
    "    comparator_type=\"NUMERIC\"  # Using numeric comparator\n",
    ")\n",
    "print(f\"\\nHUNGARIAN with NUMERIC comparator (non-matching case):\")\n",
    "print(f\"  Expected: {expected_num_list}\")\n",
    "print(f\"  Actual: {actual_num_list}\")\n",
    "print(f\"  Result: Matched={matched}, Score={score:.2f}\")\n",
    "print(f\"  Note: The values '$200' and '210' don't match numerically\")\n",
    "\n",
    "# Test with NUMERIC comparator (matching case)\n",
    "expected_num_list = [\"$500\", \"$150.00\", \"$200\"]\n",
    "actual_num_list = [\"500\", \"150\", \"200\"]  # Exact numeric matches after normalization\n",
    "matched, score, reason = compare_values(\n",
    "    expected=expected_num_list,\n",
    "    actual=actual_num_list,\n",
    "    method=EvaluationMethod.HUNGARIAN,\n",
    "    comparator_type=\"NUMERIC\"  # Using numeric comparator\n",
    ")\n",
    "print(f\"\\nHUNGARIAN with NUMERIC comparator (matching case):\")\n",
    "print(f\"  Expected: {expected_num_list}\")\n",
    "print(f\"  Actual: {actual_num_list}\")\n",
    "print(f\"  Result: Matched={matched}, Score={score:.2f}\")\n",
    "print(f\"  Note: All numeric values match after normalization\")\n",
    "\n",
    "# HUNGARIAN method - Non-list values (should convert to list)\n",
    "test_comparison(EvaluationMethod.HUNGARIAN, \"Single item\", \"Single item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: LLM Method\n",
    "Testing semantic comparison using a Large Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM method - High semantic match (different wording, same meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",\n",
    "    document_class=\"BankStatement\",\n",
    "    attr_name=\"statement_summary\",\n",
    "    attr_description=\"Summary of the bank statement\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM method - No semantic match (different meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits of $2,500 and withdrawals of $1,200, leaving a balance of $3,800.\",\n",
    "    document_class=\"BankStatement\",\n",
    "    attr_name=\"statement_summary\",\n",
    "    attr_description=\"Summary of the bank statement\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM method - Partial semantic match (some differences)\n",
    "test_comparison(\n",
    "    EvaluationMethod.LLM,\n",
    "    \"Policy effective date: January 15, 2023 to January 14, 2024\",\n",
    "    \"Policy period begins on Jan 15, 2023 and expires on Jan 15, 2024\",\n",
    "    document_class=\"InsurancePolicy\",\n",
    "    attr_name=\"policy_period\",\n",
    "    attr_description=\"The dates during which the insurance policy is effective\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: SEMANTIC Method\n",
    "Testing semantic comparison using embeddings with different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEMANTIC method - High similarity (different wording but same meaning)\n",
    "test_comparison(\n",
    "    EvaluationMethod.SEMANTIC,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",\n",
    "    threshold=0.8\n",
    ")\n",
    "\n",
    "# SEMANTIC method - Medium similarity (related content)\n",
    "matched, score, _ = test_comparison(\n",
    "    EvaluationMethod.SEMANTIC,\n",
    "    \"Policy effective date: January 15, 2023 to January 14, 2024\",\n",
    "    \"Coverage period: From Jan 15, 2023 through January 15, 2024\",\n",
    "    threshold=0.8\n",
    ")\n",
    "print(f\"With threshold=0.7: {score >= 0.7}\")\n",
    "\n",
    "# SEMANTIC method - Low similarity (different content)\n",
    "test_comparison(\n",
    "    EvaluationMethod.SEMANTIC,\n",
    "    \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "    \"Insurance policy with a premium of $850 per year and a deductible of $500.\",\n",
    "    threshold=0.8\n",
    ")\n",
    "\n",
    "# SEMANTIC method - Different threshold test\n",
    "print(\"\\nTesting different thresholds with medium similarity content:\")\n",
    "for threshold in [0.5, 0.7, 0.8, 0.9]:\n",
    "    matched, score, _ = test_comparison(\n",
    "        EvaluationMethod.SEMANTIC,\n",
    "        \"The patient was diagnosed with hypertension and prescribed lisinopril 10mg daily.\",\n",
    "        \"Patient has high blood pressure and was given medication to take once per day.\",\n",
    "        threshold=threshold\n",
    "    )\n",
    "    print(f\"Threshold {threshold}: Match={matched}, Score={score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing SEMANTIC vs LLM Methods\n",
    "\n",
    "Let's compare results from the SEMANTIC method (using embeddings) with the LLM method on various examples to understand their respective strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compare both methods\n",
    "def compare_semantic_vs_llm(expected: str, actual: str, description: str = \"Test comparison\"):\n",
    "    \"\"\"Compare SEMANTIC and LLM methods on the same input.\"\"\"\n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(f\"Comparison: {description}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Actual: {actual}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    # Run semantic comparison\n",
    "    semantic_matched, semantic_score, semantic_reason = test_comparison(\n",
    "        EvaluationMethod.SEMANTIC,\n",
    "        expected,\n",
    "        actual,\n",
    "        threshold=0.8\n",
    "    )\n",
    "    \n",
    "    # Run LLM comparison\n",
    "    llm_matched, llm_score, llm_reason = test_comparison(\n",
    "        EvaluationMethod.LLM,\n",
    "        expected,\n",
    "        actual,\n",
    "        document_class=\"TestDoc\",\n",
    "        attr_name=\"test_attr\",\n",
    "        attr_description=\"Test attribute\"\n",
    "    )\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\nComparison of results:\")\n",
    "    print(f\"{'Method':<10} {'Matched':<10} {'Score':<10} {'Reason'}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"{'SEMANTIC':<10} {semantic_matched!s:<10} {semantic_score:<10.4f} {semantic_reason or ''}\")\n",
    "    print(f\"{'LLM':<10} {llm_matched!s:<10} {llm_score:<10.4f} {llm_reason or ''}\")\n",
    "    \n",
    "    return {\n",
    "        \"semantic\": (semantic_matched, semantic_score, semantic_reason),\n",
    "        \"llm\": (llm_matched, llm_score, llm_reason)\n",
    "    }\n",
    "\n",
    "# Test cases where both methods should give similar results\n",
    "compare_semantic_vs_llm(\n",
    "    \"Patient diagnosed with pneumonia and prescribed antibiotics for 10 days.\",\n",
    "    \"The patient has pneumonia and was given a 10-day course of antibiotics.\",\n",
    "    \"Similar medical information with different wording\"\n",
    ")\n",
    "\n",
    "# Test cases where SEMANTIC might be better\n",
    "compare_semantic_vs_llm(\n",
    "    \"Total payment due: $1,543.27\",\n",
    "    \"Amount to be paid: $1,543.27\",\n",
    "    \"Financial information with different formats but exact amounts\"\n",
    ")\n",
    "\n",
    "# Test cases where LLM might be better\n",
    "compare_semantic_vs_llm(\n",
    "    \"Room temperature maintained at 72Â°F during the experiment.\",\n",
    "    \"The experiment was conducted in standard laboratory conditions at room temperature.\",\n",
    "    \"Implicit vs explicit information\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup test config\n",
    "test_config = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"name\": \"TestDocument\",\n",
    "            \"attributes\": [\n",
    "                {\n",
    "                    \"name\": \"exact_match_attr\",\n",
    "                    \"description\": \"Attribute for exact matching\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"numeric_attr\",\n",
    "                    \"description\": \"Attribute for numeric matching\",\n",
    "                    \"evaluation_method\": \"NUMERIC_EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"fuzzy_attr\",\n",
    "                    \"description\": \"Attribute for fuzzy matching\",\n",
    "                    \"evaluation_method\": \"FUZZY\",\n",
    "                    \"evaluation_threshold\": 0.8\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"list_attr\",\n",
    "                    \"description\": \"Attribute for list comparison\",\n",
    "                    \"evaluation_method\": \"HUNGARIAN\",\n",
    "                    \"hungarian_comparator\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"list_attr_fuzzy\",\n",
    "                    \"description\": \"Attribute for list comparison with fuzzy matching\",\n",
    "                    \"evaluation_method\": \"HUNGARIAN\",\n",
    "                    \"hungarian_comparator\": \"FUZZY\",\n",
    "                    \"evaluation_threshold\": 0.7  # Threshold for fuzzy matching within Hungarian\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"list_attr_numeric\",\n",
    "                    \"description\": \"Attribute for list comparison with numeric matching\",\n",
    "                    \"evaluation_method\": \"HUNGARIAN\",\n",
    "                    \"hungarian_comparator\": \"NUMERIC\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"llm_attr\",\n",
    "                    \"description\": \"Attribute for semantic comparison\",\n",
    "                    \"evaluation_method\": \"LLM\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_in_actual\",\n",
    "                    \"description\": \"Attribute missing in actual results\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_in_expected\",\n",
    "                    \"description\": \"Attribute missing in expected results\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"missing_everywhere\",\n",
    "                    \"description\": \"Attribute missing in both expected and actual\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"semantic_attr\",\n",
    "                    \"description\": \"Attribute for semantic embedding comparison\",\n",
    "                    \"evaluation_method\": \"SEMANTIC\",\n",
    "                    \"evaluation_threshold\": 0.8\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"evaluation\": {\n",
    "        \"llm_method\": {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator for document extraction attributes.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.  Here's the exact format:\n",
    "{\n",
    "  \"match\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"Your explanation here\"\n",
    "}\n",
    "\"\"\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update mock S3 retrieval function to include semantic_attr\n",
    "def mock_s3_get_json(uri: str) -> Dict[str, Any]:\n",
    "    \"\"\"Mock S3 file retrieval.\"\"\"\n",
    "    if \"expected\" in uri:\n",
    "        return {\n",
    "            \"exact_match_attr\": \"Exact Match Value\",\n",
    "            \"numeric_attr\": \"$1,250.00\",\n",
    "            \"fuzzy_attr\": \"John Alexander Smith\",\n",
    "            \"list_attr\": [\"Item 1\", \"Item 2\", \"Item 3\"],\n",
    "            \"list_attr_fuzzy\": [\"Payment method: Credit Card\", \"Due date: Jan 15, 2023\", \"Reference: ABC-123\"],\n",
    "            \"list_attr_numeric\": [\"$500.00\", \"$150.00\", \"$200.00\"],\n",
    "            \"llm_attr\": \"Monthly statement showing deposits of $1,250, withdrawals of $850, ending balance of $2,400.\",\n",
    "            \"semantic_attr\": \"Patient was diagnosed with hypertension and prescribed lisinopril 10mg daily.\",\n",
    "            \"missing_in_actual\": \"This value exists in expected only\",\n",
    "            # missing_in_expected is intentionally omitted\n",
    "            # missing_everywhere is intentionally omitted\n",
    "        }\n",
    "    else:  # actual results\n",
    "        return {\n",
    "            \"exact_match_attr\": \"Exact Match Value\",  # Exact match\n",
    "            \"numeric_attr\": 1250,  # Numeric match\n",
    "            \"fuzzy_attr\": \"John A Smith\",  # Fuzzy match\n",
    "            \"list_attr\": [\"Item 1\", \"Item 3\", \"Item 2\"],  # List with different order\n",
    "            \"list_attr_fuzzy\": [\"Reference: ABC123\", \"Payment: CC\", \"Due: January 15, 2023\"],  # Fuzzy matches\n",
    "            \"list_attr_numeric\": [500, 150, 200],  # Numeric comparison (now matches)\n",
    "            \"llm_attr\": \"Statement with deposits totaling $1,250 and withdrawals of $850, leaving a balance of $2,400.\",  # Semantic match\n",
    "            \"semantic_attr\": \"Patient has high blood pressure and was given medication to take once per day.\",  # Semantic embedding match\n",
    "            # missing_in_actual is intentionally omitted\n",
    "            \"missing_in_expected\": \"This value exists in actual only\",\n",
    "            # missing_everywhere is intentionally omitted\n",
    "        }\n",
    "\n",
    "# Set up mock storage - we'll still use this for S3\n",
    "class MockS3:\n",
    "    # Store report content for later display\n",
    "    report_content = \"\"\n",
    "    results_content = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_json_content(uri: str) -> Dict[str, Any]:\n",
    "        return mock_s3_get_json(uri)\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_content(content: Any, bucket: str, key: str, content_type: str = None):\n",
    "        print(f\"Writing content to s3://{bucket}/{key}\")\n",
    "        if key.endswith(\"results.json\"):\n",
    "            # Store the results for later access\n",
    "            MockS3.results_content = content\n",
    "            print(f\"Evaluation results summary: {json.dumps(content.get('overall_metrics', {}), indent=2)}\")\n",
    "        elif key.endswith(\"report.md\"):\n",
    "            # Store the markdown report for later display\n",
    "            MockS3.report_content = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock documents for evaluation\n",
    "def create_test_document(doc_id: str, is_expected: bool = False) -> Document:\n",
    "    \"\"\"Create a test document with a section.\"\"\"\n",
    "    section = Section(\n",
    "        section_id=\"sec-001\",\n",
    "        classification=\"TestDocument\",\n",
    "        extraction_result_uri=f\"s3://test-bucket/{doc_id}/{'expected' if is_expected else 'actual'}/extraction.json\"\n",
    "    )\n",
    "    \n",
    "    doc = Document(\n",
    "        id=doc_id,\n",
    "        sections=[section],\n",
    "        input_key=doc_id,\n",
    "        input_bucket=\"test-bucket\",\n",
    "        output_bucket=\"test-bucket\",\n",
    "    )\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Create test documents\n",
    "actual_doc = create_test_document(\"test-doc-001\")\n",
    "expected_doc = create_test_document(\"test-doc-001-baseline\", is_expected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate document\n",
    "# Only patch S3 module - use real Bedrock\n",
    "import idp_common.evaluation.service\n",
    "idp_common.evaluation.service.s3 = MockS3\n",
    "\n",
    "# Create evaluation service\n",
    "evaluation_service = EvaluationService(region=\"us-east-1\", config=test_config)\n",
    "\n",
    "# Evaluate document\n",
    "result_doc = evaluation_service.evaluate_document(actual_doc, expected_doc, store_results=True)\n",
    "\n",
    "# Print results\n",
    "if hasattr(result_doc, 'evaluation_result'):\n",
    "    eval_result = result_doc.evaluation_result\n",
    "    print(f\"\\nOverall metrics: {eval_result.overall_metrics}\")\n",
    "    \n",
    "    # Check section results\n",
    "    for section_result in eval_result.section_results:\n",
    "        print(f\"\\nSection {section_result.section_id} - Class: {section_result.document_class}\")\n",
    "        print(f\"Metrics: {section_result.metrics}\")\n",
    "        \n",
    "        # Print attribute details\n",
    "        print(\"\\nAttribute Details:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Name':<20} {'Method':<15} {'Expected':<25} {'Actual':<25} {'Matched':<10} {'Score':<10} {'Reason'}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for attr in section_result.attributes:\n",
    "            expected_val = str(attr.expected)[:25]\n",
    "            actual_val = str(attr.actual)[:25]\n",
    "            method = attr.evaluation_method\n",
    "            reason = attr.reason[:50] + \"...\" if attr.reason and len(attr.reason) > 50 else (attr.reason or \"\")\n",
    "            print(f\"{attr.name:<20} {method:<15} {expected_val:<25} {actual_val:<25} {attr.matched!s:<10} {attr.score:<10.2f} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Evaluation Report\n",
    "\n",
    "Let's display the markdown evaluation report that was generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Display the markdown report\n",
    "if MockS3.report_content:\n",
    "    display(Markdown(MockS3.report_content))\n",
    "else:\n",
    "    print(\"No evaluation report was generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Smart Attribute Discovery and Evaluation\n",
    "\n",
    "This section demonstrates the \"Smart attribute discovery and evaluation\" feature of the EvaluationService, which:\n",
    "1. Automatically discovers attributes in the data not defined in configuration\n",
    "2. Applies default evaluation methods to unconfigured attributes\n",
    "3. Properly handles attributes found only in expected data, only in actual data, or in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a minimal configuration with only some attributes defined\n",
    "minimal_config = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"name\": \"InvoiceDocument\",\n",
    "            \"attributes\": [\n",
    "                {\n",
    "                    \"name\": \"invoice_number\",\n",
    "                    \"description\": \"The unique identifier for the invoice\",\n",
    "                    \"evaluation_method\": \"EXACT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"amount_due\",\n",
    "                    \"description\": \"The total amount to be paid\",\n",
    "                    \"evaluation_method\": \"NUMERIC_EXACT\"\n",
    "                }\n",
    "                # Note: Other attributes are intentionally omitted from configuration\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"evaluation\": {\n",
    "        \"llm_method\": {\n",
    "            \"model\": \"us.amazon.nova-lite-v1:0\",\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_k\": 250,\n",
    "            \"system_prompt\": \"You are an evaluator for document extraction attributes.\",\n",
    "            \"task_prompt\": \"\"\"I need to evaluate attribute extraction for a document of class: {DOCUMENT_CLASS}.\n",
    "\n",
    "For the attribute named \"{ATTRIBUTE_NAME}\" described as \"{ATTRIBUTE_DESCRIPTION}\":\n",
    "- Expected value: {EXPECTED_VALUE}\n",
    "- Actual value: {ACTUAL_VALUE}\n",
    "\n",
    "Do these values match in meaning, taking into account formatting differences, word order, abbreviations, and semantic equivalence?\n",
    "Provide your assessment as a JSON with three fields:\n",
    "- \"match\": boolean (true if they match, false if not)\n",
    "- \"score\": number between 0 and 1 representing the confidence/similarity score\n",
    "- \"reason\": brief explanation of your decision\n",
    "\n",
    "Respond ONLY with the JSON and nothing else.  Here's the exact format:\n",
    "{\n",
    "  \"match\": true or false,\n",
    "  \"score\": 0.0 to 1.0,\n",
    "  \"reason\": \"Your explanation here\"\n",
    "}\n",
    "\"\"\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override S3 mock data function to simulate invoice data with unconfigured attributes\n",
    "def mock_invoice_s3_get_json(uri: str) -> Dict[str, Any]:\n",
    "    \"\"\"Mock S3 file retrieval with invoice data including unconfigured attributes.\"\"\"\n",
    "    if \"expected\" in uri:\n",
    "        return {\n",
    "            # Configured attributes\n",
    "            \"invoice_number\": \"INV-12345\",  # Configured with EXACT method\n",
    "            \"amount_due\": \"$1,250.00\",     # Configured with NUMERIC_EXACT method\n",
    "            \n",
    "            # Unconfigured attributes - only in expected\n",
    "            \"reference_number\": \"REF-98765\",\n",
    "            \n",
    "            # Unconfigured attributes - in both expected and actual\n",
    "            \"issue_date\": \"January 15, 2023\",\n",
    "            \"due_date\": \"February 15, 2023\",\n",
    "            \"vendor_name\": \"Acme Corporation Inc.\",\n",
    "            \"payment_terms\": \"Net 30\"\n",
    "        }\n",
    "    else:  # actual results\n",
    "        return {\n",
    "            # Configured attributes\n",
    "            \"invoice_number\": \"INV-12345\",  # Exact match with expected\n",
    "            \"amount_due\": 1250,            # Numeric match with expected\n",
    "            \n",
    "            # Unconfigured attributes - only in actual\n",
    "            \"purchase_order\": \"PO-54321\",\n",
    "            \n",
    "            # Unconfigured attributes - in both expected and actual with various match qualities\n",
    "            \"issue_date\": \"01/15/2023\",                     # Different format but same date\n",
    "            \"due_date\": \"02/15/2023\",                       # Different format but same date\n",
    "            \"vendor_name\": \"ACME Corp.\",                    # Abbreviated but similar\n",
    "            \"payment_terms\": \"Payment due within 30 days\"    # Different wording but same meaning\n",
    "        }\n",
    "\n",
    "# Create test invoice documents\n",
    "def create_invoice_document(doc_id: str, is_expected: bool = False) -> Document:\n",
    "    \"\"\"Create a test invoice document with a section.\"\"\"\n",
    "    section = Section(\n",
    "        section_id=\"inv-001\",\n",
    "        classification=\"InvoiceDocument\",\n",
    "        extraction_result_uri=f\"s3://test-bucket/{doc_id}/{'expected' if is_expected else 'actual'}/extraction.json\"\n",
    "    )\n",
    "    \n",
    "    doc = Document(\n",
    "        id=doc_id,\n",
    "        sections=[section],\n",
    "        input_key=doc_id,\n",
    "        input_bucket=\"test-bucket\",\n",
    "        output_bucket=\"test-bucket\",\n",
    "    )\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Set up a new mock for this example\n",
    "class MockInvoiceS3:\n",
    "    # Store report content for later display\n",
    "    report_content = \"\"\n",
    "    results_content = {}\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_json_content(uri: str) -> Dict[str, Any]:\n",
    "        return mock_invoice_s3_get_json(uri)\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_content(content: Any, bucket: str, key: str, content_type: str = None):\n",
    "        print(f\"Writing content to s3://{bucket}/{key}\")\n",
    "        if key.endswith(\"results.json\"):\n",
    "            # Store the results for later access\n",
    "            MockInvoiceS3.results_content = content\n",
    "        elif key.endswith(\"report.md\"):\n",
    "            # Store the markdown report for later display\n",
    "            MockInvoiceS3.report_content = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate document with smart attribute discovery\n",
    "# Patch S3 module with our invoice mock\n",
    "idp_common.evaluation.service.s3 = MockInvoiceS3\n",
    "\n",
    "# Create test documents\n",
    "actual_invoice = create_invoice_document(\"invoice-001\")\n",
    "expected_invoice = create_invoice_document(\"invoice-001-baseline\", is_expected=True)\n",
    "\n",
    "# Create evaluation service with minimal config\n",
    "evaluation_service = EvaluationService(region=\"us-east-1\", config=minimal_config)\n",
    "\n",
    "# Evaluate document\n",
    "print(\"Evaluating invoice document with smart attribute discovery...\")\n",
    "result_doc = evaluation_service.evaluate_document(actual_invoice, expected_invoice, store_results=True)\n",
    "\n",
    "# Print results with focus on discovered attributes\n",
    "if hasattr(result_doc, 'evaluation_result'):\n",
    "    eval_result = result_doc.evaluation_result\n",
    "    print(f\"\\nOverall metrics: {eval_result.overall_metrics}\")\n",
    "    \n",
    "    # Check section results\n",
    "    for section_result in eval_result.section_results:\n",
    "        print(f\"\\nSection {section_result.section_id} - Class: {section_result.document_class}\")\n",
    "        \n",
    "        # Find configured vs unconfigured attributes\n",
    "        configured_attrs = []\n",
    "        unconfigured_attrs = []\n",
    "        \n",
    "        for attr in section_result.attributes:\n",
    "            # Check if the attribute has a message about being unconfigured\n",
    "            if attr.reason and \"attribute not in the configuration\" in attr.reason:\n",
    "                unconfigured_attrs.append(attr)\n",
    "            else:\n",
    "                configured_attrs.append(attr)\n",
    "        \n",
    "        # Print summary of attribute counts\n",
    "        print(f\"Total attributes evaluated: {len(section_result.attributes)}\")\n",
    "        print(f\"  - Configured attributes: {len(configured_attrs)}\")\n",
    "        print(f\"  - Auto-discovered attributes: {len(unconfigured_attrs)}\")\n",
    "        \n",
    "        # Print attribute details - CONFIGURED\n",
    "        if configured_attrs:\n",
    "            print(\"\\nCONFIGURED Attribute Details:\")\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"{'Name':<20} {'Method':<15} {'Expected':<25} {'Actual':<25} {'Matched':<10} {'Score':<10}\")\n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            for attr in configured_attrs:\n",
    "                expected_val = str(attr.expected)[:25]\n",
    "                actual_val = str(attr.actual)[:25]\n",
    "                method = attr.evaluation_method\n",
    "                print(f\"{attr.name:<20} {method:<15} {expected_val:<25} {actual_val:<25} {attr.matched!s:<10} {attr.score:<10.2f}\")\n",
    "        \n",
    "        # Print attribute details - UNCONFIGURED\n",
    "        if unconfigured_attrs:\n",
    "            print(\"\\nAUTO-DISCOVERED Attribute Details:\")\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"{'Name':<20} {'Method':<15} {'Expected':<25} {'Actual':<25} {'Matched':<10} {'Score':<10} {'Reason'}\")\n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            for attr in unconfigured_attrs:\n",
    "                expected_val = str(attr.expected)[:25]\n",
    "                actual_val = str(attr.actual)[:25]\n",
    "                method = attr.evaluation_method\n",
    "                reason = attr.reason[:50] + \"...\" if attr.reason and len(attr.reason) > 50 else (attr.reason or \"\")\n",
    "                print(f\"{attr.name:<20} {method:<15} {expected_val:<25} {actual_val:<25} {attr.matched!s:<10} {attr.score:<10.2f} {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Smart Attribute Discovery Evaluation Report\n",
    "\n",
    "Let's display the markdown evaluation report that was generated for the smart attribute discovery scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the markdown report\n",
    "if MockInvoiceS3.report_content:\n",
    "    display(Markdown(MockInvoiceS3.report_content))\n",
    "else:\n",
    "    print(\"No evaluation report was generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Attribute Discovery Scenario Summary\n",
    "\n",
    "The smart attribute discovery and evaluation feature provides the following benefits:\n",
    "\n",
    "1. **Auto-discovery of attributes**\n",
    "   - Finds attributes not explicitly defined in the configuration\n",
    "   - Compares all data fields across expected and actual results\n",
    "   - Works with minimal or even no attribute configuration\n",
    "\n",
    "2. **Default Evaluation Method**\n",
    "   - Applies LLM method to unconfigured attributes by default \n",
    "   - Provides semantic comparison for discovered attributes\n",
    "   - Attaches explanations that the attribute was not in configuration\n",
    "\n",
    "3. **Handles All Possible Cases**\n",
    "   - Attributes in both expected and actual results\n",
    "   - Attributes only in expected results (false negatives)\n",
    "   - Attributes only in actual results (false positives)\n",
    "   - Attributes that don't exist in either (true negatives)\n",
    "\n",
    "4. **Benefits**\n",
    "   - Exploratory evaluation without complete configuration\n",
    "   - Comprehensive metrics that include all found attributes\n",
    "   - Flexibility as extraction models evolve or change output formats\n",
    "   - Identification of potential new attributes to add to configuration\n",
    "\n",
    "This feature is particularly useful during the early stages of implementation when the complete attribute schema may not be fully defined, or when handling variations in extraction outputs that contain unexpected information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## Summary of All Demonstrated Features\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. All evaluation methods available in the IDP library:\n",
    "   - EXACT - Exact string matching\n",
    "   - NUMERIC_EXACT - Numeric value matching\n",
    "   - FUZZY - Fuzzy string matching with adjustable thresholds\n",
    "   - SEMANTIC - Semantic similarity comparison using Titan embeddings\n",
    "   - HUNGARIAN - List comparison using the Hungarian algorithm with configurable comparator types:\n",
    "     - EXACT comparator - Exact string matching for list items\n",
    "     - FUZZY comparator - Fuzzy string matching for list items\n",
    "     - NUMERIC comparator - Numeric comparison for list items\n",
    "   - LLM - Semantic comparison using Large Language Models\n",
    "\n",
    "2. Semantic Comparison Methods:\n",
    "   - SEMANTIC - Uses Bedrock Titan embeddings and cosine similarity for efficient matching\n",
    "   - LLM - Uses Bedrock Claude for more nuanced semantic understanding with reasoning\n",
    "\n",
    "3. Benefits of SEMANTIC vs LLM methods:\n",
    "   - SEMANTIC is faster and less expensive than LLM-based evaluation\n",
    "   - LLM provides explanations for matches/mismatches\n",
    "   - SEMANTIC works well for standard text comparisons\n",
    "   - LLM better understands implicit information and complex reasoning\n",
    "\n",
    "4. Handling of edge cases:\n",
    "   - Attributes missing in actual results\n",
    "   - Attributes missing in expected results\n",
    "   - Attributes missing in both actual and expected results\n",
    "   - Empty string values\n",
    "\n",
    "5. Full document evaluation with mixed evaluation methods\n",
    "   - Comprehensive metrics calculation\n",
    "   - Detailed attribute-level results\n",
    "\n",
    "6. Threshold sensitivity analysis for fuzzy and semantic matching\n",
    "   - How different threshold values affect match results\n",
    "   - Trade-offs between precision and recall\n",
    "\n",
    "7. Smart attribute discovery and evaluation:\n",
    "   - Auto-discovery of attributes not in configuration\n",
    "   - Default semantic evaluation with LLM method\n",
    "   - Comprehensive handling of all attribute cases\n",
    "   - Support for exploratory evaluation and evolving schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "1. All evaluation methods available in the IDP library:\n",
    "   - EXACT - Exact string matching\n",
    "   - NUMERIC_EXACT - Numeric value matching\n",
    "   - FUZZY - Fuzzy string matching with adjustable thresholds\n",
    "   - SEMANTIC - Semantic similarity comparison using Titan embeddings\n",
    "   - HUNGARIAN - List comparison using the Hungarian algorithm\n",
    "   - LLM - Semantic comparison using Large Language Models\n",
    "\n",
    "2. Semantic Comparison Methods:\n",
    "   - SEMANTIC - Uses Bedrock Titan embeddings and cosine similarity for efficient matching\n",
    "   - LLM - Uses Bedrock Claude for more nuanced semantic understanding with reasoning\n",
    "\n",
    "3. Benefits of SEMANTIC vs LLM methods:\n",
    "   - SEMANTIC is faster and less expensive than LLM-based evaluation\n",
    "   - LLM provides explanations for matches/mismatches\n",
    "   - SEMANTIC works well for standard text comparisons\n",
    "   - LLM better understands implicit information and complex reasoning\n",
    "\n",
    "4. Handling of edge cases:\n",
    "   - Attributes missing in actual results\n",
    "   - Attributes missing in expected results\n",
    "   - Attributes missing in both actual and expected results\n",
    "   - Empty string values\n",
    "\n",
    "5. Full document evaluation with mixed evaluation methods\n",
    "   - Comprehensive metrics calculation\n",
    "   - Detailed attribute-level results\n",
    "\n",
    "6. Threshold sensitivity analysis for fuzzy and semantic matching\n",
    "   - How different threshold values affect match results\n",
    "   - Trade-offs between precision and recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
