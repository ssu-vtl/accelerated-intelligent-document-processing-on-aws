{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification with Bedrock Models\n",
    "\n",
    "This notebook evaluates two document classification models using the RVL-CDIP dataset:\n",
    "1. Amazon Nova Lite (us.amazon.nova-lite-v1:0)\n",
    "2. Provisioned Bedrock model (arn:aws:bedrock:us-east-1:195275636621:provisioned-model/qsr1bg9tbf1v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import base64\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import io\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS credentials and region\n",
    "region = \"us-east-1\"\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=region)\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# Define models with their IDs and providers\n",
    "MODELS = {\n",
    "    \"nova_lite\": {\n",
    "        \"id\": \"us.amazon.nova-lite-v1:0\",\n",
    "        \"provider\": \"amazon\"\n",
    "    },\n",
    "    \"nova_pro\": {\n",
    "        \"id\": \"us.amazon.nova-pro-v1:0\",\n",
    "        \"provider\": \"amazon\"\n",
    "    },\n",
    "    \"nova_premier\": {\n",
    "        \"id\": \"us.amazon.nova-premier-v1:0\",\n",
    "        \"provider\": \"amazon\"\n",
    "    },\n",
    "    \"ft_nova_lite\": {\n",
    "        \"id\": \"arn:aws:bedrock:us-east-1:195275636621:provisioned-model/qsr1bg9tbf1v\",\n",
    "        \"provider\": \"amazon\"\n",
    "    },\n",
    "    \"ft_nova_lite_lr_e4_wr_10\": {\n",
    "        \"id\": \"arn:aws:bedrock:us-east-1:195275636621:provisioned-model/6q5nfhuh1pnq\",\n",
    "        \"provider\": \"amazon\"\n",
    "    },\n",
    "    \"claude_3_5_haiku\": {\n",
    "        \"id\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "        \"provider\": \"anthropic\"\n",
    "    },\n",
    "    \"claude_3_5_sonnet_v2\": {\n",
    "        \"id\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        \"provider\": \"anthropic\"\n",
    "    },\n",
    "    \"claude_3_7\": {\n",
    "        \"id\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        \"provider\": \"anthropic\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mapping for RVL-CDIP dataset\n",
    "label_mapping = {\n",
    "    0: \"advertissement\",\n",
    "    1: \"budget\",\n",
    "    2: \"email\",\n",
    "    3: \"file_folder\",\n",
    "    4: \"form\",\n",
    "    5: \"handwritten\",\n",
    "    6: \"invoice\",\n",
    "    7: \"letter\",\n",
    "    8: \"memo\",\n",
    "    9: \"news_article\",\n",
    "    10: \"presentation\",\n",
    "    11: \"questionnaire\",\n",
    "    12: \"resume\",\n",
    "    13: \"scientific_publication\",\n",
    "    14: \"scientific_report\",\n",
    "    15: \"specification\"\n",
    "}\n",
    "\n",
    "# Create reverse mapping for evaluation\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RVL-CDIP test dataset\n",
    "ds = load_dataset(\"chainyo/rvl-cdip\", split=\"test\")\n",
    "print(f\"Dataset loaded: {ds}\")\n",
    "\n",
    "# Get the unique labels\n",
    "unique_labels = np.unique(ds[\"label\"])\n",
    "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
    "print(f\"Labels: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100 random images per class\n",
    "samples_per_label = 100\n",
    "sampled_data = []\n",
    "\n",
    "# Get all indices first\n",
    "all_indices = list(range(len(ds)))\n",
    "random.shuffle(all_indices)  # Shuffle for randomness\n",
    "\n",
    "# Track how many samples we've collected per label\n",
    "samples_count = {label: 0 for label in unique_labels}\n",
    "errors_count = 0\n",
    "\n",
    "# Process indices until we have enough samples for each label\n",
    "for idx in tqdm(all_indices):\n",
    "    # Stop if we have enough samples for all labels\n",
    "    if all(count >= samples_per_label for count in samples_count.values()):\n",
    "        break\n",
    "        \n",
    "    try:\n",
    "        # Try to access this sample\n",
    "        sample = ds[idx]\n",
    "        label = sample[\"label\"]\n",
    "        \n",
    "        # If we still need samples for this label, add it\n",
    "        if samples_count[label] < samples_per_label:\n",
    "            sampled_data.append(sample)\n",
    "            samples_count[label] += 1\n",
    "            # print(f\"Added sample for label {label} ({label_mapping[label]}), total: {samples_count[label]}\")\n",
    "    except Exception as e:\n",
    "        errors_count += 1\n",
    "        if errors_count % 100 == 0:  # Print every 100 errors to avoid flooding output\n",
    "            print(f\"Error processing sample at index {idx}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Print summary\n",
    "for label in unique_labels:\n",
    "    print(f\"Label {label} ({label_mapping[label]}): Sampled {samples_count[label]} samples\")\n",
    "print(f\"Total sampled data: {len(sampled_data)}\")\n",
    "print(f\"Total errors encountered: {errors_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save Sampled Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique bucket name\n",
    "bucket_name = \"idp-evaluation-data-us-east-1\"\n",
    "directory = \"rvl-cdip-test-data\"\n",
    "os.makedirs(\"temp_images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save image to S3\n",
    "def save_to_s3(sample, index):\n",
    "    image = sample[\"image\"]\n",
    "    label = sample[\"label\"]\n",
    "    \n",
    "    # Generate filename\n",
    "    label_name = label_mapping[label]\n",
    "    filename = f\"{label_name}_{index}.png\"\n",
    "    \n",
    "    # Save locally first\n",
    "    local_path = os.path.join(\"temp_images\", filename)\n",
    "    image.save(local_path, format=\"PNG\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_path = f\"{directory}/{label_name}/{filename}\"\n",
    "    s3.upload_file(local_path, bucket_name, s3_path)\n",
    "    \n",
    "    # Remove local file\n",
    "    os.remove(local_path)\n",
    "    \n",
    "    return f\"s3://{bucket_name}/{s3_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a small test batch\n",
    "# test_batch = sampled_data\n",
    "# for i, sample in enumerate(test_batch):\n",
    "#     s3_uri = save_to_s3(sample, i)\n",
    "#     print(f\"Uploaded sample {i} to {s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt and task prompt for document classification\n",
    "system_prompt = \"You are a document classification expert who can analyze and identify document types from images.\"\n",
    "\n",
    "task_prompt = \"\"\"Classify this document into one of these types:\n",
    "- advertissement\n",
    "- budget\n",
    "- email\n",
    "- file_folder\n",
    "- form\n",
    "- handwritten\n",
    "- invoice\n",
    "- letter\n",
    "- memo\n",
    "- news_article\n",
    "- presentation\n",
    "- questionnaire\n",
    "- resume\n",
    "- scientific_publication\n",
    "- scientific_report\n",
    "- specification\n",
    "\n",
    "Return your response as JSON: {\"type\": \"document_type_name\"}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(image, model_id, model_provider):\n",
    "    # Convert image to base64\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    image_bytes = buffered.getvalue()\n",
    "    base64_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    \n",
    "    # Invoke model with retry logic\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if model_provider == \"amazon\":\n",
    "                # Nova models use the converse API with system parameter\n",
    "                system = [{\"text\": system_prompt}]\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"image\": {\"format\": \"jpeg\", \"source\": {\"bytes\": image_bytes}}},\n",
    "                            {\"text\": task_prompt}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "                # Configure inference parameters\n",
    "                inf_params = {\"maxTokens\": 1000, \"topP\": 0.1, \"temperature\": 0.0}\n",
    "                \n",
    "                response = bedrock_runtime.converse(\n",
    "                    modelId=model_id,\n",
    "                    messages=messages,\n",
    "                    system=system,\n",
    "                    inferenceConfig=inf_params\n",
    "                )\n",
    "                return response\n",
    "            else:\n",
    "                # Claude models use the converse API with a different format\n",
    "                # Following the working example format\n",
    "                message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"text\": task_prompt},\n",
    "                        {\n",
    "                            \"image\": {\n",
    "                                \"format\": \"jpeg\",\n",
    "                                \"source\": {\n",
    "                                    \"bytes\": image_bytes\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                messages = [message]\n",
    "                \n",
    "                # Send the message using converse API for Claude models\n",
    "                response = bedrock_runtime.converse(\n",
    "                    modelId=model_id,\n",
    "                    messages=messages\n",
    "                )\n",
    "                return response\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                # print(f\"Retry {attempt + 1}/{max_retries}: {str(e)}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response, model_provider):\n",
    "    if \"error\" in response:\n",
    "        return \"error\", response[\"error\"]\n",
    "    \n",
    "    try:\n",
    "        content = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    except (KeyError, IndexError):\n",
    "        return \"error\", \"Failed to extract content from Nova response\"\n",
    "    \n",
    "    # Try to parse as JSON\n",
    "    try:\n",
    "        import re\n",
    "        json_match = re.search(r'\\{[^\\{\\}]*\"type\"\\s*:\\s*\"[^\"]+\"[^\\{\\}]*\\}', content)\n",
    "        if json_match:\n",
    "            json_content = json.loads(json_match.group(0))\n",
    "            if \"type\" in json_content:\n",
    "                return \"success\", json_content[\"type\"].lower().strip()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # If JSON parsing fails, try to extract the document type using regex\n",
    "    match = re.search(r'\"type\"\\s*:\\s*\"([^\"]+)\"', content)\n",
    "    if match:\n",
    "        return \"success\", match.group(1).lower().strip()\n",
    "    \n",
    "    return \"unknown\", content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single sample\n",
    "# if len(sampled_data) > 0:\n",
    "#     test_sample = sampled_data[0]\n",
    "#     print(f\"Testing with sample of label: {label_mapping[test_sample['label']]}\")\n",
    "    \n",
    "#     # Test Nova Lite model\n",
    "#     model = MODELS[\"claude_3_7\"]\n",
    "#     response = invoke_model(test_sample[\"image\"], model[\"id\"], model[\"provider\"])\n",
    "#     status, prediction = parse_response(response, model[\"provider\"])\n",
    "#     print(f\"Prediction: {prediction} (status: {status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import concurrent.futures for parallel processing\n",
    "import concurrent.futures\n",
    "\n",
    "# Function to evaluate a single sample\n",
    "def evaluate_single_sample(args):\n",
    "    i, sample, model_info = args\n",
    "    true_label = sample[\"label\"]\n",
    "    true_label_name = label_mapping[true_label]\n",
    "    \n",
    "    # Invoke model\n",
    "    response = invoke_model(sample[\"image\"], model_info[\"id\"], model_info[\"provider\"])\n",
    "    status, prediction = parse_response(response, model_info[\"provider\"])\n",
    "    \n",
    "    # Return result with full response included\n",
    "    return {\n",
    "        \"sample_idx\": i,\n",
    "        \"true_label\": true_label,\n",
    "        \"true_label_name\": true_label_name,\n",
    "        \"prediction\": prediction,\n",
    "        \"status\": status,\n",
    "        \"correct\": prediction == true_label_name,\n",
    "        \"full_response\": response  # Add the full response\n",
    "    }\n",
    "\n",
    "# Function to evaluate a model on a subset of samples with parallel processing\n",
    "def evaluate_model(model_info, samples, model_name, max_workers=4, start_idx=0):\n",
    "    \"\"\"\n",
    "    Evaluate a model on samples with support for resuming from a specific index\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Prepare arguments for processing\n",
    "    args_list = [(i, sample, model_info) for i, sample in enumerate(samples) if i >= start_idx]\n",
    "    \n",
    "    if not args_list:\n",
    "        print(f\"No samples to process for {model_name}\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"Processing {len(args_list)} samples for {model_name}\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks and track with tqdm for progress\n",
    "        for future in tqdm(\n",
    "            concurrent.futures.as_completed([executor.submit(evaluate_single_sample, args) for args in args_list]),\n",
    "            total=len(args_list),\n",
    "            desc=f\"Evaluating {model_name}\"\n",
    "        ):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                # Save incremental results after every 10 samples\n",
    "                if len(results) % 10 == 0:\n",
    "                    with open(f\"{model_name}_partial_results.json\", \"w\") as f:\n",
    "                        json.dump(results, f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample: {str(e)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model_name, results, metrics=None):\n",
    "    \"\"\"Save checkpoint for a model evaluation\"\"\"\n",
    "    checkpoint = {\n",
    "        \"model_name\": model_name,\n",
    "        \"results\": results,\n",
    "        \"metrics\": metrics,\n",
    "        \"timestamp\": time.time()\n",
    "    }\n",
    "    \n",
    "    # Save checkpoint to file\n",
    "    with open(f\"{model_name}_checkpoint.json\", \"w\") as f:\n",
    "        json.dump(checkpoint, f)\n",
    "    \n",
    "    print(f\"Checkpoint saved for {model_name}\")\n",
    "\n",
    "def load_checkpoints():\n",
    "    \"\"\"Load all available checkpoints\"\"\"\n",
    "    checkpoints = {}\n",
    "    completed_models = []\n",
    "    \n",
    "    # Check for checkpoint files\n",
    "    for model_name in MODELS.keys():\n",
    "        checkpoint_file = f\"{model_name}_checkpoint.json\"\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            try:\n",
    "                with open(checkpoint_file, \"r\") as f:\n",
    "                    checkpoint = json.load(f)\n",
    "                checkpoints[model_name] = checkpoint\n",
    "                \n",
    "                # If metrics exist, consider this model evaluation complete\n",
    "                if checkpoint.get(\"metrics\"):\n",
    "                    completed_models.append(model_name)\n",
    "                    \n",
    "                print(f\"Found checkpoint for {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint for {model_name}: {str(e)}\")\n",
    "    \n",
    "    return checkpoints, completed_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics\n",
    "def calculate_metrics(results):\n",
    "    # Extract true labels and predictions\n",
    "    y_true = [result[\"true_label_name\"] for result in results]\n",
    "    y_pred = [result[\"prediction\"] for result in results]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    labels = list(set(y_true))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"recall\": recall,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on a small subset\n",
    "test_subset = sampled_data  # Start with just 10 samples\n",
    "\n",
    "# Dictionary to store results and metrics\n",
    "all_results = {}\n",
    "all_metrics = {}\n",
    "\n",
    "# Load existing checkpoints\n",
    "checkpoints, completed_models = load_checkpoints()\n",
    "\n",
    "# Restore results and metrics from checkpoints\n",
    "for model_name, checkpoint in checkpoints.items():\n",
    "    if \"results\" in checkpoint:\n",
    "        all_results[model_name] = checkpoint[\"results\"]\n",
    "    if \"metrics\" in checkpoint:\n",
    "        all_metrics[model_name] = checkpoint[\"metrics\"]\n",
    "\n",
    "# Evaluate models that haven't been completed\n",
    "for model_name, model_info in MODELS.items():\n",
    "    # Skip if model evaluation is already complete\n",
    "    if model_name in completed_models:\n",
    "        print(f\"Skipping {model_name} (already completed)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nEvaluating {model_name} model...\")\n",
    "    \n",
    "    # Check if we have partial results\n",
    "    if model_name in all_results:\n",
    "        print(f\"Resuming evaluation for {model_name} from checkpoint\")\n",
    "        results = all_results[model_name]\n",
    "    else:\n",
    "        # Start fresh evaluation\n",
    "        results = evaluate_model(model_info, test_subset, model_name)\n",
    "    \n",
    "    # Save results\n",
    "    all_results[model_name] = results\n",
    "    with open(f\"{model_name}_results.json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    \n",
    "    # Save checkpoint with results\n",
    "    save_checkpoint(model_name, results)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(results)\n",
    "    all_metrics[model_name] = metrics\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(f\"{model_name}_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "    \n",
    "    # Update checkpoint with metrics\n",
    "    save_checkpoint(model_name, results, metrics)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"{model_name.capitalize()} Model Metrics:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, labels, title, fixed_labels=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with consistent label ordering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cm : numpy.ndarray\n",
    "        Confusion matrix to plot\n",
    "    labels : list\n",
    "        Labels used to create the confusion matrix\n",
    "    title : str\n",
    "        Title for the plot\n",
    "    fixed_labels : list, optional\n",
    "        Fixed order of labels to use. If None, will use the provided labels.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # If fixed_labels is provided, reorder the confusion matrix\n",
    "    if fixed_labels is not None:\n",
    "        # Create a mapping from current labels to indices\n",
    "        label_to_idx = {label: i for i, label in enumerate(labels)}\n",
    "        \n",
    "        # Create a new confusion matrix with the fixed order\n",
    "        n = len(fixed_labels)\n",
    "        reordered_cm = np.zeros((n, n), dtype=int)\n",
    "        \n",
    "        # Fill in the reordered confusion matrix\n",
    "        for i, true_label in enumerate(fixed_labels):\n",
    "            if true_label in label_to_idx:\n",
    "                for j, pred_label in enumerate(fixed_labels):\n",
    "                    if pred_label in label_to_idx:\n",
    "                        reordered_cm[i, j] = cm[label_to_idx[true_label], label_to_idx[pred_label]]\n",
    "        \n",
    "        # Use the reordered matrix and fixed labels\n",
    "        cm = reordered_cm\n",
    "        labels = fixed_labels\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics[\"ft_nova_lite\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_label_order = list(label_mapping.values())\n",
    "\n",
    "# Plot Nova Lite confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    np.array(all_metrics[\"ft_nova_lite\"][\"confusion_matrix\"]),\n",
    "    all_metrics[\"ft_nova_lite\"][\"labels\"],\n",
    "    \"Confusion Matrix: Finetune Nova Lite Model on RVL-CDIP Dataset\",\n",
    "    fixed_labels=fixed_label_order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    np.array(all_metrics[\"nova_lite\"][\"confusion_matrix\"]),\n",
    "    all_metrics[\"nova_lite\"][\"labels\"],\n",
    "    \"Confusion Matrix: Nova Lite Model on RVL-CDIP Dataset\",\n",
    "    fixed_labels=fixed_label_order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    np.array(all_metrics[\"nova_pro\"][\"confusion_matrix\"]),\n",
    "    all_metrics[\"nova_pro\"][\"labels\"],\n",
    "    \"Confusion Matrix: Nova Pro Model on RVL-CDIP Dataset\",\n",
    "    fixed_labels=fixed_label_order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    np.array(all_metrics[\"nova_premier\"][\"confusion_matrix\"]),\n",
    "    all_metrics[\"nova_premier\"][\"labels\"],\n",
    "    \"Confusion Matrix: Nova Premier Model on RVL-CDIP Dataset\",\n",
    "    fixed_labels=fixed_label_order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(all_metrics, palette_name='colorblind'):\n",
    "    \"\"\"\n",
    "    Plot model performance comparison with publication-quality color palette.\n",
    "    Displays metrics as percentages with 2 decimal places (XX.XX%).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_metrics : dict\n",
    "        Dictionary containing metrics for each model\n",
    "    palette_name : str\n",
    "        Name of the color palette to use:\n",
    "        - 'colorblind': Seaborn's colorblind palette (default)\n",
    "        - 'tableau': Tableau's color palette\n",
    "        - 'deep': Seaborn's deep palette\n",
    "        - 'muted': Seaborn's muted palette\n",
    "        - 'pastel': Seaborn's pastel palette\n",
    "        - 'bright': Seaborn's bright palette\n",
    "        - 'viridis', 'plasma', 'inferno', 'magma': Matplotlib's perceptually uniform colormaps\n",
    "    \"\"\"\n",
    "    # Set up metrics and model names\n",
    "    metrics = ['Accuracy', 'F1 Score', 'Recall']\n",
    "    model_names = list(all_metrics.keys())\n",
    "    num_models = len(model_names)\n",
    "    num_metrics = len(metrics)\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(max(12, num_models * 2), 8))\n",
    "    \n",
    "    # Choose color palette\n",
    "    if palette_name == 'colorblind':\n",
    "        colors = sns.color_palette(\"colorblind\", n_colors=num_models)\n",
    "    elif palette_name == 'tableau':\n",
    "        colors = sns.color_palette(\"tab10\", n_colors=num_models)\n",
    "    elif palette_name in ['deep', 'muted', 'pastel', 'bright']:\n",
    "        colors = sns.color_palette(palette_name, n_colors=num_models)\n",
    "    elif palette_name in ['viridis', 'plasma', 'inferno', 'magma']:\n",
    "        cmap = plt.cm.get_cmap(palette_name)\n",
    "        colors = [cmap(i/num_models) for i in range(num_models)]\n",
    "    else:\n",
    "        colors = sns.color_palette(\"colorblind\", n_colors=num_models)  # Default to colorblind\n",
    "    \n",
    "    # Set width of bars based on number of models\n",
    "    total_width = 0.8\n",
    "    width = total_width / num_models\n",
    "    x = np.arange(num_metrics)\n",
    "    \n",
    "    # Plot bars for each model\n",
    "    for i, (model_name, metrics_data) in enumerate(all_metrics.items()):\n",
    "        # Convert values to percentages (0-100 scale)\n",
    "        values = [\n",
    "            metrics_data['accuracy'] * 100, \n",
    "            metrics_data['f1'] * 100, \n",
    "            metrics_data['recall'] * 100\n",
    "        ]\n",
    "        offset = (i - num_models / 2 + 0.5) * width\n",
    "        rects = ax.bar(x + offset, values, width, label=model_name, color=colors[i], \n",
    "                      edgecolor='black', linewidth=0.5)  # Add black edge for better definition\n",
    "        \n",
    "        # Format labels with exactly 2 decimal places (XX.XX%)\n",
    "        percentage_labels = [f\"{v:.2f}%\" for v in values]\n",
    "        ax.bar_label(rects, padding=3, labels=percentage_labels, fontsize=8)\n",
    "    \n",
    "    # Add labels and legend\n",
    "    ax.set_ylabel('Score (%)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics, fontsize=11)\n",
    "    \n",
    "    # Set y-axis to 0-100 range with some extra space for labels\n",
    "    ax.set_ylim(0, 115)\n",
    "    \n",
    "    # Set y-axis ticks to show percentages\n",
    "    ax.set_yticks([0, 20, 40, 60, 80, 100])\n",
    "    ax.set_yticklabels(['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)  # Remove top spine for cleaner look\n",
    "    ax.spines['right'].set_visible(False)  # Remove right spine for cleaner look\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), \n",
    "             ncol=min(5, num_models), frameon=True, fontsize=10)\n",
    "    \n",
    "    # Add grid lines for better readability\n",
    "    ax.yaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics['Nova Lite'] = all_metrics.pop('nova_lite')\n",
    "all_metrics['Nova Pro'] = all_metrics.pop('nova_pro')\n",
    "all_metrics['Nova Premier'] = all_metrics.pop('nova_premier')\n",
    "all_metrics['Nova Lite Finetuned'] = all_metrics.pop('ft_nova_lite')\n",
    "plot_model_comparison(all_metrics, palette_name='pastel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Download files from S3 and convert to dataframes\n",
    "def load_csv_from_s3(s3_uri):\n",
    "    s3_uri = s3_uri.replace('s3://', '')\n",
    "    bucket_name = s3_uri.split('/')[0]\n",
    "    key = '/'.join(s3_uri.split('/')[1:])\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "    content = response['Body'].read()\n",
    "    \n",
    "    return pd.read_csv(io.BytesIO(content))\n",
    "\n",
    "# Extended function to plot losses vs epoch for multiple files\n",
    "def plot_losses_vs_epoch(file_uris, legends=None):\n",
    "    \"\"\"\n",
    "    Plot losses vs epoch for multiple files.\n",
    "    \n",
    "    Args:\n",
    "        file_uris (list): List of S3 URIs for the CSV files\n",
    "        legends (list, optional): List of legend labels. If None, will use default labels.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if legends is None:\n",
    "        legends = [f\"Data {i+1}\" for i in range(len(file_uris))]\n",
    "    \n",
    "    # Ensure we have the same number of legends as files\n",
    "    if len(legends) != len(file_uris):\n",
    "        raise ValueError(\"Number of legends must match number of file URIs\")\n",
    "    \n",
    "    colors = ['b', 'r', 'g', 'c', 'm', 'y', 'k', 'orange', 'purple', 'brown']\n",
    "    markers = ['o', 's', '^', 'v', '<', '>', 'p', '*', 'h', 'D']\n",
    "    \n",
    "    # Ensure we have enough colors and markers\n",
    "    if len(file_uris) > len(colors):\n",
    "        colors = colors * (len(file_uris) // len(colors) + 1)\n",
    "    if len(file_uris) > len(markers):\n",
    "        markers = markers * (len(file_uris) // len(markers) + 1)\n",
    "    \n",
    "    for i, (uri, legend) in enumerate(zip(file_uris, legends)):\n",
    "        df = load_csv_from_s3(uri)\n",
    "        \n",
    "        # Determine which loss column to use based on the file name\n",
    "        if 'training' in uri.lower() or 'train' in uri.lower():\n",
    "            loss_col = 'training_loss' if 'training_loss' in df.columns else 'loss'\n",
    "        elif 'validation' in uri.lower() or 'val' in uri.lower():\n",
    "            loss_col = 'validation_loss' if 'validation_loss' in df.columns else 'loss'\n",
    "        else:\n",
    "            # Default to 'loss' if it exists, otherwise just use the first column that contains 'loss'\n",
    "            loss_cols = [col for col in df.columns if 'loss' in col.lower()]\n",
    "            if loss_cols:\n",
    "                loss_col = loss_cols[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Could not find a loss column in the file: {uri}\")\n",
    "        \n",
    "        # Ensure epoch column exists\n",
    "        if 'epoch_number' not in df.columns:\n",
    "            epoch_cols = [col for col in df.columns if 'epoch' in col.lower()]\n",
    "            if epoch_cols:\n",
    "                epoch_col = epoch_cols[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Could not find an epoch column in the file: {uri}\")\n",
    "        else:\n",
    "            epoch_col = 'epoch_number'\n",
    "        \n",
    "        # Aggregate by epoch\n",
    "        df_by_epoch = df.groupby(epoch_col)[loss_col].mean().reset_index()\n",
    "        \n",
    "        # Plot with line and markers\n",
    "        plt.plot(df_by_epoch[epoch_col], df_by_epoch[loss_col], f'{colors[i]}-', \n",
    "                 label=legend, linewidth=2)\n",
    "        plt.plot(df_by_epoch[epoch_col], df_by_epoch[loss_col], f'{colors[i]}{markers[i]}', alpha=0.7)\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Epoch Number', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Losses vs Epoch Number', fontsize=14)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_losses_vs_epoch(\n",
    "    ['s3://idp-model-finetune-output-us-east-1/model-customization-job-vc1pkgkffpjp/training_artifacts/step_wise_training_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-vc1pkgkffpjp/validation_artifacts/post_fine_tuning_validation/validation/validation_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-joe0bopdzf25/training_artifacts/step_wise_training_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-joe0bopdzf25/validation_artifacts/post_fine_tuning_validation/validation/validation_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-b394czjczgj4/training_artifacts/step_wise_training_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-b394czjczgj4/validation_artifacts/post_fine_tuning_validation/validation/validation_metrics.csv'],\n",
    "    ['Training Loss (Epoch-3)', 'Validation Loss (Epoch-3)', 'Training Loss (Epoch-5, Warmup-10)', 'Validation Loss (Epoch-5, Warmup-10)', 'Training Loss (Epoch-5, Warmup-0)', 'Validation Loss (Epoch-5, Warmup-0)']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# Download files from S3 and convert to dataframes\n",
    "def load_csv_from_s3(s3_uri):\n",
    "    # Parse the S3 URI\n",
    "    s3_uri = s3_uri.replace('s3://', '')\n",
    "    bucket_name = s3_uri.split('/')[0]\n",
    "    key = '/'.join(s3_uri.split('/')[1:])\n",
    "    \n",
    "    # Create S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Download file contents\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "    content = response['Body'].read()\n",
    "    \n",
    "    # Return as DataFrame\n",
    "    return pd.read_csv(io.BytesIO(content))\n",
    "\n",
    "# Extended function to plot losses vs step for multiple files\n",
    "def plot_losses_vs_step(file_uris, legends=None):\n",
    "    \"\"\"\n",
    "    Plot losses vs step for multiple files.\n",
    "    \n",
    "    Args:\n",
    "        file_uris (list): List of S3 URIs for the CSV files\n",
    "        legends (list, optional): List of legend labels. If None, will use default labels.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if legends is None:\n",
    "        legends = [f\"Data {i+1}\" for i in range(len(file_uris))]\n",
    "    \n",
    "    # Ensure we have the same number of legends as files\n",
    "    if len(legends) != len(file_uris):\n",
    "        raise ValueError(\"Number of legends must match number of file URIs\")\n",
    "    \n",
    "    colors = ['b', 'r', 'g', 'c', 'm', 'y', 'k', 'orange', 'purple', 'brown']\n",
    "    line_styles = ['-', '--', '-.', ':', '-', '--', '-.', ':', '-', '--']\n",
    "    \n",
    "    # Ensure we have enough colors and line styles\n",
    "    if len(file_uris) > len(colors):\n",
    "        colors = colors * (len(file_uris) // len(colors) + 1)\n",
    "    if len(file_uris) > len(line_styles):\n",
    "        line_styles = line_styles * (len(file_uris) // len(line_styles) + 1)\n",
    "    \n",
    "    for i, (uri, legend) in enumerate(zip(file_uris, legends)):\n",
    "        df = load_csv_from_s3(uri)\n",
    "        \n",
    "        # Determine which step column to use\n",
    "        if 'step_number' in df.columns:\n",
    "            step_col = 'step_number'\n",
    "        else:\n",
    "            step_cols = [col for col in df.columns if 'step' in col.lower()]\n",
    "            if step_cols:\n",
    "                step_col = step_cols[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Could not find a step column in the file: {uri}\")\n",
    "        \n",
    "        # Determine which loss column to use based on the file name\n",
    "        if 'training' in uri.lower() or 'train' in uri.lower():\n",
    "            loss_col = 'training_loss' if 'training_loss' in df.columns else 'loss'\n",
    "        elif 'validation' in uri.lower() or 'val' in uri.lower():\n",
    "            loss_col = 'validation_loss' if 'validation_loss' in df.columns else 'loss'\n",
    "        else:\n",
    "            # Default to 'loss' if it exists, otherwise just use the first column that contains 'loss'\n",
    "            loss_cols = [col for col in df.columns if 'loss' in col.lower()]\n",
    "            if loss_cols:\n",
    "                loss_col = loss_cols[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Could not find a loss column in the file: {uri}\")\n",
    "        \n",
    "        # Plot with distinctive style\n",
    "        plt.plot(df[step_col], df[loss_col], color=colors[i], \n",
    "                 linestyle=line_styles[i], label=legend, linewidth=2)\n",
    "        \n",
    "        # Optionally add markers at intervals for better readability\n",
    "        # Use every nth point to avoid overcrowding\n",
    "        n = max(1, len(df) // 20)  # Show about 20 markers\n",
    "        plt.plot(df[step_col][::n], df[loss_col][::n], \n",
    "                 marker='o', color=colors[i], linestyle='none', alpha=0.5, markersize=4)\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Step Number', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Losses vs Step Number', fontsize=14)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses_vs_step(\n",
    "    ['s3://idp-model-finetune-output-us-east-1/model-customization-job-vc1pkgkffpjp/training_artifacts/step_wise_training_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-vc1pkgkffpjp/validation_artifacts/post_fine_tuning_validation/validation/validation_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-joe0bopdzf25/training_artifacts/step_wise_training_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-joe0bopdzf25/validation_artifacts/post_fine_tuning_validation/validation/validation_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-b394czjczgj4/training_artifacts/step_wise_training_metrics.csv',\n",
    "     's3://idp-model-finetune-output-us-east-1/model-customization-job-b394czjczgj4/validation_artifacts/post_fine_tuning_validation/validation/validation_metrics.csv'],\n",
    "    ['Training Loss (Epoch-3)', 'Validation Loss (Epoch-3)', 'Training Loss (Epoch-5, Warmup-10)', 'Validation Loss (Epoch-5, Warmup-10)', 'Training Loss (Epoch-5, Warmup-0)', 'Validation Loss (Epoch-5, Warmup-0)']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiic-idp-accelerator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
